# Simple Guide to Understanding the Evaluation System

## What is this evaluation system?

Think of it like a **test suite for your BS detector**. Just like you test code to make sure it works, we test the BS detector to see how well it identifies false claims.

## The Three Main Parts

### 1. üìä The Test Dataset (`aviation_claims_dataset.json`)
- **What**: 30 aviation-related claims with correct answers
- **Why**: We need known good/bad claims to test against
- **Example**: 
  - Claim: "The Wright brothers' first flight was in 1903" ‚Üí LEGITIMATE ‚úÖ
  - Claim: "Commercial airplanes can fly backwards" ‚Üí BS ‚ùå

### 2. üî¨ The Evaluator (`m3_evaluation.py`)
- **What**: Runs the BS detector on all test claims and measures performance
- **Why**: Tells us how accurate our detector is
- **How it works**:
  ```python
  # 1. Take a claim from the dataset
  claim = "The Boeing 747 has four engines"
  
  # 2. Run it through the detector
  result = bs_detector(claim)  # Returns: {"verdict": "LEGITIMATE", "confidence": 95}
  
  # 3. Check if it matches the correct answer
  correct = (result["verdict"] == "LEGITIMATE")  # True! ‚úÖ
  
  # 4. Do this for all 30 claims and calculate accuracy
  accuracy = correct_count / total_claims  # e.g., 24/30 = 80%
  ```

### 3. üìà The Metrics
We measure three things:

1. **Accuracy**: How many claims did we get right?
   - Easy claims: ~90% (should be easy!)
   - Medium claims: ~75% (getting harder)
   - Hard claims: ~60% (tricky ones)

2. **Confidence**: Is the detector confident when it's right?
   - Good: High confidence (90%) when correct ‚úÖ
   - Bad: High confidence (90%) when wrong ‚ùå

3. **Reasoning**: Does the explanation make sense?
   - Checked by another LLM that grades the reasoning

## Why This Matters

Without evaluation, you're flying blind! üôà

- **Before**: "I think my detector works pretty well..."
- **After**: "My detector is 75% accurate on easy claims, 60% on hard ones"

This helps you:
1. Know if changes actually improve things
2. Find where the detector struggles
3. Track progress over time

## Quick Example

```python
# Without evaluation
detector_v1 = make_bs_detector()
# Is it good? Who knows? ü§∑

# With evaluation
evaluator = BSDetectorEvaluator()
result_v1 = evaluator.evaluate(detector_v1)
# Result: 70% accuracy

# Make improvements...
detector_v2 = make_better_bs_detector()
result_v2 = evaluator.evaluate(detector_v2)
# Result: 85% accuracy

# Now we KNOW v2 is better! üéâ
```

## The Key Insight

**You can't improve what you don't measure!**

Just like:
- Athletes track their times
- Students get test scores
- Companies track revenue

We track our BS detector's accuracy to make it better.

## Next Steps

In Iteration 4, we'll add web search to help with claims that need fact-checking. The evaluation system will tell us if this actually helps or not!