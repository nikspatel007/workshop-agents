{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Iteration 5: Human-in-the-Loop (Simplified v2)\n\nThis notebook demonstrates a simplified human-in-the-loop approach that:\n- Uses conditional routing instead of complex interrupts\n- Simulates human review for demo purposes\n- Can be easily extended for production use"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## The Concept: Simple Human Review Node\n\nInstead of using LangGraph's interrupt functionality (which requires complex state management), we use a simpler approach:\n\n1. **Review Check Node** - Determines if human review is needed\n2. **Conditional Routing** - Routes to human review or output\n3. **Simulated Review** - For demos, simulates human decisions\n4. **Production Ready** - Easy to replace simulation with real review system\n\nThis approach is:\n- ✅ Simple to understand and implement\n- ✅ Easy to test and debug\n- ✅ Straightforward to extend for production\n- ✅ No complex state management needed"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nsys.path.append('..')\n\nfrom modules.m5_human_in_loop_v2 import (\n    HumanInLoopState,\n    check_claim_with_human_review_v2,\n    create_human_in_loop_graph_v2\n)\nfrom langgraph.graph import StateGraph\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(\"✅ Imports successful!\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Visualize the graph structure\nimport base64\nfrom IPython.display import Image\n\napp = create_human_in_loop_graph_v2()\nmermaid_code = app.get_graph().draw_mermaid()\n\n# Render the graph\ngraph_bytes = mermaid_code.encode(\"utf-8\")\nbase64_string = base64.b64encode(graph_bytes).decode(\"ascii\")\nimage_url = f\"https://mermaid.ink/img/{base64_string}?type=png\"\n\nprint(\"📊 Human-in-the-Loop Graph Structure:\")\nImage(url=image_url)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test claims with different confidence levels\ntest_scenarios = [\n    {\n        \"claim\": \"The Boeing 747 has four engines\",\n        \"expected\": \"High confidence, no review needed\"\n    },\n    {\n        \"claim\": \"A startup just invented teleportation technology\",\n        \"expected\": \"Low confidence, should trigger review\"\n    },\n    {\n        \"claim\": \"SpaceX announced a Mars mission for next week\",\n        \"expected\": \"Current event with uncertainty\"\n    }\n]\n\nprint(\"🧪 Testing Human-in-the-Loop Scenarios\\n\")\n\nfor i, scenario in enumerate(test_scenarios):\n    print(f\"\\n{'='*70}\")\n    print(f\"Scenario {i+1}: {scenario['expected']}\")\n    print(f\"Claim: \\\"{scenario['claim']}\\\"\")\n    print(\"=\"*70)\n    \n    # Run the claim through the system\n    result = check_claim_with_human_review_v2(scenario['claim'])\n    \n    print(f\"\\n✅ Final Decision:\")\n    print(f\"Verdict: {result['verdict']} ({result['confidence']}%)\")\n    print(f\"Reasoning: {result['reasoning'][:100]}...\")\n    print(f\"Human Reviewed: {result.get('human_reviewed', False)}\")\n    \n    if result.get('human_review_reason'):\n        print(f\"Review Reason: {result['human_review_reason']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Testing Different Scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Interactive Human Review\n",
    "\n",
    "Let's create an interactive example where you can provide the human review:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Example: Skip human review for testing\nprint(\"🔧 Testing with human review disabled\\n\")\n\ntest_claim = \"A new AI system claims to be sentient\"\n\n# Run without human review (for testing)\nresult_no_review = check_claim_with_human_review_v2(test_claim, skip_human_review=True)\n\nprint(f\"Claim: \\\"{test_claim}\\\"\")\nprint(f\"\\nResult (no human review):\")\nprint(f\"Verdict: {result_no_review['verdict']} ({result_no_review['confidence']}%)\")\nprint(f\"Human Reviewed: {result_no_review.get('human_reviewed', False)}\")\n\nprint(\"\\n\" + \"-\"*60 + \"\\n\")\n\n# Run with human review enabled (default)\nresult_with_review = check_claim_with_human_review_v2(test_claim, skip_human_review=False)\n\nprint(f\"Result (with human review if needed):\")\nprint(f\"Verdict: {result_with_review['verdict']} ({result_with_review['confidence']}%)\")\nprint(f\"Human Reviewed: {result_with_review.get('human_reviewed', False)}\")\nif result_with_review.get('human_review_reason'):\n    print(f\"Review Reason: {result_with_review['human_review_reason']}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Example: Skip human review for testing\nprint(\"🔧 Testing with human review disabled\\n\")\n\ntest_claim = \"A new AI system claims to be sentient\"\n\n# Run without human review (for testing)\nresult_no_review = check_claim_with_human_review_v2(test_claim, skip_human_review=True)\n\nprint(f\"Claim: \\\"{test_claim}\\\"\")\nprint(f\"\\nResult (no human review):\")\nprint(f\"Verdict: {result_no_review['verdict']} ({result_no_review['confidence']}%)\")\nprint(f\"Human Reviewed: {result_no_review.get('human_reviewed', False)}\")\n\nprint(\"\\n\" + \"-\"*60 + \"\\n\")\n\n# Run with human review enabled (default)\nresult_with_review = check_claim_with_human_review_v2(test_claim, skip_human_review=False)\n\nprint(f\"Result (with human review if needed):\")\nprint(f\"Verdict: {result_with_review['verdict']} ({result_with_review['confidence']}%)\")\nprint(f\"Human Reviewed: {result_with_review.get('human_reviewed', False)}\")\nif result_with_review.get('human_review_reason'):\n    print(f\"Review Reason: {result_with_review['human_review_reason']}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Monitoring human review metrics\n",
    "class ReviewMetrics:\n",
    "    def __init__(self):\n",
    "        self.total_claims = 0\n",
    "        self.reviews_triggered = 0\n",
    "        self.avg_confidence_before = 0\n",
    "        self.avg_confidence_after = 0\n",
    "        self.review_reasons = {}\n",
    "    \n",
    "    def track_claim(self, needs_review: bool, reason: str = None, \n",
    "                   ai_confidence: int = None, human_confidence: int = None):\n",
    "        self.total_claims += 1\n",
    "        \n",
    "        if needs_review:\n",
    "            self.reviews_triggered += 1\n",
    "            if reason:\n",
    "                self.review_reasons[reason] = self.review_reasons.get(reason, 0) + 1\n",
    "            \n",
    "            if ai_confidence and human_confidence:\n",
    "                # Update running averages\n",
    "                n = self.reviews_triggered\n",
    "                self.avg_confidence_before = (\n",
    "                    (self.avg_confidence_before * (n-1) + ai_confidence) / n\n",
    "                )\n",
    "                self.avg_confidence_after = (\n",
    "                    (self.avg_confidence_after * (n-1) + human_confidence) / n\n",
    "                )\n",
    "    \n",
    "    def get_summary(self):\n",
    "        review_rate = self.reviews_triggered / self.total_claims if self.total_claims else 0\n",
    "        \n",
    "        return {\n",
    "            \"total_claims\": self.total_claims,\n",
    "            \"review_rate\": f\"{review_rate:.1%}\",\n",
    "            \"avg_confidence_improvement\": \n",
    "                f\"{self.avg_confidence_before:.0f}% → {self.avg_confidence_after:.0f}%\",\n",
    "            \"top_reasons\": sorted(\n",
    "                self.review_reasons.items(), \n",
    "                key=lambda x: x[1], \n",
    "                reverse=True\n",
    "            )[:3]\n",
    "        }\n",
    "\n",
    "# Demo metrics\n",
    "metrics = ReviewMetrics()\n",
    "\n",
    "# Simulate some tracking\n",
    "metrics.track_claim(needs_review=False)\n",
    "metrics.track_claim(needs_review=True, reason=\"Low confidence: 30%\", \n",
    "                   ai_confidence=30, human_confidence=95)\n",
    "metrics.track_claim(needs_review=True, reason=\"Current event uncertainty\",\n",
    "                   ai_confidence=45, human_confidence=80)\n",
    "metrics.track_claim(needs_review=False)\n",
    "\n",
    "print(\"📊 Human Review Metrics:\")\n",
    "summary = metrics.get_summary()\n",
    "for key, value in summary.items():\n",
    "    if key == \"top_reasons\":\n",
    "        print(f\"\\nTop Review Reasons:\")\n",
    "        for reason, count in value:\n",
    "            print(f\"  - {reason}: {count}\")\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Production pattern for human review\nclass ProductionHumanReviewSystem:\n    \"\"\"Production-ready human review system\"\"\"\n    \n    def __init__(self):\n        self.pending_reviews = {}  # id -> review info\n        self.completed_reviews = {}\n        self.review_counter = 0\n    \n    def process_claim(self, claim: str) -> dict:\n        \"\"\"Process a claim and handle human review if needed\"\"\"\n        # Run through the system\n        result = check_claim_with_human_review_v2(claim)\n        \n        if result.get('human_reviewed') and result.get('human_review_reason'):\n            # In production, this would queue for real human review\n            self.review_counter += 1\n            review_id = f\"review_{self.review_counter}\"\n            \n            self.pending_reviews[review_id] = {\n                \"claim\": claim,\n                \"ai_verdict\": result['verdict'],\n                \"ai_confidence\": result['confidence'],\n                \"reason\": result['human_review_reason'],\n                \"created_at\": datetime.now()\n            }\n            \n            return {\n                \"status\": \"pending_human_review\",\n                \"review_id\": review_id,\n                \"preliminary_result\": result\n            }\n        \n        return {\n            \"status\": \"completed\",\n            \"result\": result\n        }\n    \n    def get_pending_reviews(self) -> list:\n        \"\"\"Get all pending reviews\"\"\"\n        return [\n            {\n                \"review_id\": rid,\n                \"claim\": info[\"claim\"][:50] + \"...\",\n                \"reason\": info[\"reason\"],\n                \"waiting_time\": (datetime.now() - info[\"created_at\"]).seconds\n            }\n            for rid, info in self.pending_reviews.items()\n        ]\n\n# Demo the production pattern\nfrom datetime import datetime\n\nsystem = ProductionHumanReviewSystem()\n\n# Test various claims\ntest_claims = [\n    \"The Boeing 747 has four engines\",  # High confidence\n    \"Scientists created a perpetual motion machine\",  # Should trigger review\n    \"A new quantum computer solved climate change\",  # Should trigger review\n]\n\nprint(\"🏭 Production Human Review System Demo\\n\")\n\nfor claim in test_claims:\n    print(f\"\\nProcessing: \\\"{claim}\\\"\")\n    result = system.process_claim(claim)\n    print(f\"Status: {result['status']}\")\n    \n    if result['status'] == 'pending_human_review':\n        print(f\"Review ID: {result['review_id']}\")\n        print(f\"Reason: {result['preliminary_result']['human_review_reason']}\")\n\n# Show pending reviews\npending = system.get_pending_reviews()\nif pending:\n    print(f\"\\n\\n📋 Pending Human Reviews: {len(pending)}\")\n    for review in pending:\n        print(f\"\\n{review['review_id']}:\")\n        print(f\"  Claim: {review['claim']}\")\n        print(f\"  Reason: {review['reason']}\")\n        print(f\"  Waiting: {review['waiting_time']}s\")"
  },
  {
   "cell_type": "markdown",
   "source": "## Summary\n\n### What We Built:\n✅ **Simple Implementation** - Conditional routing to human review node  \n✅ **Demo Ready** - Simulated human review for testing  \n✅ **Production Pattern** - Easy to replace simulation with real system  \n✅ **Flexible Control** - Can skip review for testing  \n\n### Key Advantages:\n- 🎯 **Simple Code** - No complex state management\n- 🔄 **Easy Testing** - Can simulate or skip review\n- ⚡ **Straightforward** - Clear flow through the graph\n- 📊 **Observable** - Easy to add metrics and monitoring\n\n### Production Implementation:\nTo use in production, replace the `simulate_human_review_node` with:\n1. Queue the review request\n2. Return a pending status\n3. Have a separate process handle human reviews\n4. Update results asynchronously\n\n### Next Steps:\nIn Iteration 6, we'll add **Memory/Persistence** to:\n- Remember past decisions\n- Learn from human feedback\n- Build a knowledge base\n- Reduce repeated reviews",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}