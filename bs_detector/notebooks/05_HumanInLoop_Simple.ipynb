{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iteration 5: Human-in-the-Loop (Simplified)\n",
    "\n",
    "This notebook demonstrates human-in-the-loop using LangGraph's built-in interrupt functionality.\n",
    "This is a much simpler and more production-ready approach than custom implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Concept: LangGraph Interrupts\n",
    "\n",
    "LangGraph provides a built-in way to pause graph execution and wait for human input:\n",
    "\n",
    "1. **Checkpointing** - Graph state is saved when interrupted\n",
    "2. **interrupt_before** - Specify nodes that need human approval\n",
    "3. **Resume** - Continue execution with human-provided updates\n",
    "\n",
    "This approach is:\n",
    "- âœ… Simple to implement\n",
    "- âœ… Production-ready\n",
    "- âœ… Handles async naturally\n",
    "- âœ… Maintains conversation context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from modules.m5_human_in_loop_simple import (\n",
    "    HumanInLoopState,\n",
    "    check_claim_with_human_review,\n",
    "    resume_after_human_input,\n",
    "    create_human_in_loop_graph\n",
    ")\n",
    "from langgraph.graph import StateGraph\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"âœ… Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding the Flow\n",
    "\n",
    "Our simplified human-in-the-loop flow:\n",
    "\n",
    "1. AI analyzes the claim\n",
    "2. If confidence < 50% or verdict is UNCERTAIN â†’ Request human review\n",
    "3. Graph **interrupts** before human_review node\n",
    "4. Human provides feedback\n",
    "5. Graph **resumes** with human input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the graph structure\n",
    "import base64\n",
    "from IPython.display import Image\n",
    "\n",
    "app = create_human_in_loop_graph()\n",
    "mermaid_code = app.get_graph().draw_mermaid()\n",
    "\n",
    "# Render the graph\n",
    "graph_bytes = mermaid_code.encode(\"utf-8\")\n",
    "base64_string = base64.b64encode(graph_bytes).decode(\"ascii\")\n",
    "image_url = f\"https://mermaid.ink/img/{base64_string}?type=png\"\n",
    "\n",
    "print(\"ðŸ“Š Human-in-the-Loop Graph Structure:\")\n",
    "Image(url=image_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Testing Different Scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test claims with different confidence levels\n",
    "test_scenarios = [\n",
    "    {\n",
    "        \"claim\": \"The Boeing 747 has four engines\",\n",
    "        \"expected\": \"High confidence, no review needed\"\n",
    "    },\n",
    "    {\n",
    "        \"claim\": \"A startup just invented teleportation technology\",\n",
    "        \"expected\": \"Low confidence, should trigger review\"\n",
    "    },\n",
    "    {\n",
    "        \"claim\": \"SpaceX announced a Mars mission for next week\",\n",
    "        \"expected\": \"Current event with uncertainty\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"ðŸ§ª Testing Human-in-the-Loop Scenarios\\n\")\n",
    "\n",
    "for i, scenario in enumerate(test_scenarios):\n",
    "    thread_id = f\"test_{i}\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Scenario {i+1}: {scenario['expected']}\")\n",
    "    print(f\"Claim: \\\"{scenario['claim']}\\\"\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Run the claim\n",
    "    result = check_claim_with_human_review(scenario['claim'], thread_id)\n",
    "    \n",
    "    if result:\n",
    "        # No human review needed\n",
    "        print(f\"\\nâœ… AI Decision (No Review Needed):\")\n",
    "        print(f\"Verdict: {result['verdict']} ({result['confidence']}%)\")\n",
    "        print(f\"Reasoning: {result['reasoning'][:100]}...\")\n",
    "    else:\n",
    "        # Human review needed - graph is interrupted\n",
    "        print(f\"\\nðŸ¤š GRAPH INTERRUPTED - Human Review Required\")\n",
    "        print(\"\\nSimulating human input...\")\n",
    "        \n",
    "        # Simulate different human responses\n",
    "        if \"teleportation\" in scenario['claim']:\n",
    "            human_verdict = \"BS\"\n",
    "            human_confidence = 99\n",
    "            human_reasoning = \"Teleportation violates known physics\"\n",
    "        else:\n",
    "            human_verdict = \"UNCERTAIN\"\n",
    "            human_confidence = 60\n",
    "            human_reasoning = \"Need to verify with official sources\"\n",
    "        \n",
    "        # Resume with human input\n",
    "        final_result = resume_after_human_input(\n",
    "            thread_id,\n",
    "            human_verdict,\n",
    "            human_confidence,\n",
    "            human_reasoning\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nâœ… Final Decision (After Human Review):\")\n",
    "        print(f\"Verdict: {final_result['verdict']} ({final_result['confidence']}%)\")\n",
    "        print(f\"Reasoning: {final_result['reasoning']}\")\n",
    "        print(f\"Human Reviewed: {final_result.get('human_reviewed', False)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Interactive Human Review\n",
    "\n",
    "Let's create an interactive example where you can provide the human review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive claim checking\n",
    "def interactive_check(claim: str):\n",
    "    \"\"\"Interactive function for human review\"\"\"\n",
    "    import uuid\n",
    "    thread_id = str(uuid.uuid4())\n",
    "    \n",
    "    print(f\"\\nðŸ” Checking claim: \\\"{claim}\\\"\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    result = check_claim_with_human_review(claim, thread_id)\n",
    "    \n",
    "    if result:\n",
    "        print(f\"\\nâœ… AI made a confident decision:\")\n",
    "        print(f\"Verdict: {result['verdict']} ({result['confidence']}%)\")\n",
    "        print(f\"Reasoning: {result['reasoning']}\")\n",
    "        return result\n",
    "    else:\n",
    "        print(f\"\\nðŸ¤š Human review required!\")\n",
    "        print(\"\\nPlease provide your assessment:\")\n",
    "        \n",
    "        # Get human input\n",
    "        verdict = input(\"Verdict (BS/LEGITIMATE/UNCERTAIN): \").upper()\n",
    "        while verdict not in [\"BS\", \"LEGITIMATE\", \"UNCERTAIN\"]:\n",
    "            verdict = input(\"Please enter BS, LEGITIMATE, or UNCERTAIN: \").upper()\n",
    "        \n",
    "        confidence = int(input(\"Confidence (0-100): \"))\n",
    "        reasoning = input(\"Reasoning: \")\n",
    "        \n",
    "        # Resume with human input\n",
    "        final_result = resume_after_human_input(\n",
    "            thread_id,\n",
    "            verdict,\n",
    "            confidence,\n",
    "            reasoning\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nâœ… Final result:\")\n",
    "        print(f\"Verdict: {final_result['verdict']} ({final_result['confidence']}%)\")\n",
    "        print(f\"Reasoning: {final_result['reasoning']}\")\n",
    "        \n",
    "        return final_result\n",
    "\n",
    "# Test with an ambiguous claim\n",
    "test_claim = \"A new AI system passed the Turing test yesterday\"\n",
    "# interactive_check(test_claim)  # Uncomment to run interactively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Production Patterns\n",
    "\n",
    "In a production system, you would typically:\n",
    "\n",
    "1. **Queue interrupted requests** for human review\n",
    "2. **Send notifications** to reviewers\n",
    "3. **Track thread IDs** to resume later\n",
    "4. **Set timeouts** for human responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example production pattern\n",
    "class ProductionHumanReview:\n",
    "    \"\"\"Production-ready human review system\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.pending_reviews = {}  # thread_id -> claim info\n",
    "        self.completed_reviews = {}\n",
    "    \n",
    "    def process_claim(self, claim: str) -> dict:\n",
    "        \"\"\"Process a claim, queuing for human review if needed\"\"\"\n",
    "        import uuid\n",
    "        thread_id = str(uuid.uuid4())\n",
    "        \n",
    "        # First pass\n",
    "        result = check_claim_with_human_review(claim, thread_id)\n",
    "        \n",
    "        if result:\n",
    "            # AI was confident\n",
    "            return {\n",
    "                \"status\": \"completed\",\n",
    "                \"result\": result,\n",
    "                \"thread_id\": thread_id\n",
    "            }\n",
    "        else:\n",
    "            # Queue for human review\n",
    "            self.pending_reviews[thread_id] = {\n",
    "                \"claim\": claim,\n",
    "                \"status\": \"pending_review\",\n",
    "                \"created_at\": datetime.now()\n",
    "            }\n",
    "            \n",
    "            # In production: send to queue, notify reviewers, etc.\n",
    "            return {\n",
    "                \"status\": \"pending_review\",\n",
    "                \"thread_id\": thread_id,\n",
    "                \"message\": \"Claim queued for human review\"\n",
    "            }\n",
    "    \n",
    "    def submit_review(self, thread_id: str, verdict: str, \n",
    "                     confidence: int, reasoning: str) -> dict:\n",
    "        \"\"\"Submit human review and get final result\"\"\"\n",
    "        if thread_id not in self.pending_reviews:\n",
    "            return {\"error\": \"Unknown thread_id\"}\n",
    "        \n",
    "        # Resume graph with human input\n",
    "        result = resume_after_human_input(\n",
    "            thread_id, verdict, confidence, reasoning\n",
    "        )\n",
    "        \n",
    "        # Move to completed\n",
    "        review_info = self.pending_reviews.pop(thread_id)\n",
    "        review_info[\"completed_at\"] = datetime.now()\n",
    "        review_info[\"result\"] = result\n",
    "        self.completed_reviews[thread_id] = review_info\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"completed\",\n",
    "            \"result\": result,\n",
    "            \"thread_id\": thread_id\n",
    "        }\n",
    "    \n",
    "    def get_pending_reviews(self):\n",
    "        \"\"\"Get all pending reviews\"\"\"\n",
    "        return [\n",
    "            {\n",
    "                \"thread_id\": tid,\n",
    "                \"claim\": info[\"claim\"],\n",
    "                \"waiting_time\": (datetime.now() - info[\"created_at\"]).seconds\n",
    "            }\n",
    "            for tid, info in self.pending_reviews.items()\n",
    "        ]\n",
    "\n",
    "# Demo the production pattern\n",
    "from datetime import datetime\n",
    "\n",
    "system = ProductionHumanReview()\n",
    "\n",
    "# Process some claims\n",
    "claims = [\n",
    "    \"The Wright brothers flew in 1903\",  # High confidence\n",
    "    \"Cold fusion was achieved last month\",  # Needs review\n",
    "]\n",
    "\n",
    "results = []\n",
    "for claim in claims:\n",
    "    result = system.process_claim(claim)\n",
    "    results.append(result)\n",
    "    print(f\"\\nProcessed: {claim[:30]}...\")\n",
    "    print(f\"Status: {result['status']}\")\n",
    "\n",
    "# Check pending reviews\n",
    "pending = system.get_pending_reviews()\n",
    "if pending:\n",
    "    print(f\"\\nðŸ“‹ Pending Reviews: {len(pending)}\")\n",
    "    for review in pending:\n",
    "        print(f\"  - {review['claim'][:50]}... (waiting {review['waiting_time']}s)\")\n",
    "        \n",
    "        # Simulate human review\n",
    "        print(\"\\n  Simulating human review...\")\n",
    "        final = system.submit_review(\n",
    "            review['thread_id'],\n",
    "            \"BS\",\n",
    "            95,\n",
    "            \"Cold fusion remains unproven\"\n",
    "        )\n",
    "        print(f\"  Final verdict: {final['result']['verdict']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Benefits of LangGraph's Approach\n",
    "\n",
    "### âœ… Built-in Features:\n",
    "- **Checkpointing** - Automatic state persistence\n",
    "- **Thread Management** - Conversation context maintained\n",
    "- **Async Support** - Non-blocking by design\n",
    "- **Error Handling** - Graceful failure recovery\n",
    "\n",
    "### âœ… Production Ready:\n",
    "- **Scalable** - Can handle many concurrent reviews\n",
    "- **Reliable** - State persisted across restarts\n",
    "- **Flexible** - Easy to integrate with queues/databases\n",
    "- **Observable** - Built-in monitoring capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Monitoring human review metrics\n",
    "class ReviewMetrics:\n",
    "    def __init__(self):\n",
    "        self.total_claims = 0\n",
    "        self.reviews_triggered = 0\n",
    "        self.avg_confidence_before = 0\n",
    "        self.avg_confidence_after = 0\n",
    "        self.review_reasons = {}\n",
    "    \n",
    "    def track_claim(self, needs_review: bool, reason: str = None, \n",
    "                   ai_confidence: int = None, human_confidence: int = None):\n",
    "        self.total_claims += 1\n",
    "        \n",
    "        if needs_review:\n",
    "            self.reviews_triggered += 1\n",
    "            if reason:\n",
    "                self.review_reasons[reason] = self.review_reasons.get(reason, 0) + 1\n",
    "            \n",
    "            if ai_confidence and human_confidence:\n",
    "                # Update running averages\n",
    "                n = self.reviews_triggered\n",
    "                self.avg_confidence_before = (\n",
    "                    (self.avg_confidence_before * (n-1) + ai_confidence) / n\n",
    "                )\n",
    "                self.avg_confidence_after = (\n",
    "                    (self.avg_confidence_after * (n-1) + human_confidence) / n\n",
    "                )\n",
    "    \n",
    "    def get_summary(self):\n",
    "        review_rate = self.reviews_triggered / self.total_claims if self.total_claims else 0\n",
    "        \n",
    "        return {\n",
    "            \"total_claims\": self.total_claims,\n",
    "            \"review_rate\": f\"{review_rate:.1%}\",\n",
    "            \"avg_confidence_improvement\": \n",
    "                f\"{self.avg_confidence_before:.0f}% â†’ {self.avg_confidence_after:.0f}%\",\n",
    "            \"top_reasons\": sorted(\n",
    "                self.review_reasons.items(), \n",
    "                key=lambda x: x[1], \n",
    "                reverse=True\n",
    "            )[:3]\n",
    "        }\n",
    "\n",
    "# Demo metrics\n",
    "metrics = ReviewMetrics()\n",
    "\n",
    "# Simulate some tracking\n",
    "metrics.track_claim(needs_review=False)\n",
    "metrics.track_claim(needs_review=True, reason=\"Low confidence: 30%\", \n",
    "                   ai_confidence=30, human_confidence=95)\n",
    "metrics.track_claim(needs_review=True, reason=\"Current event uncertainty\",\n",
    "                   ai_confidence=45, human_confidence=80)\n",
    "metrics.track_claim(needs_review=False)\n",
    "\n",
    "print(\"ðŸ“Š Human Review Metrics:\")\n",
    "summary = metrics.get_summary()\n",
    "for key, value in summary.items():\n",
    "    if key == \"top_reasons\":\n",
    "        print(f\"\\nTop Review Reasons:\")\n",
    "        for reason, count in value:\n",
    "            print(f\"  - {reason}: {count}\")\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What We Built:\n",
    "âœ… **Simple Implementation** - Using LangGraph's built-in interrupts  \n",
    "âœ… **Production Ready** - Checkpointing, thread management included  \n",
    "âœ… **Flexible Integration** - Easy to add queues, notifications, etc.  \n",
    "âœ… **Maintains Context** - Full conversation history preserved  \n",
    "\n",
    "### Key Advantages:\n",
    "- ðŸŽ¯ **Less Code** - LangGraph handles the complexity\n",
    "- ðŸ”„ **Reliable** - State persisted automatically\n",
    "- âš¡ **Async Native** - Non-blocking by design\n",
    "- ðŸ“Š **Observable** - Easy to add monitoring\n",
    "\n",
    "### Next Steps:\n",
    "In Iteration 6, we'll add **Memory/Persistence** to:\n",
    "- Remember past decisions\n",
    "- Learn from human feedback\n",
    "- Build a knowledge base\n",
    "- Reduce repeated reviews"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}