{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iteration 5: Human-in-the-Loop\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this iteration, we add human review capabilities for cases where AI needs help. This is crucial for:\n",
    "- Handling edge cases beyond AI capabilities\n",
    "- Building user trust through transparency\n",
    "- Continuous improvement through feedback\n",
    "- Meeting compliance/regulatory requirements\n",
    "\n",
    "## What You'll Learn\n",
    "1. **Uncertainty Detection** - When should AI ask for help?\n",
    "2. **Human Review Interface** - How to present info for human decision\n",
    "3. **Feedback Integration** - Incorporating human input into the flow\n",
    "4. **Async Patterns** - Handling human input without blocking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Concept: When and Why to Ask Humans\n",
    "\n",
    "### When to Request Human Review:\n",
    "1. **Low Confidence** (<50%) - AI is unsure\n",
    "2. **Expert Disagreement** - Different agents reach different conclusions\n",
    "3. **High Stakes** - Critical decisions need human oversight\n",
    "4. **No Evidence** - Claims that can't be verified with available tools\n",
    "5. **Explicit Request** - User wants human verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Imports successful!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from modules.m5_human_in_loop import (\n",
    "    HumanReviewRequest,\n",
    "    HumanFeedback,\n",
    "    HumanInLoopState,\n",
    "    calculate_uncertainty,\n",
    "    check_claim_with_human_in_loop,\n",
    "    interactive_human_input,\n",
    "    create_human_in_loop_bs_detector\n",
    ")\n",
    "from config.llm_factory import LLMFactory\n",
    "import json\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "print(\"âœ… Imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Imports successful!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from modules.m5_human_in_loop import (\n",
    "    HumanReviewRequest,\n",
    "    HumanFeedback,\n",
    "    HumanInLoopState,\n",
    "    calculate_uncertainty,\n",
    "    check_claim_with_human_in_loop,\n",
    "    interactive_human_input,\n",
    "    create_human_in_loop_bs_detector\n",
    ")\n",
    "from config.llm_factory import LLMFactory\n",
    "import json\n",
    "from IPython.display import display, HTML\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning, module='langchain_community')\n",
    "\n",
    "print(\"âœ… Imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Uncertainty Analysis:\n",
      "\n",
      "Test 1: The Boeing 747 has four engines...\n",
      "  Confidence: 95%\n",
      "  Uncertainty Score: 0.00\n",
      "  Needs Review: No\n",
      "\n",
      "Test 2: Some new aviation technology was announced...\n",
      "  Confidence: 40%\n",
      "  Uncertainty Score: 0.50\n",
      "  Needs Review: No\n",
      "\n",
      "Test 3: Planes will be fully autonomous by 2025...\n",
      "  Confidence: 70%\n",
      "  Uncertainty Score: 0.30\n",
      "  Needs Review: No\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create test states with different uncertainty levels\n",
    "test_states = [\n",
    "    # High confidence, clear verdict\n",
    "    HumanInLoopState(\n",
    "        claim=\"The Boeing 747 has four engines\",\n",
    "        verdict=\"LEGITIMATE\", \n",
    "        confidence=95,\n",
    "        claim_type=\"technical\"\n",
    "    ),\n",
    "    \n",
    "    # Low confidence\n",
    "    HumanInLoopState(\n",
    "        claim=\"Some new aviation technology was announced\",\n",
    "        verdict=\"LEGITIMATE\",\n",
    "        confidence=40,\n",
    "        claim_type=\"current_event\"\n",
    "    ),\n",
    "    \n",
    "    # Expert disagreement\n",
    "    HumanInLoopState(\n",
    "        claim=\"Planes will be fully autonomous by 2025\",\n",
    "        verdict=\"BS\",\n",
    "        confidence=70,\n",
    "        expert_opinions={\n",
    "            \"technical_expert\": {\"verdict\": \"BS\", \"confidence\": 80},\n",
    "            \"general_expert\": {\"verdict\": \"LEGITIMATE\", \"confidence\": 60}\n",
    "        }\n",
    "    )\n",
    "]\n",
    "\n",
    "# Calculate uncertainty for each\n",
    "print(\"ðŸ” Uncertainty Analysis:\\n\")\n",
    "for i, state in enumerate(test_states, 1):\n",
    "    uncertainty = calculate_uncertainty(state)\n",
    "    print(f\"Test {i}: {state.claim[:50]}...\")\n",
    "    print(f\"  Confidence: {state.confidence}%\")\n",
    "    print(f\"  Uncertainty Score: {uncertainty:.2f}\")\n",
    "    print(f\"  Needs Review: {'Yes' if uncertainty > 0.6 else 'No'}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Human Review Request Format\n",
    "\n",
    "When we need human input, we present all relevant information clearly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ðŸ¤” HUMAN REVIEW REQUESTED\n",
      "============================================================\n",
      "\n",
      "**Claim**: ChatGPT-5 was released yesterday with consciousness\n",
      "\n",
      "**AI Assessment**:\n",
      "- Verdict: BS\n",
      "- Confidence: 45%\n",
      "- Reasoning: Claim about consciousness is dubious and no official announcement found\n",
      "\n",
      "**Uncertainty Reasons**:\n",
      "- Very low confidence: 45%\n",
      "- No evidence found for recent event\n",
      "- Extraordinary claim requiring extraordinary evidence\n",
      "\n",
      "**Expert Opinions**:\n",
      "\n",
      "Current Events Expert:\n",
      "  - Verdict: BS\n",
      "  - Confidence: 45%\n",
      "  - Reasoning: No official announcement from OpenAI found\n",
      "\n",
      "Technical Expert:\n",
      "  - Verdict: BS\n",
      "  - Confidence: 90%\n",
      "  - Reasoning: Claims of consciousness are not scientifically supported\n",
      "\n",
      "**Search Results**: 2 results found\n",
      "  1. OpenAI has not announced ChatGPT-5 as of latest search...\n",
      "  2. No credible sources report AI consciousness breakthrough...\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a sample review request\n",
    "review_request = HumanReviewRequest(\n",
    "    claim=\"ChatGPT-5 was released yesterday with consciousness\",\n",
    "    ai_verdict=\"BS\",\n",
    "    ai_confidence=45,\n",
    "    ai_reasoning=\"Claim about consciousness is dubious and no official announcement found\",\n",
    "    uncertainty_reasons=[\n",
    "        \"Very low confidence: 45%\",\n",
    "        \"No evidence found for recent event\",\n",
    "        \"Extraordinary claim requiring extraordinary evidence\"\n",
    "    ],\n",
    "    expert_opinions={\n",
    "        \"Current Events Expert\": {\n",
    "            \"verdict\": \"BS\",\n",
    "            \"confidence\": 45,\n",
    "            \"reasoning\": \"No official announcement from OpenAI found\"\n",
    "        },\n",
    "        \"Technical Expert\": {\n",
    "            \"verdict\": \"BS\", \n",
    "            \"confidence\": 90,\n",
    "            \"reasoning\": \"Claims of consciousness are not scientifically supported\"\n",
    "        }\n",
    "    },\n",
    "    search_results=[\n",
    "        {\"fact\": \"OpenAI has not announced ChatGPT-5 as of latest search\"},\n",
    "        {\"fact\": \"No credible sources report AI consciousness breakthrough\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Display formatted request\n",
    "print(review_request.format_for_human())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Simulated Human Feedback\n",
    "\n",
    "Let's see how human feedback is structured:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§‘ Human Feedback:\n",
      "Verdict: BS\n",
      "Confidence: 95%\n",
      "Reasoning: No such announcement exists. ChatGPT-5 has not been released.\n",
      "Context: I checked OpenAI's official channels and tech news sites.\n",
      "Sources: https://openai.com/blog, Tech news aggregators\n"
     ]
    }
   ],
   "source": [
    "# Example human feedback\n",
    "human_feedback = HumanFeedback(\n",
    "    verdict=\"BS\",\n",
    "    confidence=95,\n",
    "    reasoning=\"No such announcement exists. ChatGPT-5 has not been released.\",\n",
    "    additional_context=\"I checked OpenAI's official channels and tech news sites.\",\n",
    "    sources=[\"https://openai.com/blog\", \"Tech news aggregators\"]\n",
    ")\n",
    "\n",
    "print(\"ðŸ§‘ Human Feedback:\")\n",
    "print(f\"Verdict: {human_feedback.verdict}\")\n",
    "print(f\"Confidence: {human_feedback.confidence}%\")\n",
    "print(f\"Reasoning: {human_feedback.reasoning}\")\n",
    "if human_feedback.additional_context:\n",
    "    print(f\"Context: {human_feedback.additional_context}\")\n",
    "if human_feedback.sources:\n",
    "    print(f\"Sources: {', '.join(human_feedback.sources)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Complete Flow with Human-in-the-Loop\n",
    "\n",
    "Now let's see the complete system in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤– Testing Human-in-the-Loop BS Detector\n",
      "\n",
      "\n",
      "======================================================================\n",
      "Claim: \"The Boeing 747 has four engines\"\n",
      "======================================================================\n",
      "\n",
      "ðŸ“Š Final Result:\n",
      "Verdict: LEGITIMATE\n",
      "Confidence: 100%\n",
      "Human Reviewed: False\n",
      "\n",
      "======================================================================\n",
      "Claim: \"A major airline announced bankruptcy yesterday\"\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikpatel/Documents/GitHub/workshop-agents/.venv/lib/python3.13/site-packages/langchain_community/utilities/duckduckgo_search.py:63: RuntimeWarning: This package (`duckduckgo_search`) has been renamed to `ddgs`! Use `pip install ddgs` instead.\n",
      "  with DDGS() as ddgs:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Final Result:\n",
      "Verdict: BS\n",
      "Confidence: 90%\n",
      "Human Reviewed: False\n",
      "\n",
      "======================================================================\n",
      "Claim: \"Scientists discovered anti-gravity technology last week\"\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikpatel/Documents/GitHub/workshop-agents/.venv/lib/python3.13/site-packages/langchain_community/utilities/duckduckgo_search.py:63: RuntimeWarning: This package (`duckduckgo_search`) has been renamed to `ddgs`! Use `pip install ddgs` instead.\n",
      "  with DDGS() as ddgs:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Final Result:\n",
      "Verdict: BS\n",
      "Confidence: 90%\n",
      "Human Reviewed: False\n"
     ]
    }
   ],
   "source": [
    "# Test claims that might trigger human review\n",
    "test_claims = [\n",
    "    # Should be straightforward - no human review\n",
    "    \"The Boeing 747 has four engines\",\n",
    "    \n",
    "    # Ambiguous current event - might trigger review\n",
    "    \"A major airline announced bankruptcy yesterday\",\n",
    "    \n",
    "    # Extraordinary claim - should trigger review\n",
    "    \"Scientists discovered anti-gravity technology last week\",\n",
    "]\n",
    "\n",
    "print(\"ðŸ¤– Testing Human-in-the-Loop BS Detector\\n\")\n",
    "\n",
    "for claim in test_claims:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Claim: \\\"{claim}\\\"\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Check claim with human-in-the-loop\n",
    "    # For demo, we use simulated human input\n",
    "    result = check_claim_with_human_in_loop(claim)\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Final Result:\")\n",
    "    print(f\"Verdict: {result['verdict']}\")\n",
    "    print(f\"Confidence: {result['confidence']}%\")\n",
    "    print(f\"Human Reviewed: {result.get('human_reviewed', False)}\")\n",
    "    \n",
    "    if result.get('uncertainty_score'):\n",
    "        print(f\"Uncertainty Score: {result['uncertainty_score']:.2f}\")\n",
    "    \n",
    "    if result.get('human_feedback'):\n",
    "        print(f\"\\nHuman Feedback:\")\n",
    "        print(f\"  - Verdict: {result['human_feedback']['verdict']}\")\n",
    "        print(f\"  - Confidence: {result['human_feedback']['confidence']}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤– Testing Human-in-the-Loop BS Detector\n",
      "\n",
      "Using function: <function check_claim_with_human_in_loop at 0x114b54180>\n",
      "Module location: modules.m5_human_in_loop\n",
      "\n",
      "======================================================================\n",
      "Claim: \"The Boeing 747 has four engines\"\n",
      "======================================================================\n",
      "\n",
      "ðŸ“Š Final Result:\n",
      "Verdict: LEGITIMATE\n",
      "Confidence: 100%\n",
      "Human Reviewed: False\n",
      "Analyzing Agent: Technical Expert\n",
      "\n",
      "======================================================================\n",
      "Claim: \"A major airline announced bankruptcy yesterday\"\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikpatel/Documents/GitHub/workshop-agents/.venv/lib/python3.13/site-packages/langchain_community/utilities/duckduckgo_search.py:63: RuntimeWarning: This package (`duckduckgo_search`) has been renamed to `ddgs`! Use `pip install ddgs` instead.\n",
      "  with DDGS() as ddgs:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Final Result:\n",
      "Verdict: BS\n",
      "Confidence: 85%\n",
      "Human Reviewed: False\n",
      "Analyzing Agent: Current Events Expert (with tools)\n",
      "\n",
      "======================================================================\n",
      "Claim: \"Scientists discovered anti-gravity technology last week\"\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikpatel/Documents/GitHub/workshop-agents/.venv/lib/python3.13/site-packages/langchain_community/utilities/duckduckgo_search.py:63: RuntimeWarning: This package (`duckduckgo_search`) has been renamed to `ddgs`! Use `pip install ddgs` instead.\n",
      "  with DDGS() as ddgs:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Final Result:\n",
      "Verdict: BS\n",
      "Confidence: 90%\n",
      "Human Reviewed: False\n",
      "Analyzing Agent: Current Events Expert (with tools)\n"
     ]
    }
   ],
   "source": [
    "# Test claims that might trigger human review\n",
    "test_claims = [\n",
    "    # Should be straightforward - no human review\n",
    "    \"The Boeing 747 has four engines\",\n",
    "    \n",
    "    # Ambiguous current event - might trigger review\n",
    "    \"A major airline announced bankruptcy yesterday\",\n",
    "    \n",
    "    # Extraordinary claim - should trigger review\n",
    "    \"Scientists discovered anti-gravity technology last week\",\n",
    "]\n",
    "\n",
    "print(\"ðŸ¤– Testing Human-in-the-Loop BS Detector\\n\")\n",
    "\n",
    "# Add debugging\n",
    "print(f\"Using function: {check_claim_with_human_in_loop}\")\n",
    "print(f\"Module location: {check_claim_with_human_in_loop.__module__}\")\n",
    "\n",
    "for claim in test_claims:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Claim: \\\"{claim}\\\"\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    try:\n",
    "        # Check claim with human-in-the-loop\n",
    "        # For demo, we use simulated human input\n",
    "        result = check_claim_with_human_in_loop(claim)\n",
    "        \n",
    "        print(f\"\\nðŸ“Š Final Result:\")\n",
    "        print(f\"Verdict: {result['verdict']}\")\n",
    "        print(f\"Confidence: {result['confidence']}%\")\n",
    "        print(f\"Human Reviewed: {result.get('human_reviewed', False)}\")\n",
    "        print(f\"Analyzing Agent: {result.get('analyzing_agent', 'N/A')}\")\n",
    "        \n",
    "        if result.get('uncertainty_score'):\n",
    "            print(f\"Uncertainty Score: {result['uncertainty_score']:.2f}\")\n",
    "        \n",
    "        if result.get('human_feedback'):\n",
    "            print(f\"\\nHuman Feedback:\")\n",
    "            print(f\"  - Verdict: {result['human_feedback']['verdict']}\")\n",
    "            print(f\"  - Confidence: {result['human_feedback']['confidence']}%\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ Error: {type(e).__name__}: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Different Scenarios\n",
    "\n",
    "Let's test claims with different characteristics to see when human review is triggered:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¬ Testing Human Review Triggering\n",
      "\n",
      "Uncertainty Score: 0.40\n",
      "Needs Human Review: True\n",
      "\n",
      "Review Reasons:\n",
      "  - Very low confidence: 30%\n",
      "\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ðŸ¤” HUMAN REVIEW REQUESTED\n",
      "============================================================\n",
      "\n",
      "**Claim**: Some ambiguous aviation claim that's hard to verify\n",
      "\n",
      "**AI Assessment**:\n",
      "- Verdict: BS\n",
      "- Confidence: 30%\n",
      "- Reasoning: Highly uncertain about this claim\n",
      "\n",
      "**Uncertainty Reasons**:\n",
      "- Very low confidence: 30%\n",
      "\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's force a human review by creating a low-confidence scenario\n",
    "print(\"ðŸ”¬ Testing Human Review Triggering\\n\")\n",
    "\n",
    "# Create a state with very low confidence to trigger review\n",
    "from modules.m5_human_in_loop import uncertainty_detector_node\n",
    "\n",
    "test_state = HumanInLoopState(\n",
    "    claim=\"Some ambiguous aviation claim that's hard to verify\",\n",
    "    verdict=\"BS\",\n",
    "    confidence=30,  # Very low confidence\n",
    "    reasoning=\"Highly uncertain about this claim\",\n",
    "    claim_type=\"general\",\n",
    "    analyzing_agent=\"General Expert\"\n",
    ")\n",
    "\n",
    "# Run uncertainty detection\n",
    "updates = uncertainty_detector_node(test_state)\n",
    "\n",
    "print(f\"Uncertainty Score: {updates['uncertainty_score']:.2f}\")\n",
    "print(f\"Needs Human Review: {updates['needs_human_review']}\")\n",
    "if updates.get('review_reasons'):\n",
    "    print(f\"\\nReview Reasons:\")\n",
    "    for reason in updates['review_reasons']:\n",
    "        print(f\"  - {reason}\")\n",
    "\n",
    "if updates.get('human_review_request'):\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(updates['human_review_request'].format_for_human())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ® Interactive Human-in-the-Loop Demo\n",
      "You'll be asked to review claims when the AI is uncertain.\n",
      "\n",
      "Testing claim: \"The new supersonic passenger jet breaks physics laws\"\n",
      "\n",
      "\n",
      "============================================================\n",
      "Final Decision:\n",
      "Verdict: BS\n",
      "Confidence: 95%\n",
      "Reasoning: The claim that a new supersonic passenger jet \"breaks physics laws\" is highly unlikely and technically inaccurate. Aviation design and operation are strictly governed by the fundamental laws of physics, including aerodynamics, thermodynamics, and materials science. Supersonic flight is well understood and has been achieved beforeâ€”most notably by the Concorde and military jetsâ€”without violating any physical laws.\n",
      "\n",
      "Any new supersonic passenger jet must operate within the constraints of known physics, such as overcoming shockwave formation, managing heat generated by air friction at high speeds, and maintaining structural integrity under high dynamic pressures. While new technological advancements can improve efficiency, reduce sonic booms, or enhance materials, they do not and cannot contravene physical laws.\n",
      "\n",
      "Therefore, the claim is either sensational marketing language or a misunderstanding of the technology. No credible engineering or scientific evidence supports the notion that any aircraft can break the fundamental laws of physics.\n"
     ]
    }
   ],
   "source": [
    "# Interactive demo with real human input\n",
    "print(\"ðŸŽ® Interactive Human-in-the-Loop Demo\")\n",
    "print(\"You'll be asked to review claims when the AI is uncertain.\\n\")\n",
    "\n",
    "# Test with a claim that should trigger review\n",
    "ambiguous_claim = \"The new supersonic passenger jet breaks physics laws\"\n",
    "\n",
    "print(f\"Testing claim: \\\"{ambiguous_claim}\\\"\\n\")\n",
    "\n",
    "# Use interactive handler\n",
    "# Note: In Jupyter, this will create input fields\n",
    "result = check_claim_with_human_in_loop(\n",
    "    ambiguous_claim,\n",
    "    human_input_handler=interactive_human_input\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Final Decision:\")\n",
    "print(f\"Verdict: {result['verdict']}\")\n",
    "print(f\"Confidence: {result['confidence']}%\")\n",
    "print(f\"Reasoning: {result['reasoning']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ® Interactive Human-in-the-Loop Demo\n",
      "You'll be asked to review claims when the AI is uncertain.\n",
      "\n",
      "Testing claim: \"The new supersonic passenger jet breaks physics laws\"\n",
      "\n",
      "\n",
      "============================================================\n",
      "Final Decision:\n",
      "Verdict: BS\n",
      "Confidence: 95%\n",
      "Reasoning: The claim that a supersonic passenger jet \"breaks physics laws\" is almost certainly false. Supersonic flight is well-understood and has been achieved multiple times, notably with the Concorde and various military aircraft. The physics governing supersonic flight are based on well-established principles of aerodynamics, thermodynamics, and materials science. While new supersonic jets may incorporate advanced technologiesâ€”such as improved engine designs, aerodynamic shaping, and noise reduction techniquesâ€”these advancements work within the constraints of physical laws rather than violating them. \n",
      "\n",
      "If a jet purportedly \"breaks physics laws,\" it would imply impossible phenomena like exceeding the speed of sound without the expected sonic boom, or achieving propulsion without reaction forces, which contradicts Newtonian mechanics and fluid dynamics. Any credible supersonic design complies with these principles; otherwise, it would not be feasible or demonstrable.\n",
      "\n",
      "Therefore, the claim is a hyperbolic or misleading statement rather than a legitimate technical assertion.\n"
     ]
    }
   ],
   "source": [
    "# Interactive demo with real human input\n",
    "print(\"ðŸŽ® Interactive Human-in-the-Loop Demo\")\n",
    "print(\"You'll be asked to review claims when the AI is uncertain.\\n\")\n",
    "\n",
    "# Test with a claim that should trigger review\n",
    "ambiguous_claim = \"The new supersonic passenger jet breaks physics laws\"\n",
    "\n",
    "print(f\"Testing claim: \\\"{ambiguous_claim}\\\"\\n\")\n",
    "\n",
    "# For non-interactive demo, we'll use a mock handler\n",
    "def mock_human_input(request: HumanReviewRequest) -> HumanFeedback:\n",
    "    print(\"ðŸ¤– Simulating human review...\")\n",
    "    return HumanFeedback(\n",
    "        verdict=\"BS\",\n",
    "        confidence=95,\n",
    "        reasoning=\"Laws of physics cannot be broken. Likely hyperbolic marketing claim.\",\n",
    "        additional_context=\"Supersonic jets must still obey aerodynamics and thermodynamics\"\n",
    "    )\n",
    "\n",
    "result = check_claim_with_human_in_loop(\n",
    "    ambiguous_claim,\n",
    "    human_input_handler=mock_human_input\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Final Decision:\")\n",
    "print(f\"Verdict: {result['verdict']}\")\n",
    "print(f\"Confidence: {result['confidence']}%\")\n",
    "print(f\"Reasoning: {result['reasoning']}\")\n",
    "\n",
    "# To use interactive input in Jupyter, uncomment below:\n",
    "result = check_claim_with_human_in_loop(\n",
    "    ambiguous_claim,\n",
    "    human_input_handler=interactive_human_input\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from typing import Optional\n",
    "import uuid\n",
    "\n",
    "class AsyncHumanReviewQueue:\n",
    "    \"\"\"Async queue for human review requests\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.pending_reviews = {}\n",
    "        self.completed_reviews = {}\n",
    "    \n",
    "    async def request_review(self, review_request: HumanReviewRequest) -> str:\n",
    "        \"\"\"Submit a review request and get a ticket ID\"\"\"\n",
    "        ticket_id = str(uuid.uuid4())\n",
    "        self.pending_reviews[ticket_id] = {\n",
    "            \"request\": review_request,\n",
    "            \"status\": \"pending\",\n",
    "            \"submitted_at\": datetime.now()\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nðŸ“‹ Review Request Submitted\")\n",
    "        print(f\"Ticket ID: {ticket_id}\")\n",
    "        print(f\"Claim: {review_request.claim[:50]}...\")\n",
    "        print(f\"Status: Pending human review\")\n",
    "        \n",
    "        return ticket_id\n",
    "    \n",
    "    async def check_review_status(self, ticket_id: str) -> Optional[HumanFeedback]:\n",
    "        \"\"\"Check if review is complete\"\"\"\n",
    "        if ticket_id in self.completed_reviews:\n",
    "            return self.completed_reviews[ticket_id][\"feedback\"]\n",
    "        elif ticket_id in self.pending_reviews:\n",
    "            return None\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown ticket ID: {ticket_id}\")\n",
    "    \n",
    "    async def submit_feedback(self, ticket_id: str, feedback: HumanFeedback):\n",
    "        \"\"\"Submit human feedback for a review request\"\"\"\n",
    "        if ticket_id not in self.pending_reviews:\n",
    "            raise ValueError(f\"Unknown ticket ID: {ticket_id}\")\n",
    "        \n",
    "        review = self.pending_reviews.pop(ticket_id)\n",
    "        review[\"status\"] = \"completed\"\n",
    "        review[\"feedback\"] = feedback\n",
    "        review[\"completed_at\"] = datetime.now()\n",
    "        \n",
    "        self.completed_reviews[ticket_id] = review\n",
    "        print(f\"âœ… Review completed for ticket {ticket_id}\")\n",
    "\n",
    "# Demo async pattern\n",
    "async def demo_async_review():\n",
    "    queue = AsyncHumanReviewQueue()\n",
    "    \n",
    "    # Submit review request\n",
    "    review_req = HumanReviewRequest(\n",
    "        claim=\"Quantum computers solved P=NP yesterday\",\n",
    "        ai_verdict=\"BS\",\n",
    "        ai_confidence=30,\n",
    "        uncertainty_reasons=[\"Extraordinary claim\", \"Very low confidence\"]\n",
    "    )\n",
    "    \n",
    "    ticket_id = await queue.request_review(review_req)\n",
    "    \n",
    "    # Check status (would be None initially)\n",
    "    status = await queue.check_review_status(ticket_id)\n",
    "    print(f\"\\nInitial status: {status}\")\n",
    "    \n",
    "    # Simulate human providing feedback\n",
    "    print(\"\\nâ³ Simulating human review...\")\n",
    "    await asyncio.sleep(1)\n",
    "    \n",
    "    feedback = HumanFeedback(\n",
    "        verdict=\"BS\",\n",
    "        confidence=99,\n",
    "        reasoning=\"P=NP remains unsolved. This would be worldwide news.\"\n",
    "    )\n",
    "    \n",
    "    await queue.submit_feedback(ticket_id, feedback)\n",
    "    \n",
    "    # Check status again\n",
    "    final_feedback = await queue.check_review_status(ticket_id)\n",
    "    print(f\"\\nFinal feedback: {final_feedback.verdict} ({final_feedback.confidence}%)\")\n",
    "\n",
    "# Run async demo\n",
    "await demo_async_review()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from typing import Optional\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "\n",
    "class AsyncHumanReviewQueue:\n",
    "    \"\"\"Async queue for human review requests\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.pending_reviews = {}\n",
    "        self.completed_reviews = {}\n",
    "    \n",
    "    async def request_review(self, review_request: HumanReviewRequest) -> str:\n",
    "        \"\"\"Submit a review request and get a ticket ID\"\"\"\n",
    "        ticket_id = str(uuid.uuid4())\n",
    "        self.pending_reviews[ticket_id] = {\n",
    "            \"request\": review_request,\n",
    "            \"status\": \"pending\",\n",
    "            \"submitted_at\": datetime.now()\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nðŸ“‹ Review Request Submitted\")\n",
    "        print(f\"Ticket ID: {ticket_id}\")\n",
    "        print(f\"Claim: {review_request.claim[:50]}...\")\n",
    "        print(f\"Status: Pending human review\")\n",
    "        \n",
    "        return ticket_id\n",
    "    \n",
    "    async def check_review_status(self, ticket_id: str) -> Optional[HumanFeedback]:\n",
    "        \"\"\"Check if review is complete\"\"\"\n",
    "        if ticket_id in self.completed_reviews:\n",
    "            return self.completed_reviews[ticket_id][\"feedback\"]\n",
    "        elif ticket_id in self.pending_reviews:\n",
    "            return None\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown ticket ID: {ticket_id}\")\n",
    "    \n",
    "    async def submit_feedback(self, ticket_id: str, feedback: HumanFeedback):\n",
    "        \"\"\"Submit human feedback for a review request\"\"\"\n",
    "        if ticket_id not in self.pending_reviews:\n",
    "            raise ValueError(f\"Unknown ticket ID: {ticket_id}\")\n",
    "        \n",
    "        review = self.pending_reviews.pop(ticket_id)\n",
    "        review[\"status\"] = \"completed\"\n",
    "        review[\"feedback\"] = feedback\n",
    "        review[\"completed_at\"] = datetime.now()\n",
    "        \n",
    "        self.completed_reviews[ticket_id] = review\n",
    "        print(f\"âœ… Review completed for ticket {ticket_id}\")\n",
    "\n",
    "# Demo async pattern\n",
    "async def demo_async_review():\n",
    "    queue = AsyncHumanReviewQueue()\n",
    "    \n",
    "    # Submit review request\n",
    "    review_req = HumanReviewRequest(\n",
    "        claim=\"Quantum computers solved P=NP yesterday\",\n",
    "        ai_verdict=\"BS\",\n",
    "        ai_confidence=30,\n",
    "        uncertainty_reasons=[\"Extraordinary claim\", \"Very low confidence\"]\n",
    "    )\n",
    "    \n",
    "    ticket_id = await queue.request_review(review_req)\n",
    "    \n",
    "    # Check status (would be None initially)\n",
    "    status = await queue.check_review_status(ticket_id)\n",
    "    print(f\"\\nInitial status: {status}\")\n",
    "    \n",
    "    # Simulate human providing feedback\n",
    "    print(\"\\nâ³ Simulating human review...\")\n",
    "    await asyncio.sleep(1)\n",
    "    \n",
    "    feedback = HumanFeedback(\n",
    "        verdict=\"BS\",\n",
    "        confidence=99,\n",
    "        reasoning=\"P=NP remains unsolved. This would be worldwide news.\"\n",
    "    )\n",
    "    \n",
    "    await queue.submit_feedback(ticket_id, feedback)\n",
    "    \n",
    "    # Check status again\n",
    "    final_feedback = await queue.check_review_status(ticket_id)\n",
    "    print(f\"\\nFinal feedback: {final_feedback.verdict} ({final_feedback.confidence}%)\")\n",
    "\n",
    "# Run async demo\n",
    "try:\n",
    "    await demo_async_review()\n",
    "except RuntimeError:\n",
    "    # If not in async context, create one\n",
    "    asyncio.run(demo_async_review())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from IPython.display import Image\n",
    "\n",
    "# Mermaid diagram of the flow\n",
    "mermaid_diagram = \"\"\"\n",
    "graph TD\n",
    "    A[Claim Input] --> R{Router}\n",
    "    \n",
    "    R -->|Technical| TE[Technical Expert]\n",
    "    R -->|Historical| HE[Historical Expert]\n",
    "    R -->|Current Event| CE[Current Events Expert<br/>+ Web Search]\n",
    "    R -->|General| GE[General Expert]\n",
    "    \n",
    "    TE --> UD{Uncertainty<br/>Detector}\n",
    "    HE --> UD\n",
    "    CE --> UD\n",
    "    GE --> UD\n",
    "    \n",
    "    UD -->|High Uncertainty| HR[Human Review<br/>Request]\n",
    "    UD -->|Low Uncertainty| FO[Format Output]\n",
    "    \n",
    "    HR --> HF[Human<br/>Feedback]\n",
    "    HF --> FO\n",
    "    \n",
    "    FO --> V[Final Verdict]\n",
    "    \n",
    "    style HR fill:#ff9999\n",
    "    style HF fill:#99ff99\n",
    "    style UD fill:#ffcc99\n",
    "\"\"\"\n",
    "\n",
    "def render_mermaid_diagram(graph_def):\n",
    "    graph_bytes = graph_def.encode(\"utf-8\")\n",
    "    base64_string = base64.b64encode(graph_bytes).decode(\"ascii\")\n",
    "    image_url = f\"https://mermaid.ink/img/{base64_string}?type=png\"\n",
    "    return Image(url=image_url)\n",
    "\n",
    "print(\"ðŸ“Š Human-in-the-Loop Flow:\")\n",
    "render_mermaid_diagram(mermaid_diagram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Production Considerations\n",
    "\n",
    "### Key Design Decisions:\n",
    "\n",
    "1. **When to Ask for Help**\n",
    "   - Confidence thresholds\n",
    "   - Expert disagreement\n",
    "   - Claim categories\n",
    "   - Business rules\n",
    "\n",
    "2. **User Experience**\n",
    "   - Clear presentation of uncertainty\n",
    "   - Relevant context provided\n",
    "   - Easy feedback interface\n",
    "   - Response time expectations\n",
    "\n",
    "3. **Async Patterns**\n",
    "   - Queue-based systems\n",
    "   - Webhook notifications\n",
    "   - Polling vs push\n",
    "   - Timeout handling\n",
    "\n",
    "4. **Feedback Loop**\n",
    "   - Store human decisions\n",
    "   - Learn from corrections\n",
    "   - Improve uncertainty detection\n",
    "   - Track reviewer accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example metrics tracking\n",
    "class HumanReviewMetrics:\n",
    "    \"\"\"Track metrics for human review system\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.total_reviews = 0\n",
    "        self.review_triggers = {}\n",
    "        self.agreement_rate = 0\n",
    "        self.avg_response_time = 0\n",
    "    \n",
    "    def track_review(self, request: HumanReviewRequest, feedback: HumanFeedback):\n",
    "        self.total_reviews += 1\n",
    "        \n",
    "        # Track trigger reasons\n",
    "        for reason in request.uncertainty_reasons:\n",
    "            self.review_triggers[reason] = self.review_triggers.get(reason, 0) + 1\n",
    "        \n",
    "        # Track agreement\n",
    "        if request.ai_verdict == feedback.verdict:\n",
    "            self.agreement_rate = (\n",
    "                (self.agreement_rate * (self.total_reviews - 1) + 1) \n",
    "                / self.total_reviews\n",
    "            )\n",
    "        else:\n",
    "            self.agreement_rate = (\n",
    "                (self.agreement_rate * (self.total_reviews - 1)) \n",
    "                / self.total_reviews\n",
    "            )\n",
    "    \n",
    "    def get_summary(self):\n",
    "        return {\n",
    "            \"total_reviews\": self.total_reviews,\n",
    "            \"agreement_rate\": f\"{self.agreement_rate:.1%}\",\n",
    "            \"top_triggers\": sorted(\n",
    "                self.review_triggers.items(), \n",
    "                key=lambda x: x[1], \n",
    "                reverse=True\n",
    "            )[:3]\n",
    "        }\n",
    "\n",
    "# Demo metrics\n",
    "metrics = HumanReviewMetrics()\n",
    "\n",
    "# Simulate some reviews\n",
    "test_reviews = [\n",
    "    (HumanReviewRequest(\n",
    "        claim=\"Test 1\",\n",
    "        ai_verdict=\"BS\",\n",
    "        uncertainty_reasons=[\"Low confidence: 40%\"]\n",
    "    ), HumanFeedback(verdict=\"BS\", confidence=90, reasoning=\"Confirmed BS\")),\n",
    "    \n",
    "    (HumanReviewRequest(\n",
    "        claim=\"Test 2\",\n",
    "        ai_verdict=\"LEGITIMATE\",\n",
    "        uncertainty_reasons=[\"Expert disagreement\", \"Low confidence: 45%\"]\n",
    "    ), HumanFeedback(verdict=\"BS\", confidence=85, reasoning=\"Actually false\")),\n",
    "]\n",
    "\n",
    "for req, feedback in test_reviews:\n",
    "    metrics.track_review(req, feedback)\n",
    "\n",
    "print(\"ðŸ“Š Human Review Metrics:\")\n",
    "summary = metrics.get_summary()\n",
    "print(f\"Total Reviews: {summary['total_reviews']}\")\n",
    "print(f\"AI-Human Agreement: {summary['agreement_rate']}\")\n",
    "print(\"\\nTop Review Triggers:\")\n",
    "for trigger, count in summary['top_triggers']:\n",
    "    print(f\"  - {trigger}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What We Built:\n",
    "1. **Uncertainty Detection** - Multi-factor scoring system\n",
    "2. **Human Review Requests** - Clear, informative format\n",
    "3. **Feedback Integration** - Structured human input\n",
    "4. **Async Patterns** - Non-blocking review queue\n",
    "\n",
    "### Key Takeaways:\n",
    "- ðŸŽ¯ **Know When to Ask** - Clear criteria for human review\n",
    "- ðŸ“Š **Provide Context** - Give humans all relevant information  \n",
    "- âš¡ **Don't Block** - Async patterns for production systems\n",
    "- ðŸ“ˆ **Track & Learn** - Metrics to improve over time\n",
    "\n",
    "### Benefits:\n",
    "- âœ… Handles edge cases gracefully\n",
    "- âœ… Builds user trust\n",
    "- âœ… Enables continuous improvement\n",
    "- âœ… Meets compliance requirements\n",
    "\n",
    "### Next Steps:\n",
    "In Iteration 6, we'll add **Memory** so our system can:\n",
    "- Remember past human feedback\n",
    "- Learn from previous decisions\n",
    "- Build a knowledge base over time\n",
    "- Reduce repeated human reviews"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
