{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iteration 5: Human-in-the-Loop\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this iteration, we add human review capabilities for cases where AI needs help. This is crucial for:\n",
    "- Handling edge cases beyond AI capabilities\n",
    "- Building user trust through transparency\n",
    "- Continuous improvement through feedback\n",
    "- Meeting compliance/regulatory requirements\n",
    "\n",
    "## What You'll Learn\n",
    "1. **Uncertainty Detection** - When should AI ask for help?\n",
    "2. **Human Review Interface** - How to present info for human decision\n",
    "3. **Feedback Integration** - Incorporating human input into the flow\n",
    "4. **Async Patterns** - Handling human input without blocking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Concept: When and Why to Ask Humans\n",
    "\n",
    "### When to Request Human Review:\n",
    "1. **Low Confidence** (<50%) - AI is unsure\n",
    "2. **Expert Disagreement** - Different agents reach different conclusions\n",
    "3. **High Stakes** - Critical decisions need human oversight\n",
    "4. **No Evidence** - Claims that can't be verified with available tools\n",
    "5. **Explicit Request** - User wants human verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Imports successful!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from modules.m5_human_in_loop import (\n",
    "    HumanReviewRequest,\n",
    "    HumanFeedback,\n",
    "    HumanInLoopState,\n",
    "    calculate_uncertainty,\n",
    "    check_claim_with_human_in_loop,\n",
    "    interactive_human_input,\n",
    "    create_human_in_loop_bs_detector\n",
    ")\n",
    "from config.llm_factory import LLMFactory\n",
    "import json\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "print(\"âœ… Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding Uncertainty Detection\n",
    "\n",
    "First, let's see how we calculate uncertainty scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Uncertainty Analysis:\n",
      "\n",
      "Test 1: The Boeing 747 has four engines...\n",
      "  Confidence: 95%\n",
      "  Uncertainty Score: 0.00\n",
      "  Needs Review: No\n",
      "\n",
      "Test 2: Some new aviation technology was announced...\n",
      "  Confidence: 40%\n",
      "  Uncertainty Score: 0.50\n",
      "  Needs Review: No\n",
      "\n",
      "Test 3: Planes will be fully autonomous by 2025...\n",
      "  Confidence: 70%\n",
      "  Uncertainty Score: 0.30\n",
      "  Needs Review: No\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create test states with different uncertainty levels\n",
    "test_states = [\n",
    "    # High confidence, clear verdict\n",
    "    HumanInLoopState(\n",
    "        claim=\"The Boeing 747 has four engines\",\n",
    "        verdict=\"LEGITIMATE\", \n",
    "        confidence=95,\n",
    "        claim_type=\"technical\"\n",
    "    ),\n",
    "    \n",
    "    # Low confidence\n",
    "    HumanInLoopState(\n",
    "        claim=\"Some new aviation technology was announced\",\n",
    "        verdict=\"LEGITIMATE\",\n",
    "        confidence=40,\n",
    "        claim_type=\"current_event\"\n",
    "    ),\n",
    "    \n",
    "    # Expert disagreement\n",
    "    HumanInLoopState(\n",
    "        claim=\"Planes will be fully autonomous by 2025\",\n",
    "        verdict=\"BS\",\n",
    "        confidence=70,\n",
    "        expert_opinions={\n",
    "            \"technical_expert\": {\"verdict\": \"BS\", \"confidence\": 80},\n",
    "            \"general_expert\": {\"verdict\": \"LEGITIMATE\", \"confidence\": 60}\n",
    "        }\n",
    "    )\n",
    "]\n",
    "\n",
    "# Calculate uncertainty for each\n",
    "print(\"ðŸ” Uncertainty Analysis:\\n\")\n",
    "for i, state in enumerate(test_states, 1):\n",
    "    uncertainty = calculate_uncertainty(state)\n",
    "    print(f\"Test {i}: {state.claim[:50]}...\")\n",
    "    print(f\"  Confidence: {state.confidence}%\")\n",
    "    print(f\"  Uncertainty Score: {uncertainty:.2f}\")\n",
    "    print(f\"  Needs Review: {'Yes' if uncertainty > 0.6 else 'No'}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Human Review Request Format\n",
    "\n",
    "When we need human input, we present all relevant information clearly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ðŸ¤” HUMAN REVIEW REQUESTED\n",
      "============================================================\n",
      "\n",
      "**Claim**: ChatGPT-5 was released yesterday with consciousness\n",
      "\n",
      "**AI Assessment**:\n",
      "- Verdict: BS\n",
      "- Confidence: 45%\n",
      "- Reasoning: Claim about consciousness is dubious and no official announcement found\n",
      "\n",
      "**Uncertainty Reasons**:\n",
      "- Very low confidence: 45%\n",
      "- No evidence found for recent event\n",
      "- Extraordinary claim requiring extraordinary evidence\n",
      "\n",
      "**Expert Opinions**:\n",
      "\n",
      "Current Events Expert:\n",
      "  - Verdict: BS\n",
      "  - Confidence: 45%\n",
      "  - Reasoning: No official announcement from OpenAI found\n",
      "\n",
      "Technical Expert:\n",
      "  - Verdict: BS\n",
      "  - Confidence: 90%\n",
      "  - Reasoning: Claims of consciousness are not scientifically supported\n",
      "\n",
      "**Search Results**: 2 results found\n",
      "  1. OpenAI has not announced ChatGPT-5 as of latest search...\n",
      "  2. No credible sources report AI consciousness breakthrough...\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a sample review request\n",
    "review_request = HumanReviewRequest(\n",
    "    claim=\"ChatGPT-5 was released yesterday with consciousness\",\n",
    "    ai_verdict=\"BS\",\n",
    "    ai_confidence=45,\n",
    "    ai_reasoning=\"Claim about consciousness is dubious and no official announcement found\",\n",
    "    uncertainty_reasons=[\n",
    "        \"Very low confidence: 45%\",\n",
    "        \"No evidence found for recent event\",\n",
    "        \"Extraordinary claim requiring extraordinary evidence\"\n",
    "    ],\n",
    "    expert_opinions={\n",
    "        \"Current Events Expert\": {\n",
    "            \"verdict\": \"BS\",\n",
    "            \"confidence\": 45,\n",
    "            \"reasoning\": \"No official announcement from OpenAI found\"\n",
    "        },\n",
    "        \"Technical Expert\": {\n",
    "            \"verdict\": \"BS\", \n",
    "            \"confidence\": 90,\n",
    "            \"reasoning\": \"Claims of consciousness are not scientifically supported\"\n",
    "        }\n",
    "    },\n",
    "    search_results=[\n",
    "        {\"fact\": \"OpenAI has not announced ChatGPT-5 as of latest search\"},\n",
    "        {\"fact\": \"No credible sources report AI consciousness breakthrough\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Display formatted request\n",
    "print(review_request.format_for_human())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Simulated Human Feedback\n",
    "\n",
    "Let's see how human feedback is structured:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§‘ Human Feedback:\n",
      "Verdict: BS\n",
      "Confidence: 95%\n",
      "Reasoning: No such announcement exists. ChatGPT-5 has not been released.\n",
      "Context: I checked OpenAI's official channels and tech news sites.\n",
      "Sources: https://openai.com/blog, Tech news aggregators\n"
     ]
    }
   ],
   "source": [
    "# Example human feedback\n",
    "human_feedback = HumanFeedback(\n",
    "    verdict=\"BS\",\n",
    "    confidence=95,\n",
    "    reasoning=\"No such announcement exists. ChatGPT-5 has not been released.\",\n",
    "    additional_context=\"I checked OpenAI's official channels and tech news sites.\",\n",
    "    sources=[\"https://openai.com/blog\", \"Tech news aggregators\"]\n",
    ")\n",
    "\n",
    "print(\"ðŸ§‘ Human Feedback:\")\n",
    "print(f\"Verdict: {human_feedback.verdict}\")\n",
    "print(f\"Confidence: {human_feedback.confidence}%\")\n",
    "print(f\"Reasoning: {human_feedback.reasoning}\")\n",
    "if human_feedback.additional_context:\n",
    "    print(f\"Context: {human_feedback.additional_context}\")\n",
    "if human_feedback.sources:\n",
    "    print(f\"Sources: {', '.join(human_feedback.sources)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Complete Flow with Human-in-the-Loop\n",
    "\n",
    "Now let's see the complete system in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤– Testing Human-in-the-Loop BS Detector\n",
      "\n",
      "\n",
      "======================================================================\n",
      "Claim: \"The Boeing 747 has four engines\"\n",
      "======================================================================\n",
      "\n",
      "ðŸ“Š Final Result:\n",
      "Verdict: ERROR\n",
      "Confidence: 0%\n",
      "Human Reviewed: False\n",
      "\n",
      "======================================================================\n",
      "Claim: \"A major airline announced bankruptcy yesterday\"\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikpatel/Documents/GitHub/workshop-agents/.venv/lib/python3.13/site-packages/langchain_community/utilities/duckduckgo_search.py:63: RuntimeWarning: This package (`duckduckgo_search`) has been renamed to `ddgs`! Use `pip install ddgs` instead.\n",
      "  with DDGS() as ddgs:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Final Result:\n",
      "Verdict: ERROR\n",
      "Confidence: 0%\n",
      "Human Reviewed: False\n",
      "\n",
      "======================================================================\n",
      "Claim: \"Scientists discovered anti-gravity technology last week\"\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikpatel/Documents/GitHub/workshop-agents/.venv/lib/python3.13/site-packages/langchain_community/utilities/duckduckgo_search.py:63: RuntimeWarning: This package (`duckduckgo_search`) has been renamed to `ddgs`! Use `pip install ddgs` instead.\n",
      "  with DDGS() as ddgs:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Final Result:\n",
      "Verdict: ERROR\n",
      "Confidence: 0%\n",
      "Human Reviewed: False\n"
     ]
    }
   ],
   "source": [
    "# Test claims that might trigger human review\n",
    "test_claims = [\n",
    "    # Should be straightforward - no human review\n",
    "    \"The Boeing 747 has four engines\",\n",
    "    \n",
    "    # Ambiguous current event - might trigger review\n",
    "    \"A major airline announced bankruptcy yesterday\",\n",
    "    \n",
    "    # Extraordinary claim - should trigger review\n",
    "    \"Scientists discovered anti-gravity technology last week\",\n",
    "]\n",
    "\n",
    "print(\"ðŸ¤– Testing Human-in-the-Loop BS Detector\\n\")\n",
    "\n",
    "for claim in test_claims:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Claim: \\\"{claim}\\\"\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Check claim with human-in-the-loop\n",
    "    # For demo, we use simulated human input\n",
    "    result = check_claim_with_human_in_loop(claim)\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Final Result:\")\n",
    "    print(f\"Verdict: {result['verdict']}\")\n",
    "    print(f\"Confidence: {result['confidence']}%\")\n",
    "    print(f\"Human Reviewed: {result.get('human_reviewed', False)}\")\n",
    "    \n",
    "    if result.get('uncertainty_score'):\n",
    "        print(f\"Uncertainty Score: {result['uncertainty_score']:.2f}\")\n",
    "    \n",
    "    if result.get('human_feedback'):\n",
    "        print(f\"\\nHuman Feedback:\")\n",
    "        print(f\"  - Verdict: {result['human_feedback']['verdict']}\")\n",
    "        print(f\"  - Confidence: {result['human_feedback']['confidence']}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Interactive Human Review Demo\n",
    "\n",
    "Let's create an interactive demo where you can provide human feedback:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ® Interactive Human-in-the-Loop Demo\n",
      "You'll be asked to review claims when the AI is uncertain.\n",
      "\n",
      "Testing claim: \"The new supersonic passenger jet breaks physics laws\"\n",
      "\n",
      "\n",
      "============================================================\n",
      "Final Decision:\n",
      "Verdict: ERROR\n",
      "Confidence: 0%\n",
      "Reasoning: Processing failed\n"
     ]
    }
   ],
   "source": [
    "# Interactive demo with real human input\n",
    "print(\"ðŸŽ® Interactive Human-in-the-Loop Demo\")\n",
    "print(\"You'll be asked to review claims when the AI is uncertain.\\n\")\n",
    "\n",
    "# Test with a claim that should trigger review\n",
    "ambiguous_claim = \"The new supersonic passenger jet breaks physics laws\"\n",
    "\n",
    "print(f\"Testing claim: \\\"{ambiguous_claim}\\\"\\n\")\n",
    "\n",
    "# Use interactive handler\n",
    "# Note: In Jupyter, this will create input fields\n",
    "result = check_claim_with_human_in_loop(\n",
    "    ambiguous_claim,\n",
    "    human_input_handler=interactive_human_input\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Final Decision:\")\n",
    "print(f\"Verdict: {result['verdict']}\")\n",
    "print(f\"Confidence: {result['confidence']}%\")\n",
    "print(f\"Reasoning: {result['reasoning']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Async Human Review Pattern\n",
    "\n",
    "In production, human review should be asynchronous. Here's a pattern for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from typing import Optional\n",
    "import uuid\n",
    "\n",
    "class AsyncHumanReviewQueue:\n",
    "    \"\"\"Async queue for human review requests\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.pending_reviews = {}\n",
    "        self.completed_reviews = {}\n",
    "    \n",
    "    async def request_review(self, review_request: HumanReviewRequest) -> str:\n",
    "        \"\"\"Submit a review request and get a ticket ID\"\"\"\n",
    "        ticket_id = str(uuid.uuid4())\n",
    "        self.pending_reviews[ticket_id] = {\n",
    "            \"request\": review_request,\n",
    "            \"status\": \"pending\",\n",
    "            \"submitted_at\": datetime.now()\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nðŸ“‹ Review Request Submitted\")\n",
    "        print(f\"Ticket ID: {ticket_id}\")\n",
    "        print(f\"Claim: {review_request.claim[:50]}...\")\n",
    "        print(f\"Status: Pending human review\")\n",
    "        \n",
    "        return ticket_id\n",
    "    \n",
    "    async def check_review_status(self, ticket_id: str) -> Optional[HumanFeedback]:\n",
    "        \"\"\"Check if review is complete\"\"\"\n",
    "        if ticket_id in self.completed_reviews:\n",
    "            return self.completed_reviews[ticket_id][\"feedback\"]\n",
    "        elif ticket_id in self.pending_reviews:\n",
    "            return None\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown ticket ID: {ticket_id}\")\n",
    "    \n",
    "    async def submit_feedback(self, ticket_id: str, feedback: HumanFeedback):\n",
    "        \"\"\"Submit human feedback for a review request\"\"\"\n",
    "        if ticket_id not in self.pending_reviews:\n",
    "            raise ValueError(f\"Unknown ticket ID: {ticket_id}\")\n",
    "        \n",
    "        review = self.pending_reviews.pop(ticket_id)\n",
    "        review[\"status\"] = \"completed\"\n",
    "        review[\"feedback\"] = feedback\n",
    "        review[\"completed_at\"] = datetime.now()\n",
    "        \n",
    "        self.completed_reviews[ticket_id] = review\n",
    "        print(f\"âœ… Review completed for ticket {ticket_id}\")\n",
    "\n",
    "# Demo async pattern\n",
    "async def demo_async_review():\n",
    "    queue = AsyncHumanReviewQueue()\n",
    "    \n",
    "    # Submit review request\n",
    "    review_req = HumanReviewRequest(\n",
    "        claim=\"Quantum computers solved P=NP yesterday\",\n",
    "        ai_verdict=\"BS\",\n",
    "        ai_confidence=30,\n",
    "        uncertainty_reasons=[\"Extraordinary claim\", \"Very low confidence\"]\n",
    "    )\n",
    "    \n",
    "    ticket_id = await queue.request_review(review_req)\n",
    "    \n",
    "    # Check status (would be None initially)\n",
    "    status = await queue.check_review_status(ticket_id)\n",
    "    print(f\"\\nInitial status: {status}\")\n",
    "    \n",
    "    # Simulate human providing feedback\n",
    "    print(\"\\nâ³ Simulating human review...\")\n",
    "    await asyncio.sleep(1)\n",
    "    \n",
    "    feedback = HumanFeedback(\n",
    "        verdict=\"BS\",\n",
    "        confidence=99,\n",
    "        reasoning=\"P=NP remains unsolved. This would be worldwide news.\"\n",
    "    )\n",
    "    \n",
    "    await queue.submit_feedback(ticket_id, feedback)\n",
    "    \n",
    "    # Check status again\n",
    "    final_feedback = await queue.check_review_status(ticket_id)\n",
    "    print(f\"\\nFinal feedback: {final_feedback.verdict} ({final_feedback.confidence}%)\")\n",
    "\n",
    "# Run async demo\n",
    "await demo_async_review()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualizing the Enhanced Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from IPython.display import Image\n",
    "\n",
    "# Mermaid diagram of the flow\n",
    "mermaid_diagram = \"\"\"\n",
    "graph TD\n",
    "    A[Claim Input] --> R{Router}\n",
    "    \n",
    "    R -->|Technical| TE[Technical Expert]\n",
    "    R -->|Historical| HE[Historical Expert]\n",
    "    R -->|Current Event| CE[Current Events Expert<br/>+ Web Search]\n",
    "    R -->|General| GE[General Expert]\n",
    "    \n",
    "    TE --> UD{Uncertainty<br/>Detector}\n",
    "    HE --> UD\n",
    "    CE --> UD\n",
    "    GE --> UD\n",
    "    \n",
    "    UD -->|High Uncertainty| HR[Human Review<br/>Request]\n",
    "    UD -->|Low Uncertainty| FO[Format Output]\n",
    "    \n",
    "    HR --> HF[Human<br/>Feedback]\n",
    "    HF --> FO\n",
    "    \n",
    "    FO --> V[Final Verdict]\n",
    "    \n",
    "    style HR fill:#ff9999\n",
    "    style HF fill:#99ff99\n",
    "    style UD fill:#ffcc99\n",
    "\"\"\"\n",
    "\n",
    "def render_mermaid_diagram(graph_def):\n",
    "    graph_bytes = graph_def.encode(\"utf-8\")\n",
    "    base64_string = base64.b64encode(graph_bytes).decode(\"ascii\")\n",
    "    image_url = f\"https://mermaid.ink/img/{base64_string}?type=png\"\n",
    "    return Image(url=image_url)\n",
    "\n",
    "print(\"ðŸ“Š Human-in-the-Loop Flow:\")\n",
    "render_mermaid_diagram(mermaid_diagram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Production Considerations\n",
    "\n",
    "### Key Design Decisions:\n",
    "\n",
    "1. **When to Ask for Help**\n",
    "   - Confidence thresholds\n",
    "   - Expert disagreement\n",
    "   - Claim categories\n",
    "   - Business rules\n",
    "\n",
    "2. **User Experience**\n",
    "   - Clear presentation of uncertainty\n",
    "   - Relevant context provided\n",
    "   - Easy feedback interface\n",
    "   - Response time expectations\n",
    "\n",
    "3. **Async Patterns**\n",
    "   - Queue-based systems\n",
    "   - Webhook notifications\n",
    "   - Polling vs push\n",
    "   - Timeout handling\n",
    "\n",
    "4. **Feedback Loop**\n",
    "   - Store human decisions\n",
    "   - Learn from corrections\n",
    "   - Improve uncertainty detection\n",
    "   - Track reviewer accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example metrics tracking\n",
    "class HumanReviewMetrics:\n",
    "    \"\"\"Track metrics for human review system\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.total_reviews = 0\n",
    "        self.review_triggers = {}\n",
    "        self.agreement_rate = 0\n",
    "        self.avg_response_time = 0\n",
    "    \n",
    "    def track_review(self, request: HumanReviewRequest, feedback: HumanFeedback):\n",
    "        self.total_reviews += 1\n",
    "        \n",
    "        # Track trigger reasons\n",
    "        for reason in request.uncertainty_reasons:\n",
    "            self.review_triggers[reason] = self.review_triggers.get(reason, 0) + 1\n",
    "        \n",
    "        # Track agreement\n",
    "        if request.ai_verdict == feedback.verdict:\n",
    "            self.agreement_rate = (\n",
    "                (self.agreement_rate * (self.total_reviews - 1) + 1) \n",
    "                / self.total_reviews\n",
    "            )\n",
    "        else:\n",
    "            self.agreement_rate = (\n",
    "                (self.agreement_rate * (self.total_reviews - 1)) \n",
    "                / self.total_reviews\n",
    "            )\n",
    "    \n",
    "    def get_summary(self):\n",
    "        return {\n",
    "            \"total_reviews\": self.total_reviews,\n",
    "            \"agreement_rate\": f\"{self.agreement_rate:.1%}\",\n",
    "            \"top_triggers\": sorted(\n",
    "                self.review_triggers.items(), \n",
    "                key=lambda x: x[1], \n",
    "                reverse=True\n",
    "            )[:3]\n",
    "        }\n",
    "\n",
    "# Demo metrics\n",
    "metrics = HumanReviewMetrics()\n",
    "\n",
    "# Simulate some reviews\n",
    "test_reviews = [\n",
    "    (HumanReviewRequest(\n",
    "        claim=\"Test 1\",\n",
    "        ai_verdict=\"BS\",\n",
    "        uncertainty_reasons=[\"Low confidence: 40%\"]\n",
    "    ), HumanFeedback(verdict=\"BS\", confidence=90, reasoning=\"Confirmed BS\")),\n",
    "    \n",
    "    (HumanReviewRequest(\n",
    "        claim=\"Test 2\",\n",
    "        ai_verdict=\"LEGITIMATE\",\n",
    "        uncertainty_reasons=[\"Expert disagreement\", \"Low confidence: 45%\"]\n",
    "    ), HumanFeedback(verdict=\"BS\", confidence=85, reasoning=\"Actually false\")),\n",
    "]\n",
    "\n",
    "for req, feedback in test_reviews:\n",
    "    metrics.track_review(req, feedback)\n",
    "\n",
    "print(\"ðŸ“Š Human Review Metrics:\")\n",
    "summary = metrics.get_summary()\n",
    "print(f\"Total Reviews: {summary['total_reviews']}\")\n",
    "print(f\"AI-Human Agreement: {summary['agreement_rate']}\")\n",
    "print(\"\\nTop Review Triggers:\")\n",
    "for trigger, count in summary['top_triggers']:\n",
    "    print(f\"  - {trigger}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What We Built:\n",
    "1. **Uncertainty Detection** - Multi-factor scoring system\n",
    "2. **Human Review Requests** - Clear, informative format\n",
    "3. **Feedback Integration** - Structured human input\n",
    "4. **Async Patterns** - Non-blocking review queue\n",
    "\n",
    "### Key Takeaways:\n",
    "- ðŸŽ¯ **Know When to Ask** - Clear criteria for human review\n",
    "- ðŸ“Š **Provide Context** - Give humans all relevant information  \n",
    "- âš¡ **Don't Block** - Async patterns for production systems\n",
    "- ðŸ“ˆ **Track & Learn** - Metrics to improve over time\n",
    "\n",
    "### Benefits:\n",
    "- âœ… Handles edge cases gracefully\n",
    "- âœ… Builds user trust\n",
    "- âœ… Enables continuous improvement\n",
    "- âœ… Meets compliance requirements\n",
    "\n",
    "### Next Steps:\n",
    "In Iteration 6, we'll add **Memory** so our system can:\n",
    "- Remember past human feedback\n",
    "- Learn from previous decisions\n",
    "- Build a knowledge base over time\n",
    "- Reduce repeated human reviews"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
