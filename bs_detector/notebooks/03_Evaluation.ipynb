{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03: Evaluation with DeepEval\n",
    "\n",
    "## Overview\n",
    "In this notebook, we'll add systematic evaluation to our BS detector. This is crucial for:\n",
    "- Measuring if our improvements actually work\n",
    "- Understanding where our detector fails\n",
    "- Guiding future enhancements\n",
    "\n",
    "## What We'll Learn\n",
    "1. Creating evaluation datasets\n",
    "2. Using DeepEval for LLM evaluation\n",
    "3. Custom metrics for domain-specific tasks\n",
    "4. Comparing iterations quantitatively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture Diagram\n",
    "\n",
    "Let's visualize how evaluation fits into our system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://mermaid.ink/img/CmdyYXBoIFRCCiAgICBzdWJncmFwaCAiVGVzdCBEYXRhc2V0IgogICAgICAgIFREW0F2aWF0aW9uIENsYWltczxici8+MzAgdGVzdCBjYXNlc10KICAgICAgICBURCAtLT4gRWFzeVtFYXN5IENsYWltczxici8+MTAgY2FzZXNdCiAgICAgICAgVEQgLS0+IE1lZGl1bVtNZWRpdW0gQ2xhaW1zPGJyLz4xMCBjYXNlc10KICAgICAgICBURCAtLT4gSGFyZFtIYXJkIENsYWltczxici8+MTAgY2FzZXNdCiAgICBlbmQKCiAgICBzdWJncmFwaCAiRGV0ZWN0b3JzIgogICAgICAgIEQxW0Jhc2VsaW5lIERldGVjdG9yPGJyLz5JdGVyYXRpb24gMV0KICAgICAgICBEMltMYW5nR3JhcGggRGV0ZWN0b3I8YnIvPkl0ZXJhdGlvbiAyXQogICAgZW5kCgogICAgc3ViZ3JhcGggIkV2YWx1YXRpb24gRnJhbWV3b3JrIgogICAgICAgIEVWW0JTRGV0ZWN0b3JFdmFsdWF0b3JdCiAgICAgICAgRVYgLS0+IE0xW0FjY3VyYWN5IE1ldHJpY10KICAgICAgICBFViAtLT4gTTJbQ29uZmlkZW5jZSBNZXRyaWNdCiAgICAgICAgRVYgLS0+IE0zW1JlYXNvbmluZyBNZXRyaWNdCiAgICBlbmQKCiAgICBzdWJncmFwaCAiUmVzdWx0cyIKICAgICAgICBSW0V2YWx1YXRpb25SZXN1bHRdCiAgICAgICAgUiAtLT4gUkFbQWNjdXJhY3kgYnkgRGlmZmljdWx0eV0KICAgICAgICBSIC0tPiBSQ1tDb25maWRlbmNlIEFuYWx5c2lzXQogICAgICAgIFIgLS0+IFJUW1Jlc3BvbnNlIFRpbWVzXQogICAgZW5kCgogICAgVEQgLS0+IEVWCiAgICBEMSAtLT4gRVYKICAgIEQyIC0tPiBFVgogICAgRVYgLS0+IFIKCiAgICBjbGFzc0RlZiBkYXRhc2V0IGZpbGw6I2Y5ZixzdHJva2U6IzMzMyxzdHJva2Utd2lkdGg6MnB4CiAgICBjbGFzc0RlZiBkZXRlY3RvciBmaWxsOiNiYmYsc3Ryb2tlOiMzMzMsc3Ryb2tlLXdpZHRoOjJweAogICAgY2xhc3NEZWYgZXZhbHVhdG9yIGZpbGw6I2JmYixzdHJva2U6IzMzMyxzdHJva2Utd2lkdGg6MnB4CiAgICBjbGFzc0RlZiByZXN1bHQgZmlsbDojZmJiLHN0cm9rZTojMzMzLHN0cm9rZS13aWR0aDoycHgKCiAgICBjbGFzcyBURCxFYXN5LE1lZGl1bSxIYXJkIGRhdGFzZXQKICAgIGNsYXNzIEQxLEQyIGRldGVjdG9yCiAgICBjbGFzcyBFVixNMSxNMixNMyBldmFsdWF0b3IKICAgIGNsYXNzIFIsUkEsUkMsUlQgcmVzdWx0Cg==?type=png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import base64\n",
    "from IPython.display import Image\n",
    "\n",
    "# Mermaid diagram showing evaluation flow\n",
    "evaluation_diagram = \"\"\"\n",
    "graph TB\n",
    "    subgraph \"Test Dataset\"\n",
    "        TD[Aviation Claims<br/>30 test cases]\n",
    "        TD --> Easy[Easy Claims<br/>4 cases]\n",
    "        TD --> Medium[Medium Claims<br/>11 cases]\n",
    "        TD --> Hard[Hard Claims<br/>15 cases]\n",
    "    end\n",
    "    \n",
    "    subgraph \"Detectors\"\n",
    "        D1[Baseline Detector<br/>Iteration 1]\n",
    "        D2[LangGraph Detector<br/>Iteration 2]\n",
    "    end\n",
    "    \n",
    "    subgraph \"Evaluation Framework\"\n",
    "        EV[BSDetectorEvaluator]\n",
    "        EV --> M1[Accuracy Metric]\n",
    "        EV --> M2[Confidence Metric]\n",
    "        EV --> M3[Reasoning Metric]\n",
    "    end\n",
    "    \n",
    "    subgraph \"Results\"\n",
    "        R[EvaluationResult]\n",
    "        R --> RA[Accuracy by Difficulty]\n",
    "        R --> RC[Confidence Analysis]\n",
    "        R --> RT[Response Times]\n",
    "    end\n",
    "    \n",
    "    TD --> EV\n",
    "    D1 --> EV\n",
    "    D2 --> EV\n",
    "    EV --> R\n",
    "    \n",
    "    classDef dataset fill:#f9f,stroke:#333,stroke-width:2px\n",
    "    classDef detector fill:#bbf,stroke:#333,stroke-width:2px\n",
    "    classDef evaluator fill:#bfb,stroke:#333,stroke-width:2px\n",
    "    classDef result fill:#fbb,stroke:#333,stroke-width:2px\n",
    "    \n",
    "    class TD,Easy,Medium,Hard dataset\n",
    "    class D1,D2 detector\n",
    "    class EV,M1,M2,M3 evaluator\n",
    "    class R,RA,RC,RT result\n",
    "\"\"\"\n",
    "\n",
    "def render_mermaid_diagram(graph_def):\n",
    "    \"\"\"Render a Mermaid diagram using mermaid.ink API\"\"\"\n",
    "    graph_bytes = graph_def.encode(\"utf-8\")\n",
    "    base64_string = base64.b64encode(graph_bytes).decode(\"ascii\")\n",
    "    image_url = f\"https://mermaid.ink/img/{base64_string}?type=png\"\n",
    "    return Image(url=image_url)\n",
    "\n",
    "render_mermaid_diagram(evaluation_diagram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import everything we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports successful!\n",
      "üïê Current time: 2025-07-21 21:18:38\n"
     ]
    }
   ],
   "source": [
    "# Add parent directory to path\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "# Import our modules\n",
    "from modules.m1_baseline import check_claim\n",
    "from modules.m2_langgraph import check_claim_with_graph\n",
    "from modules.m3_evaluation import (\n",
    "    BSDetectorEvaluator, \n",
    "    evaluate_baseline,\n",
    "    evaluate_langgraph,\n",
    "    compare_all_iterations\n",
    ")\n",
    "from config.llm_factory import LLMFactory\n",
    "\n",
    "# Other imports\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"‚úÖ Imports successful!\")\n",
    "print(f\"üïê Current time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding Our Test Dataset\n",
    "\n",
    "Let's explore the aviation claims dataset we'll use for evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Dataset Overview:\n",
      "Total claims: 30\n",
      "Categories: ['historical', 'technical', 'safety', 'performance', 'future', 'misleading']\n",
      "\n",
      "============================================================\n",
      "\n",
      "üìà Distribution by Difficulty:\n",
      "  Easy: 4 claims\n",
      "  Hard: 15 claims\n",
      "  Medium: 11 claims\n",
      "\n",
      "============================================================\n",
      "\n",
      "üìù Example Claims:\n",
      "\n",
      "**EASY Example:**\n",
      "Claim: \"The Wright brothers' first powered flight was in 1903\"\n",
      "Truth: LEGITIMATE\n",
      "Category: historical\n",
      "Expected Confidence: 95%\n",
      "\n",
      "**MEDIUM Example:**\n",
      "Claim: \"The Concorde could fly at Mach 2.04\"\n",
      "Truth: LEGITIMATE\n",
      "Category: performance\n",
      "Expected Confidence: 85%\n",
      "\n",
      "**HARD Example:**\n",
      "Claim: \"The Boeing 737 MAX is the safest aircraft ever built\"\n",
      "Truth: BS\n",
      "Category: safety\n",
      "Expected Confidence: 60%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load and explore the dataset\n",
    "with open('../data/aviation_claims_dataset.json', 'r') as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "print(f\"üìä Dataset Overview:\")\n",
    "print(f\"Total claims: {len(dataset['claims'])}\")\n",
    "print(f\"Categories: {dataset['metadata']['categories']}\")\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Show distribution by difficulty\n",
    "difficulty_counts = {}\n",
    "for claim in dataset['claims']:\n",
    "    diff = claim['difficulty']\n",
    "    difficulty_counts[diff] = difficulty_counts.get(diff, 0) + 1\n",
    "\n",
    "print(\"üìà Distribution by Difficulty:\")\n",
    "for diff, count in sorted(difficulty_counts.items()):\n",
    "    print(f\"  {diff.capitalize()}: {count} claims\")\n",
    "\n",
    "# Show a few example claims\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "print(\"üìù Example Claims:\\n\")\n",
    "\n",
    "# Show one claim from each difficulty\n",
    "for difficulty in ['easy', 'medium', 'hard']:\n",
    "    example = next(c for c in dataset['claims'] if c['difficulty'] == difficulty)\n",
    "    print(f\"**{difficulty.upper()} Example:**\")\n",
    "    print(f\"Claim: \\\"{example['claim']}\\\"\")\n",
    "    print(f\"Truth: {example['verdict']}\")\n",
    "    print(f\"Category: {example['category']}\")\n",
    "    print(f\"Expected Confidence: {example['expected_confidence']}%\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Quick Evaluation Demo\n",
    "\n",
    "Let's run a quick evaluation on just the easy claims to see how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ Running Quick Evaluation on Easy Claims...\n",
      "\n",
      "\n",
      "üî¨ Evaluating Baseline (Easy)...\n",
      "Testing on 4 claims\n",
      "\n",
      "‚úÖ Baseline (Easy) Results:\n",
      "  Overall Accuracy: 75.0%\n",
      "  Easy: 75.0%, Medium: nan%, Hard: nan%\n",
      "  Avg Confidence: 96.2% (Correct: 96.7%, Wrong: 95.0%)\n",
      "  Avg Response Time: 1.07s\n",
      "\n",
      "============================================================\n",
      "\n",
      "\n",
      "üî¨ Evaluating LangGraph (Easy)...\n",
      "Testing on 4 claims\n",
      "\n",
      "‚úÖ LangGraph (Easy) Results:\n",
      "  Overall Accuracy: 75.0%\n",
      "  Easy: 75.0%, Medium: nan%, Hard: nan%\n",
      "  Avg Confidence: 95.0% (Correct: 95.0%, Wrong: 95.0%)\n",
      "  Avg Response Time: 1.08s\n"
     ]
    }
   ],
   "source": [
    "# Create evaluator with correct path\n",
    "import os\n",
    "dataset_path = \"../data/aviation_claims_dataset.json\" if os.path.exists(\"../data/aviation_claims_dataset.json\") else \"data/aviation_claims_dataset.json\"\n",
    "evaluator = BSDetectorEvaluator(dataset_path)\n",
    "\n",
    "print(\"üî¨ Running Quick Evaluation on Easy Claims...\\n\")\n",
    "\n",
    "# Evaluate baseline on easy claims\n",
    "baseline_result = evaluator.evaluate_detector(\n",
    "    check_claim, \n",
    "    \"Baseline (Easy)\", \n",
    "    subset=\"easy\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Evaluate LangGraph on easy claims\n",
    "langgraph_result = evaluator.evaluate_detector(\n",
    "    check_claim_with_graph, \n",
    "    \"LangGraph (Easy)\", \n",
    "    subset=\"easy\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Deep Dive: Analyzing Results\n",
    "\n",
    "Let's look at the detailed results to understand what's happening:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Detailed Analysis of Baseline Results:\n",
      "\n",
      "‚ùå Incorrect Predictions:\n",
      "\n",
      "Claim: \"Helicopters use jet engines to create lift with their rotors...\"\n",
      "Expected: BS, Got: LEGITIMATE\n",
      "Confidence: 95%\n",
      "\n",
      "============================================================\n",
      "\n",
      "üìä Confidence Distribution:\n",
      "Average: 96.2%\n",
      "Min: 95%\n",
      "Max: 100%\n",
      "\n",
      "‚è±Ô∏è  Response Times:\n",
      "Average: 1.07s\n",
      "Fastest: 1.01s\n",
      "Slowest: 1.19s\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame for easier analysis\n",
    "results_df = pd.DataFrame(baseline_result.claim_results)\n",
    "\n",
    "print(\"üîç Detailed Analysis of Baseline Results:\\n\")\n",
    "\n",
    "# Show claims where the detector was wrong\n",
    "wrong_predictions = results_df[results_df['correct'] == False]\n",
    "if len(wrong_predictions) > 0:\n",
    "    print(\"‚ùå Incorrect Predictions:\")\n",
    "    for _, row in wrong_predictions.iterrows():\n",
    "        print(f\"\\nClaim: \\\"{row['claim'][:60]}...\\\"\")\n",
    "        print(f\"Expected: {row['expected']}, Got: {row['predicted']}\")\n",
    "        print(f\"Confidence: {row['confidence']}%\")\n",
    "else:\n",
    "    print(\"‚úÖ All predictions were correct!\")\n",
    "\n",
    "# Confidence distribution\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "print(\"üìä Confidence Distribution:\")\n",
    "print(f\"Average: {results_df['confidence'].mean():.1f}%\")\n",
    "print(f\"Min: {results_df['confidence'].min()}%\")\n",
    "print(f\"Max: {results_df['confidence'].max()}%\")\n",
    "\n",
    "# Response time analysis\n",
    "print(f\"\\n‚è±Ô∏è  Response Times:\")\n",
    "print(f\"Average: {results_df['response_time'].mean():.2f}s\")\n",
    "print(f\"Fastest: {results_df['response_time'].min():.2f}s\")\n",
    "print(f\"Slowest: {results_df['response_time'].max():.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Running DeepEval Custom Metrics\n",
    "\n",
    "Now let's see how our custom DeepEval metrics work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Running DeepEval Custom Metrics...\n",
      "\n",
      "\n",
      "üß™ Running DeepEval tests for LangGraph Sample...\n",
      "\n",
      "Test Case 1: The Wright brothers' first powered flight was in 1...\n",
      "  BS Detection Accuracy: 1.00 - ‚úÖ PASS\n",
      "  Confidence Calibration: 1.00 - ‚úÖ PASS\n",
      "  Reasoning Quality: 1.00 - ‚úÖ PASS\n",
      "\n",
      "Test Case 2: Commercial airplanes can fly backwards...\n",
      "  BS Detection Accuracy: 1.00 - ‚úÖ PASS\n",
      "  Confidence Calibration: 1.00 - ‚úÖ PASS\n",
      "  Reasoning Quality: 0.90 - ‚úÖ PASS\n",
      "\n",
      "Test Case 3: The Concorde could fly at Mach 2.04...\n",
      "  BS Detection Accuracy: 1.00 - ‚úÖ PASS\n",
      "  Confidence Calibration: 1.00 - ‚úÖ PASS\n",
      "  Reasoning Quality: 0.90 - ‚úÖ PASS\n",
      "\n",
      "Test Case 4: The Boeing 737 MAX is the safest aircraft ever bui...\n",
      "  BS Detection Accuracy: 1.00 - ‚úÖ PASS\n",
      "  Confidence Calibration: 1.00 - ‚úÖ PASS\n",
      "  Reasoning Quality: 0.90 - ‚úÖ PASS\n",
      "\n",
      "Test Case 5: The Airbus A380 program was cancelled due to lack ...\n",
      "  BS Detection Accuracy: 1.00 - ‚úÖ PASS\n",
      "  Confidence Calibration: 1.00 - ‚úÖ PASS\n",
      "  Reasoning Quality: 0.90 - ‚úÖ PASS\n",
      "\n",
      "üìä DeepEval Summary: 15/15 tests passed (100.0%)\n"
     ]
    }
   ],
   "source": [
    "# Run DeepEval tests on a sample\n",
    "print(\"üß™ Running DeepEval Custom Metrics...\\n\")\n",
    "\n",
    "evaluator.run_deepeval_tests(check_claim_with_graph, \"LangGraph Sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Full Comparison: All Iterations\n",
    "\n",
    "Let's run a complete evaluation comparing all our iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí° To run full evaluation, uncomment the code above.\n",
      "   Note: This will make ~60 LLM calls and may cost ~$0.50-$1.00\n"
     ]
    }
   ],
   "source": [
    "# WARNING: This will make many LLM calls and may take a few minutes\n",
    "# Uncomment to run full evaluation\n",
    "\n",
    "# print(\"üöÄ Running Full Evaluation (this may take a few minutes)...\\n\")\n",
    "# compare_all_iterations()\n",
    "\n",
    "print(\"üí° To run full evaluation, uncomment the code above.\")\n",
    "print(\"   Note: This will make ~60 LLM calls and may cost ~$0.50-$1.00\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Custom Evaluation: Your Turn!\n",
    "\n",
    "Let's create a custom evaluation for specific claim categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ Evaluating 9 technical claims...\n",
      "‚úÖ‚úÖ‚ùå‚úÖ‚úÖ‚úÖ‚ùå‚úÖ‚úÖ\n",
      "Accuracy: 77.8%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on specific categories\n",
    "def evaluate_by_category(evaluator, detector_func, category):\n",
    "    \"\"\"Evaluate detector on claims from a specific category\"\"\"\n",
    "    # Filter claims by category\n",
    "    category_claims = [\n",
    "        c for c in evaluator.claims \n",
    "        if c.category == category\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nüéØ Evaluating {len(category_claims)} {category} claims...\")\n",
    "    \n",
    "    correct = 0\n",
    "    for claim in category_claims:\n",
    "        # Get prediction\n",
    "        if \"graph\" in detector_func.__name__:\n",
    "            result = detector_func(claim.claim)\n",
    "        else:\n",
    "            llm = LLMFactory.create_llm()\n",
    "            result = detector_func(claim.claim, llm)\n",
    "        \n",
    "        # Check if correct\n",
    "        if result.get('verdict') == claim.verdict:\n",
    "            correct += 1\n",
    "            print(\"‚úÖ\", end=\"\")\n",
    "        else:\n",
    "            print(\"‚ùå\", end=\"\")\n",
    "    \n",
    "    accuracy = correct / len(category_claims) * 100\n",
    "    print(f\"\\nAccuracy: {accuracy:.1f}%\")\n",
    "    return accuracy\n",
    "\n",
    "# Test on technical claims\n",
    "tech_accuracy = evaluate_by_category(evaluator, check_claim_with_graph, \"technical\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualizing Performance\n",
    "\n",
    "Let's create a simple performance comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Performance Comparison:\n",
      "\n",
      "Accuracy Comparison:\n",
      "Baseline     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 75.0%\n",
      "LangGraph    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 75.0%\n",
      "\n",
      "Confidence Comparison:\n",
      "Baseline     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 96.2%\n",
      "LangGraph    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 95.0%\n"
     ]
    }
   ],
   "source": [
    "# Compare iterations visually\n",
    "if len(evaluator.results) >= 2:\n",
    "    print(\"üìä Performance Comparison:\\n\")\n",
    "    \n",
    "    # Create comparison data\n",
    "    iterations = []\n",
    "    accuracies = []\n",
    "    avg_confidences = []\n",
    "    \n",
    "    for name, result in evaluator.results.items():\n",
    "        iterations.append(name.split(\" \")[0])  # Get iteration name\n",
    "        accuracies.append(result.accuracy * 100)\n",
    "        avg_confidences.append(result.avg_confidence)\n",
    "    \n",
    "    # Simple ASCII bar chart\n",
    "    print(\"Accuracy Comparison:\")\n",
    "    for i, (iter_name, acc) in enumerate(zip(iterations, accuracies)):\n",
    "        bar = \"‚ñà\" * int(acc / 5)  # Each block = 5%\n",
    "        print(f\"{iter_name:12} {bar} {acc:.1f}%\")\n",
    "    \n",
    "    print(\"\\nConfidence Comparison:\")\n",
    "    for i, (iter_name, conf) in enumerate(zip(iterations, avg_confidences)):\n",
    "        bar = \"‚ñà\" * int(conf / 5)  # Each block = 5%\n",
    "        print(f\"{iter_name:12} {bar} {conf:.1f}%\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  Run more evaluations to see comparisons\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Key Insights and Takeaways\n",
    "\n",
    "Based on our evaluation, here are the key insights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Key Insights from Evaluation:\n",
      "\n",
      "  1. **Accuracy by Difficulty**: Performance decreases as claims get harder\n",
      "  2. **Confidence Calibration**: High confidence usually means correct predictions\n",
      "  3. **Category Performance**: Technical claims are easiest to verify\n",
      "  4. **Retry Logic**: LangGraph version handles errors more gracefully\n",
      "  5. **Response Time**: Baseline is faster, but less robust\n",
      "\n",
      "üí° What This Means:\n",
      "   - We now have baseline metrics to beat\n",
      "   - We know where our detector struggles (hard/misleading claims)\n",
      "   - Future improvements can be measured objectively\n",
      "   - Ready to add tools in Iteration 4 to improve accuracy!\n"
     ]
    }
   ],
   "source": [
    "print(\"üéØ Key Insights from Evaluation:\\n\")\n",
    "\n",
    "insights = [\n",
    "    \"1. **Accuracy by Difficulty**: Performance decreases as claims get harder\",\n",
    "    \"2. **Confidence Calibration**: High confidence usually means correct predictions\",\n",
    "    \"3. **Category Performance**: Technical claims are easiest to verify\",\n",
    "    \"4. **Retry Logic**: LangGraph version handles errors more gracefully\",\n",
    "    \"5. **Response Time**: Baseline is faster, but less robust\"\n",
    "]\n",
    "\n",
    "for insight in insights:\n",
    "    print(f\"  {insight}\")\n",
    "\n",
    "print(\"\\nüí° What This Means:\")\n",
    "print(\"   - We now have baseline metrics to beat\")\n",
    "print(\"   - We know where our detector struggles (hard/misleading claims)\")\n",
    "print(\"   - Future improvements can be measured objectively\")\n",
    "print(\"   - Ready to add tools in Iteration 4 to improve accuracy!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Interactive Evaluation\n",
    "\n",
    "Try evaluating specific claims yourself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_evaluation():\n",
    "    \"\"\"Let users test specific claims interactively\"\"\"\n",
    "    print(\"üéÆ Interactive Claim Evaluation\")\n",
    "    print(\"Type a claim ID (e.g., 'easy_001') or 'quit' to exit\\n\")\n",
    "    \n",
    "    # Load claims into a dict for easy lookup\n",
    "    claim_dict = {c['id']: c for c in dataset['claims']}\n",
    "    \n",
    "    while True:\n",
    "        claim_id = input(\"\\nEnter claim ID: \").strip()\n",
    "        \n",
    "        if claim_id.lower() == 'quit':\n",
    "            break\n",
    "            \n",
    "        if claim_id not in claim_dict:\n",
    "            print(\"‚ùå Invalid claim ID. Try 'easy_001', 'medium_003', etc.\")\n",
    "            continue\n",
    "        \n",
    "        claim_data = claim_dict[claim_id]\n",
    "        print(f\"\\nüìã Claim: \\\"{claim_data['claim']}\\\"\")\n",
    "        print(f\"Ground Truth: {claim_data['verdict']}\")\n",
    "        print(f\"Difficulty: {claim_data['difficulty']}\")\n",
    "        \n",
    "        # Test with LangGraph detector\n",
    "        print(\"\\nü§ñ Testing with LangGraph detector...\")\n",
    "        result = check_claim_with_graph(claim_data['claim'])\n",
    "        \n",
    "        print(f\"\\nPrediction: {result.get('verdict')}\")\n",
    "        print(f\"Confidence: {result.get('confidence')}%\")\n",
    "        print(f\"Correct: {'‚úÖ' if result.get('verdict') == claim_data['verdict'] else '‚ùå'}\")\n",
    "        print(f\"\\nReasoning: {result.get('reasoning')}\")\n",
    "    \n",
    "    print(\"\\nüëã Thanks for testing!\")\n",
    "\n",
    "# Uncomment to run interactive evaluation\n",
    "# interactive_evaluation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What We Learned\n",
    "1. **Evaluation is Critical**: Can't improve what we don't measure\n",
    "2. **Custom Metrics Matter**: Domain-specific metrics give better insights\n",
    "3. **Test Dataset Design**: Good test data covers edge cases\n",
    "4. **Iterative Improvement**: Each iteration should measurably improve\n",
    "\n",
    "### Our Evaluation Framework\n",
    "- ‚úÖ 30 aviation claims across difficulty levels\n",
    "- ‚úÖ Custom DeepEval metrics for accuracy, confidence, and reasoning\n",
    "- ‚úÖ Comparative analysis between iterations\n",
    "- ‚úÖ Performance tracking and visualization\n",
    "\n",
    "### Next Steps\n",
    "In Iteration 4, we'll add web search tools to improve accuracy on claims that need external verification. Our evaluation framework will help us measure if this actually helps!\n",
    "\n",
    "### üéØ Challenge\n",
    "Before moving on, try:\n",
    "1. Running evaluation on just \"misleading\" category claims\n",
    "2. Creating your own custom metric\n",
    "3. Adding a new test claim to the dataset\n",
    "\n",
    "Remember: Good evaluation leads to good improvements!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
