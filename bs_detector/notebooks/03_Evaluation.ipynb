{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03: Evaluation with DeepEval\n",
    "\n",
    "## Overview\n",
    "In this notebook, we'll add systematic evaluation to our BS detector. This is crucial for:\n",
    "- Measuring if our improvements actually work\n",
    "- Understanding where our detector fails\n",
    "- Guiding future enhancements\n",
    "\n",
    "## What We'll Learn\n",
    "1. Creating evaluation datasets\n",
    "2. Using DeepEval for LLM evaluation\n",
    "3. Custom metrics for domain-specific tasks\n",
    "4. Comparing iterations quantitatively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture Diagram\n",
    "\n",
    "Let's visualize how evaluation fits into our system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://mermaid.ink/img/CmdyYXBoIFRCCiAgICBzdWJncmFwaCAiVGVzdCBEYXRhc2V0IgogICAgICAgIFREW0F2aWF0aW9uIENsYWltczxici8+MzAgdGVzdCBjYXNlc10KICAgICAgICBURCAtLT4gRWFzeVtFYXN5IENsYWltczxici8+NCBjYXNlc10KICAgICAgICBURCAtLT4gTWVkaXVtW01lZGl1bSBDbGFpbXM8YnIvPjExIGNhc2VzXQogICAgICAgIFREIC0tPiBIYXJkW0hhcmQgQ2xhaW1zPGJyLz4xNSBjYXNlc10KICAgIGVuZAoKICAgIHN1YmdyYXBoICJEZXRlY3RvcnMiCiAgICAgICAgRDFbQmFzZWxpbmUgRGV0ZWN0b3I8YnIvPkl0ZXJhdGlvbiAxXQogICAgICAgIEQyW0xhbmdHcmFwaCBEZXRlY3Rvcjxici8+SXRlcmF0aW9uIDJdCiAgICBlbmQKCiAgICBzdWJncmFwaCAiRXZhbHVhdGlvbiBGcmFtZXdvcmsiCiAgICAgICAgRVZbQlNEZXRlY3RvckV2YWx1YXRvcl0KICAgICAgICBFViAtLT4gTTFbQWNjdXJhY3kgTWV0cmljXQogICAgICAgIEVWIC0tPiBNMltDb25maWRlbmNlIE1ldHJpY10KICAgICAgICBFViAtLT4gTTNbUmVhc29uaW5nIE1ldHJpY10KICAgIGVuZAoKICAgIHN1YmdyYXBoICJSZXN1bHRzIgogICAgICAgIFJbRXZhbHVhdGlvblJlc3VsdF0KICAgICAgICBSIC0tPiBSQVtBY2N1cmFjeSBieSBEaWZmaWN1bHR5XQogICAgICAgIFIgLS0+IFJDW0NvbmZpZGVuY2UgQW5hbHlzaXNdCiAgICAgICAgUiAtLT4gUlRbUmVzcG9uc2UgVGltZXNdCiAgICBlbmQKCiAgICBURCAtLT4gRVYKICAgIEQxIC0tPiBFVgogICAgRDIgLS0+IEVWCiAgICBFViAtLT4gUgoKICAgIGNsYXNzRGVmIGRhdGFzZXQgZmlsbDojZjlmLHN0cm9rZTojMzMzLHN0cm9rZS13aWR0aDoycHgKICAgIGNsYXNzRGVmIGRldGVjdG9yIGZpbGw6I2JiZixzdHJva2U6IzMzMyxzdHJva2Utd2lkdGg6MnB4CiAgICBjbGFzc0RlZiBldmFsdWF0b3IgZmlsbDojYmZiLHN0cm9rZTojMzMzLHN0cm9rZS13aWR0aDoycHgKICAgIGNsYXNzRGVmIHJlc3VsdCBmaWxsOiNmYmIsc3Ryb2tlOiMzMzMsc3Ryb2tlLXdpZHRoOjJweAoKICAgIGNsYXNzIFRELEVhc3ksTWVkaXVtLEhhcmQgZGF0YXNldAogICAgY2xhc3MgRDEsRDIgZGV0ZWN0b3IKICAgIGNsYXNzIEVWLE0xLE0yLE0zIGV2YWx1YXRvcgogICAgY2xhc3MgUixSQSxSQyxSVCByZXN1bHQK?type=png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import base64\n",
    "from IPython.display import Image\n",
    "\n",
    "# Mermaid diagram showing evaluation flow\n",
    "evaluation_diagram = \"\"\"\n",
    "graph TB\n",
    "    subgraph \"Test Dataset\"\n",
    "        TD[Aviation Claims<br/>30 test cases]\n",
    "        TD --> Easy[Easy Claims<br/>4 cases]\n",
    "        TD --> Medium[Medium Claims<br/>11 cases]\n",
    "        TD --> Hard[Hard Claims<br/>15 cases]\n",
    "    end\n",
    "    \n",
    "    subgraph \"Detectors\"\n",
    "        D1[Baseline Detector<br/>Iteration 1]\n",
    "        D2[LangGraph Detector<br/>Iteration 2]\n",
    "    end\n",
    "    \n",
    "    subgraph \"Evaluation Framework\"\n",
    "        EV[BSDetectorEvaluator]\n",
    "        EV --> M1[Accuracy Metric]\n",
    "        EV --> M2[Confidence Metric]\n",
    "        EV --> M3[Reasoning Metric]\n",
    "    end\n",
    "    \n",
    "    subgraph \"Results\"\n",
    "        R[EvaluationResult]\n",
    "        R --> RA[Accuracy by Difficulty]\n",
    "        R --> RC[Confidence Analysis]\n",
    "        R --> RT[Response Times]\n",
    "    end\n",
    "    \n",
    "    TD --> EV\n",
    "    D1 --> EV\n",
    "    D2 --> EV\n",
    "    EV --> R\n",
    "    \n",
    "    classDef dataset fill:#f9f,stroke:#333,stroke-width:2px\n",
    "    classDef detector fill:#bbf,stroke:#333,stroke-width:2px\n",
    "    classDef evaluator fill:#bfb,stroke:#333,stroke-width:2px\n",
    "    classDef result fill:#fbb,stroke:#333,stroke-width:2px\n",
    "    \n",
    "    class TD,Easy,Medium,Hard dataset\n",
    "    class D1,D2 detector\n",
    "    class EV,M1,M2,M3 evaluator\n",
    "    class R,RA,RC,RT result\n",
    "\"\"\"\n",
    "\n",
    "def render_mermaid_diagram(graph_def):\n",
    "    \"\"\"Render a Mermaid diagram using mermaid.ink API\"\"\"\n",
    "    graph_bytes = graph_def.encode(\"utf-8\")\n",
    "    base64_string = base64.b64encode(graph_bytes).decode(\"ascii\")\n",
    "    image_url = f\"https://mermaid.ink/img/{base64_string}?type=png\"\n",
    "    return Image(url=image_url)\n",
    "\n",
    "render_mermaid_diagram(evaluation_diagram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import everything we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Imports successful!\n",
      "ðŸ• Current time: 2025-07-22 22:23:04\n"
     ]
    }
   ],
   "source": [
    "# Add parent directory to path\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "# Import our modules\n",
    "from modules.m1_baseline import check_claim\n",
    "from modules.m2_langgraph import check_claim_with_graph\n",
    "from modules.m3_evaluation import (\n",
    "    BSDetectorEvaluator, \n",
    "    evaluate_baseline,\n",
    "    evaluate_langgraph,\n",
    "    compare_all_iterations\n",
    ")\n",
    "from config.llm_factory import LLMFactory\n",
    "\n",
    "# Other imports\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"âœ… Imports successful!\")\n",
    "print(f\"ðŸ• Current time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding Our Test Dataset\n",
    "\n",
    "Let's explore the aviation claims dataset we'll use for evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Dataset Overview:\n",
      "Total claims: 30\n",
      "Categories: ['historical', 'technical', 'safety', 'performance', 'future', 'misleading']\n",
      "\n",
      "============================================================\n",
      "\n",
      "ðŸ“ˆ Distribution by Difficulty:\n",
      "  Easy: 4 claims\n",
      "  Hard: 15 claims\n",
      "  Medium: 11 claims\n",
      "\n",
      "============================================================\n",
      "\n",
      "ðŸ“ Example Claims:\n",
      "\n",
      "**EASY Example:**\n",
      "Claim: \"The Wright brothers' first powered flight was in 1903\"\n",
      "Truth: LEGITIMATE\n",
      "Category: historical\n",
      "Expected Confidence: 95%\n",
      "\n",
      "**MEDIUM Example:**\n",
      "Claim: \"The Concorde could fly at Mach 2.04\"\n",
      "Truth: LEGITIMATE\n",
      "Category: performance\n",
      "Expected Confidence: 85%\n",
      "\n",
      "**HARD Example:**\n",
      "Claim: \"The Boeing 737 MAX is the safest aircraft ever built\"\n",
      "Truth: BS\n",
      "Category: safety\n",
      "Expected Confidence: 60%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load and explore the dataset\n",
    "with open('../data/aviation_claims_dataset.json', 'r') as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "print(f\"ðŸ“Š Dataset Overview:\")\n",
    "print(f\"Total claims: {len(dataset['claims'])}\")\n",
    "print(f\"Categories: {dataset['metadata']['categories']}\")\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Show distribution by difficulty\n",
    "difficulty_counts = {}\n",
    "for claim in dataset['claims']:\n",
    "    diff = claim['difficulty']\n",
    "    difficulty_counts[diff] = difficulty_counts.get(diff, 0) + 1\n",
    "\n",
    "print(\"ðŸ“ˆ Distribution by Difficulty:\")\n",
    "for diff, count in sorted(difficulty_counts.items()):\n",
    "    print(f\"  {diff.capitalize()}: {count} claims\")\n",
    "\n",
    "# Show a few example claims\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "print(\"ðŸ“ Example Claims:\\n\")\n",
    "\n",
    "# Show one claim from each difficulty\n",
    "for difficulty in ['easy', 'medium', 'hard']:\n",
    "    example = next(c for c in dataset['claims'] if c['difficulty'] == difficulty)\n",
    "    print(f\"**{difficulty.upper()} Example:**\")\n",
    "    print(f\"Claim: \\\"{example['claim']}\\\"\")\n",
    "    print(f\"Truth: {example['verdict']}\")\n",
    "    print(f\"Category: {example['category']}\")\n",
    "    print(f\"Expected Confidence: {example['expected_confidence']}%\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Quick Evaluation Demo\n",
    "\n",
    "Let's run a quick evaluation on just the easy claims to see how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¬ Running Quick Evaluation on Easy Claims...\n",
      "\n",
      "\n",
      "ðŸ”¬ Evaluating Baseline (Easy)...\n",
      "Testing on 4 claims\n",
      "\n",
      "âœ… Baseline (Easy) Results:\n",
      "  Overall Accuracy: 75.0%\n",
      "  Easy: 75.0%, Medium: nan%, Hard: nan%\n",
      "  Avg Confidence: 95.0% (Correct: 95.0%, Wrong: 95.0%)\n",
      "  Avg Response Time: 1.33s\n",
      "\n",
      "============================================================\n",
      "\n",
      "\n",
      "ðŸ”¬ Evaluating LangGraph (Easy)...\n",
      "Testing on 4 claims\n",
      "\n",
      "âœ… LangGraph (Easy) Results:\n",
      "  Overall Accuracy: 75.0%\n",
      "  Easy: 75.0%, Medium: nan%, Hard: nan%\n",
      "  Avg Confidence: 95.0% (Correct: 95.0%, Wrong: 95.0%)\n",
      "  Avg Response Time: 1.09s\n"
     ]
    }
   ],
   "source": [
    "# Create evaluator with correct path\n",
    "import os\n",
    "dataset_path = \"../data/aviation_claims_dataset.json\" if os.path.exists(\"../data/aviation_claims_dataset.json\") else \"data/aviation_claims_dataset.json\"\n",
    "evaluator = BSDetectorEvaluator(dataset_path)\n",
    "\n",
    "print(\"ðŸ”¬ Running Quick Evaluation on Easy Claims...\\n\")\n",
    "\n",
    "# Evaluate baseline on easy claims\n",
    "baseline_result = evaluator.evaluate_detector(\n",
    "    check_claim, \n",
    "    \"Baseline (Easy)\", \n",
    "    subset=\"easy\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Evaluate LangGraph on easy claims\n",
    "langgraph_result = evaluator.evaluate_detector(\n",
    "    check_claim_with_graph, \n",
    "    \"LangGraph (Easy)\", \n",
    "    subset=\"easy\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Deep Dive: Analyzing Results\n",
    "\n",
    "Let's look at the detailed results to understand what's happening:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Detailed Analysis of Baseline Results:\n",
      "\n",
      "âŒ Incorrect Predictions:\n",
      "\n",
      "Claim: \"Helicopters use jet engines to create lift with their rotors...\"\n",
      "Expected: BS, Got: LEGITIMATE\n",
      "Confidence: 95%\n",
      "\n",
      "============================================================\n",
      "\n",
      "ðŸ“Š Confidence Distribution:\n",
      "Average: 95.0%\n",
      "Min: 95%\n",
      "Max: 95%\n",
      "\n",
      "â±ï¸  Response Times:\n",
      "Average: 1.33s\n",
      "Fastest: 1.06s\n",
      "Slowest: 1.70s\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame for easier analysis\n",
    "results_df = pd.DataFrame(baseline_result.claim_results)\n",
    "\n",
    "print(\"ðŸ” Detailed Analysis of Baseline Results:\\n\")\n",
    "\n",
    "# Show claims where the detector was wrong\n",
    "wrong_predictions = results_df[results_df['correct'] == False]\n",
    "if len(wrong_predictions) > 0:\n",
    "    print(\"âŒ Incorrect Predictions:\")\n",
    "    for _, row in wrong_predictions.iterrows():\n",
    "        print(f\"\\nClaim: \\\"{row['claim'][:60]}...\\\"\")\n",
    "        print(f\"Expected: {row['expected']}, Got: {row['predicted']}\")\n",
    "        print(f\"Confidence: {row['confidence']}%\")\n",
    "else:\n",
    "    print(\"âœ… All predictions were correct!\")\n",
    "\n",
    "# Confidence distribution\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "print(\"ðŸ“Š Confidence Distribution:\")\n",
    "print(f\"Average: {results_df['confidence'].mean():.1f}%\")\n",
    "print(f\"Min: {results_df['confidence'].min()}%\")\n",
    "print(f\"Max: {results_df['confidence'].max()}%\")\n",
    "\n",
    "# Response time analysis\n",
    "print(f\"\\nâ±ï¸  Response Times:\")\n",
    "print(f\"Average: {results_df['response_time'].mean():.2f}s\")\n",
    "print(f\"Fastest: {results_df['response_time'].min():.2f}s\")\n",
    "print(f\"Slowest: {results_df['response_time'].max():.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Running DeepEval Custom Metrics\n",
    "\n",
    "Now let's see how our custom DeepEval metrics work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ª Running DeepEval Custom Metrics...\n",
      "\n",
      "\n",
      "ðŸ§ª Running DeepEval tests for LangGraph Sample...\n",
      "\n",
      "Test Case 1: The Wright brothers' first powered flight was in 1...\n",
      "  BS Detection Accuracy: 1.00 - âœ… PASS\n",
      "  Confidence Calibration: 1.00 - âœ… PASS\n",
      "  Reasoning Quality: 1.00 - âœ… PASS\n",
      "\n",
      "Test Case 2: Commercial airplanes can fly backwards...\n",
      "  BS Detection Accuracy: 1.00 - âœ… PASS\n",
      "  Confidence Calibration: 1.00 - âœ… PASS\n",
      "  Reasoning Quality: 1.00 - âœ… PASS\n",
      "\n",
      "Test Case 3: The Concorde could fly at Mach 2.04...\n",
      "  BS Detection Accuracy: 1.00 - âœ… PASS\n",
      "  Confidence Calibration: 1.00 - âœ… PASS\n",
      "  Reasoning Quality: 1.00 - âœ… PASS\n",
      "\n",
      "Test Case 4: The Boeing 737 MAX is the safest aircraft ever bui...\n",
      "  BS Detection Accuracy: 1.00 - âœ… PASS\n",
      "  Confidence Calibration: 1.00 - âœ… PASS\n",
      "  Reasoning Quality: 0.90 - âœ… PASS\n",
      "\n",
      "Test Case 5: The Airbus A380 program was cancelled due to lack ...\n",
      "  BS Detection Accuracy: 1.00 - âœ… PASS\n",
      "  Confidence Calibration: 1.00 - âœ… PASS\n",
      "  Reasoning Quality: 0.90 - âœ… PASS\n",
      "\n",
      "ðŸ“Š DeepEval Summary: 15/15 tests passed (100.0%)\n"
     ]
    }
   ],
   "source": [
    "# Run DeepEval tests on a sample\n",
    "print(\"ðŸ§ª Running DeepEval Custom Metrics...\\n\")\n",
    "\n",
    "evaluator.run_deepeval_tests(check_claim_with_graph, \"LangGraph Sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Full Comparison: All Iterations\n",
    "\n",
    "Let's run a complete evaluation comparing all our iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ To run full evaluation, uncomment the code above.\n",
      "   Note: This will make ~60 LLM calls and may cost ~$0.50-$1.00\n"
     ]
    }
   ],
   "source": [
    "# WARNING: This will make many LLM calls and may take a few minutes\n",
    "# Uncomment to run full evaluation\n",
    "\n",
    "# print(\"ðŸš€ Running Full Evaluation (this may take a few minutes)...\\n\")\n",
    "# compare_all_iterations()\n",
    "\n",
    "print(\"ðŸ’¡ To run full evaluation, uncomment the code above.\")\n",
    "print(\"   Note: This will make ~60 LLM calls and may cost ~$0.50-$1.00\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Custom Evaluation: Your Turn!\n",
    "\n",
    "Let's create a custom evaluation for specific claim categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¯ Evaluating 9 technical claims...\n",
      "âœ…âœ…âŒâœ…âœ…âœ…âŒâœ…âœ…\n",
      "Accuracy: 77.8%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on specific categories\n",
    "def evaluate_by_category(evaluator, detector_func, category):\n",
    "    \"\"\"Evaluate detector on claims from a specific category\"\"\"\n",
    "    # Filter claims by category\n",
    "    category_claims = [\n",
    "        c for c in evaluator.claims \n",
    "        if c.category == category\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ Evaluating {len(category_claims)} {category} claims...\")\n",
    "    \n",
    "    correct = 0\n",
    "    for claim in category_claims:\n",
    "        # Get prediction\n",
    "        if \"graph\" in detector_func.__name__:\n",
    "            result = detector_func(claim.claim)\n",
    "        else:\n",
    "            llm = LLMFactory.create_llm()\n",
    "            result = detector_func(claim.claim, llm)\n",
    "        \n",
    "        # Check if correct\n",
    "        if result.get('verdict') == claim.verdict:\n",
    "            correct += 1\n",
    "            print(\"âœ…\", end=\"\")\n",
    "        else:\n",
    "            print(\"âŒ\", end=\"\")\n",
    "    \n",
    "    accuracy = correct / len(category_claims) * 100\n",
    "    print(f\"\\nAccuracy: {accuracy:.1f}%\")\n",
    "    return accuracy\n",
    "\n",
    "# Test on technical claims\n",
    "tech_accuracy = evaluate_by_category(evaluator, check_claim_with_graph, \"technical\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualizing Performance\n",
    "\n",
    "Let's create a simple performance comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Performance Comparison:\n",
      "\n",
      "Accuracy Comparison:\n",
      "Baseline     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 75.0%\n",
      "LangGraph    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 75.0%\n",
      "\n",
      "Confidence Comparison:\n",
      "Baseline     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 95.0%\n",
      "LangGraph    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 95.0%\n"
     ]
    }
   ],
   "source": [
    "# Compare iterations visually\n",
    "if len(evaluator.results) >= 2:\n",
    "    print(\"ðŸ“Š Performance Comparison:\\n\")\n",
    "    \n",
    "    # Create comparison data\n",
    "    iterations = []\n",
    "    accuracies = []\n",
    "    avg_confidences = []\n",
    "    \n",
    "    for name, result in evaluator.results.items():\n",
    "        iterations.append(name.split(\" \")[0])  # Get iteration name\n",
    "        accuracies.append(result.accuracy * 100)\n",
    "        avg_confidences.append(result.avg_confidence)\n",
    "    \n",
    "    # Simple ASCII bar chart\n",
    "    print(\"Accuracy Comparison:\")\n",
    "    for i, (iter_name, acc) in enumerate(zip(iterations, accuracies)):\n",
    "        bar = \"â–ˆ\" * int(acc / 5)  # Each block = 5%\n",
    "        print(f\"{iter_name:12} {bar} {acc:.1f}%\")\n",
    "    \n",
    "    print(\"\\nConfidence Comparison:\")\n",
    "    for i, (iter_name, conf) in enumerate(zip(iterations, avg_confidences)):\n",
    "        bar = \"â–ˆ\" * int(conf / 5)  # Each block = 5%\n",
    "        print(f\"{iter_name:12} {bar} {conf:.1f}%\")\n",
    "else:\n",
    "    print(\"â„¹ï¸  Run more evaluations to see comparisons\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Key Insights and Takeaways\n",
    "\n",
    "Based on our evaluation, here are the key insights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ Key Insights from Evaluation:\n",
      "\n",
      "  1. **Accuracy by Difficulty**: Performance decreases as claims get harder\n",
      "  2. **Confidence Calibration**: High confidence usually means correct predictions\n",
      "  3. **Category Performance**: Technical claims are easiest to verify\n",
      "  4. **Retry Logic**: LangGraph version handles errors more gracefully\n",
      "  5. **Response Time**: Baseline is faster, but less robust\n",
      "\n",
      "ðŸ’¡ What This Means:\n",
      "   - We now have baseline metrics to beat\n",
      "   - We know where our detector struggles (hard/misleading claims)\n",
      "   - Future improvements can be measured objectively\n",
      "   - Ready to add tools in Iteration 4 to improve accuracy!\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸŽ¯ Key Insights from Evaluation:\\n\")\n",
    "\n",
    "insights = [\n",
    "    \"1. **Accuracy by Difficulty**: Performance decreases as claims get harder\",\n",
    "    \"2. **Confidence Calibration**: High confidence usually means correct predictions\",\n",
    "    \"3. **Category Performance**: Technical claims are easiest to verify\",\n",
    "    \"4. **Retry Logic**: LangGraph version handles errors more gracefully\",\n",
    "    \"5. **Response Time**: Baseline is faster, but less robust\"\n",
    "]\n",
    "\n",
    "for insight in insights:\n",
    "    print(f\"  {insight}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ What This Means:\")\n",
    "print(\"   - We now have baseline metrics to beat\")\n",
    "print(\"   - We know where our detector struggles (hard/misleading claims)\")\n",
    "print(\"   - Future improvements can be measured objectively\")\n",
    "print(\"   - Ready to add tools in Iteration 4 to improve accuracy!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Interactive Evaluation\n",
    "\n",
    "Try evaluating specific claims yourself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_evaluation():\n",
    "    \"\"\"Let users test specific claims interactively\"\"\"\n",
    "    print(\"ðŸŽ® Interactive Claim Evaluation\")\n",
    "    print(\"Type a claim ID (e.g., 'easy_001') or 'quit' to exit\\n\")\n",
    "    \n",
    "    # Load claims into a dict for easy lookup\n",
    "    claim_dict = {c['id']: c for c in dataset['claims']}\n",
    "    \n",
    "    while True:\n",
    "        claim_id = input(\"\\nEnter claim ID: \").strip()\n",
    "        \n",
    "        if claim_id.lower() == 'quit':\n",
    "            break\n",
    "            \n",
    "        if claim_id not in claim_dict:\n",
    "            print(\"âŒ Invalid claim ID. Try 'easy_001', 'medium_003', etc.\")\n",
    "            continue\n",
    "        \n",
    "        claim_data = claim_dict[claim_id]\n",
    "        print(f\"\\nðŸ“‹ Claim: \\\"{claim_data['claim']}\\\"\")\n",
    "        print(f\"Ground Truth: {claim_data['verdict']}\")\n",
    "        print(f\"Difficulty: {claim_data['difficulty']}\")\n",
    "        \n",
    "        # Test with LangGraph detector\n",
    "        print(\"\\nðŸ¤– Testing with LangGraph detector...\")\n",
    "        result = check_claim_with_graph(claim_data['claim'])\n",
    "        \n",
    "        print(f\"\\nPrediction: {result.get('verdict')}\")\n",
    "        print(f\"Confidence: {result.get('confidence')}%\")\n",
    "        print(f\"Correct: {'âœ…' if result.get('verdict') == claim_data['verdict'] else 'âŒ'}\")\n",
    "        print(f\"\\nReasoning: {result.get('reasoning')}\")\n",
    "    \n",
    "    print(\"\\nðŸ‘‹ Thanks for testing!\")\n",
    "\n",
    "# Uncomment to run interactive evaluation\n",
    "# interactive_evaluation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What We Learned\n",
    "1. **Evaluation is Critical**: Can't improve what we don't measure\n",
    "2. **Custom Metrics Matter**: Domain-specific metrics give better insights\n",
    "3. **Test Dataset Design**: Good test data covers edge cases\n",
    "4. **Iterative Improvement**: Each iteration should measurably improve\n",
    "\n",
    "### Our Evaluation Framework\n",
    "- âœ… 30 aviation claims across difficulty levels\n",
    "- âœ… Custom DeepEval metrics for accuracy, confidence, and reasoning\n",
    "- âœ… Comparative analysis between iterations\n",
    "- âœ… Performance tracking and visualization\n",
    "\n",
    "### Next Steps\n",
    "In Iteration 4, we'll add web search tools to improve accuracy on claims that need external verification. Our evaluation framework will help us measure if this actually helps!\n",
    "\n",
    "### ðŸŽ¯ Challenge\n",
    "Before moving on, try:\n",
    "1. Running evaluation on just \"misleading\" category claims\n",
    "2. Creating your own custom metric\n",
    "3. Adding a new test claim to the dataset\n",
    "\n",
    "Remember: Good evaluation leads to good improvements!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Production Evaluation: Unknown Data\n",
    "\n",
    "Now let's see how to evaluate claims WITHOUT ground truth - this is what you need in production!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Production Evaluator Ready!\n",
      "\n",
      "This evaluator works WITHOUT ground truth by measuring:\n",
      "- Reasoning quality (LLM-as-judge)\n",
      "- Confidence calibration\n",
      "- Consistency with similar claims\n",
      "- Domain detection and drift\n",
      "- Anomaly detection\n"
     ]
    }
   ],
   "source": [
    "# Import the production evaluator\n",
    "from modules.m3_production_evaluation import (\n",
    "    ProductionEvaluator, \n",
    "    ProductionMetrics,\n",
    "    evaluate_unknown_claim\n",
    ")\n",
    "\n",
    "# Create a production evaluator\n",
    "prod_evaluator = ProductionEvaluator()\n",
    "\n",
    "print(\"ðŸš€ Production Evaluator Ready!\")\n",
    "print(\"\\nThis evaluator works WITHOUT ground truth by measuring:\")\n",
    "print(\"- Reasoning quality (LLM-as-judge)\")\n",
    "print(\"- Confidence calibration\")\n",
    "print(\"- Consistency with similar claims\")\n",
    "print(\"- Domain detection and drift\")\n",
    "print(\"- Anomaly detection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ª Evaluating claims from different domains...\n",
      "\n",
      "ðŸ“ Claim: \"The new Boeing 797 will have folding wings\"\n",
      "   Verdict: BS (confidence: 85%)\n",
      "   Domain: aviation\n",
      "   Trust Score: 0.70\n",
      "   Anomaly Score: 0.67\n",
      "   Needs Human Review: âœ… NO\n",
      "\n",
      "ðŸ“ Claim: \"Quantum computers can break all encryption instantly\"\n",
      "   Verdict: BS (confidence: 90%)\n",
      "   Domain: technology\n",
      "   Trust Score: 0.66\n",
      "   Anomaly Score: 0.67\n",
      "   Needs Human Review: âœ… NO\n",
      "\n",
      "ðŸ“ Claim: \"Drinking coffee cures all diseases\"\n",
      "   Verdict: BS (confidence: 95%)\n",
      "   Domain: medical\n",
      "   Trust Score: 0.59\n",
      "   Anomaly Score: 0.67\n",
      "   Needs Human Review: ðŸš¨ YES\n",
      "   Reason: Low quality metrics, \n",
      "\n",
      "ðŸ“ Claim: \"AI will replace all programmers by next year\"\n",
      "   Verdict: BS (confidence: 95%)\n",
      "   Domain: general\n",
      "   Trust Score: 0.61\n",
      "   Anomaly Score: 0.50\n",
      "   Needs Human Review: âœ… NO\n",
      "\n",
      "ðŸ“ Claim: \"This one weird trick makes you a millionaire\"\n",
      "   Verdict: BS (confidence: 95%)\n",
      "   Domain: general\n",
      "   Trust Score: 0.58\n",
      "   Anomaly Score: 0.50\n",
      "   Needs Human Review: ðŸš¨ YES\n",
      "   Reason: Low quality metrics, \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test on various domains (no ground truth needed!)\n",
    "test_claims = [\n",
    "    # Aviation (in-domain)\n",
    "    \"The new Boeing 797 will have folding wings\",\n",
    "    \n",
    "    # Technology (out-of-domain)\n",
    "    \"Quantum computers can break all encryption instantly\",\n",
    "    \n",
    "    # Medical (out-of-domain) \n",
    "    \"Drinking coffee cures all diseases\",\n",
    "    \n",
    "    # Ambiguous claim\n",
    "    \"AI will replace all programmers by next year\",\n",
    "    \n",
    "    # Suspicious claim\n",
    "    \"This one weird trick makes you a millionaire\"\n",
    "]\n",
    "\n",
    "print(\"ðŸ§ª Evaluating claims from different domains...\\n\")\n",
    "\n",
    "for claim in test_claims:\n",
    "    print(f\"ðŸ“ Claim: \\\"{claim}\\\"\")\n",
    "    \n",
    "    # Get BS detector result\n",
    "    result = check_claim_with_graph(claim)\n",
    "    \n",
    "    # Evaluate without ground truth\n",
    "    metrics = prod_evaluator.evaluate(claim, result)\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"   Verdict: {result['verdict']} (confidence: {result['confidence']}%)\")\n",
    "    print(f\"   Domain: {prod_evaluator.drift_detector.detect_domain(claim)[0]}\")\n",
    "    print(f\"   Trust Score: {metrics.trust_score:.2f}\")\n",
    "    print(f\"   Anomaly Score: {metrics.anomaly_score:.2f}\")\n",
    "    print(f\"   Needs Human Review: {'ðŸš¨ YES' if metrics.requires_human_review else 'âœ… NO'}\")\n",
    "    \n",
    "    if metrics.requires_human_review:\n",
    "        print(f\"   Reason: \", end=\"\")\n",
    "        if metrics.anomaly_score > 0.7:\n",
    "            print(\"Unusual claim, \", end=\"\")\n",
    "        if metrics.trust_score < 0.6:\n",
    "            print(\"Low quality metrics, \", end=\"\")\n",
    "        if metrics.reasoning_quality < 0.5:\n",
    "            print(\"Poor reasoning\", end=\"\")\n",
    "        print()\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Detailed Metrics Breakdown\n",
      "\n",
      "Claim: \"AI will replace all programmers by next year\"\n",
      "Verdict: BS (confidence: 95%)\n",
      "\n",
      "Quality Metrics:\n",
      "  Reasoning Quality: 1.00\n",
      "  Claim Plausibility: 1.00\n",
      "  Evidence Quality: 0.40\n",
      "  Logical Coherence: 1.00\n",
      "\n",
      "Behavioral Metrics:\n",
      "  Confidence Calibration: 0.39\n",
      "  Consistency Score: 1.00\n",
      "  Token Efficiency: 0.97\n",
      "\n",
      "Drift Detection:\n",
      "  Domain Confidence: 0.50\n",
      "  Anomaly Score: 0.50\n",
      "\n",
      "ðŸŽ¯ Overall Trust Score: 0.71\n",
      "ðŸš¨ Requires Human Review: False\n"
     ]
    }
   ],
   "source": [
    "# Show detailed metrics breakdown\n",
    "print(\"ðŸ“Š Detailed Metrics Breakdown\\n\")\n",
    "\n",
    "# Pick an interesting claim\n",
    "claim = \"AI will replace all programmers by next year\"\n",
    "result = check_claim_with_graph(claim)\n",
    "metrics = prod_evaluator.evaluate(claim, result)\n",
    "\n",
    "print(f\"Claim: \\\"{claim}\\\"\")\n",
    "print(f\"Verdict: {result['verdict']} (confidence: {result['confidence']}%)\\n\")\n",
    "\n",
    "# Show all metrics\n",
    "print(\"Quality Metrics:\")\n",
    "print(f\"  Reasoning Quality: {metrics.reasoning_quality:.2f}\")\n",
    "print(f\"  Claim Plausibility: {metrics.claim_plausibility:.2f}\")\n",
    "print(f\"  Evidence Quality: {metrics.evidence_quality:.2f}\")\n",
    "print(f\"  Logical Coherence: {metrics.logical_coherence:.2f}\")\n",
    "\n",
    "print(\"\\nBehavioral Metrics:\")\n",
    "print(f\"  Confidence Calibration: {metrics.confidence_calibration:.2f}\")\n",
    "print(f\"  Consistency Score: {metrics.consistency_score:.2f}\")\n",
    "print(f\"  Token Efficiency: {metrics.token_efficiency:.2f}\")\n",
    "\n",
    "print(\"\\nDrift Detection:\")\n",
    "print(f\"  Domain Confidence: {metrics.domain_confidence:.2f}\")\n",
    "print(f\"  Anomaly Score: {metrics.anomaly_score:.2f}\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Overall Trust Score: {metrics.trust_score:.2f}\")\n",
    "print(f\"ðŸš¨ Requires Human Review: {metrics.requires_human_review}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ˆ Evaluation Summary (Production Metrics)\n",
      "\n",
      "Total Evaluations: 6\n",
      "Average Trust Score: 0.64\n",
      "Human Review Rate: 33.3%\n",
      "Low Trust Claims: 2\n",
      "\n",
      "Domain Distribution:\n",
      "  aviation: 1 claims\n",
      "  technology: 1 claims\n",
      "  medical: 1 claims\n",
      "  general: 3 claims\n",
      "\n",
      "ðŸš¨ Claims Flagged for Human Review:\n",
      "\n",
      "- \"Drinking coffee cures all diseases\"\n",
      "  Trust Score: 0.59\n",
      "  Verdict: BS\n",
      "\n",
      "- \"This one weird trick makes you a millionaire\"\n",
      "  Trust Score: 0.58\n",
      "  Verdict: BS\n"
     ]
    }
   ],
   "source": [
    "# Get evaluation summary\n",
    "print(\"ðŸ“ˆ Evaluation Summary (Production Metrics)\\n\")\n",
    "\n",
    "summary = prod_evaluator.get_evaluation_summary()\n",
    "\n",
    "print(f\"Total Evaluations: {summary['total_evaluations']}\")\n",
    "print(f\"Average Trust Score: {summary['avg_trust_score']:.2f}\")\n",
    "print(f\"Human Review Rate: {summary['human_review_rate']:.1%}\")\n",
    "print(f\"Low Trust Claims: {summary['low_trust_claims']}\")\n",
    "\n",
    "print(\"\\nDomain Distribution:\")\n",
    "for domain, count in summary['domain_distribution'].items():\n",
    "    print(f\"  {domain}: {count} claims\")\n",
    "\n",
    "# Show which claims need human review\n",
    "print(\"\\nðŸš¨ Claims Flagged for Human Review:\")\n",
    "for item in prod_evaluator.export_for_human_review():\n",
    "    print(f\"\\n- \\\"{item['claim']}\\\"\")\n",
    "    print(f\"  Trust Score: {item['metrics']['trust_score']:.2f}\")\n",
    "    print(f\"  Verdict: {item['result']['verdict']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Differences: Known vs Unknown Data Evaluation\n",
    "\n",
    "### Known Data (Traditional Evaluation)\n",
    "- âœ… Have ground truth labels\n",
    "- âœ… Can calculate exact accuracy\n",
    "- âŒ Limited to test set\n",
    "- âŒ Doesn't work in production\n",
    "\n",
    "### Unknown Data (Production Evaluation)\n",
    "- âŒ No ground truth\n",
    "- âœ… Works on any claim\n",
    "- âœ… Uses proxy metrics (quality, consistency, drift)\n",
    "- âœ… Identifies when human review is needed\n",
    "\n",
    "### The Production Metrics\n",
    "1. **LLM-as-Judge**: Another LLM evaluates the reasoning quality\n",
    "2. **Confidence Calibration**: Does confidence match the language used?\n",
    "3. **Consistency**: Similar claims should get similar verdicts\n",
    "4. **Drift Detection**: Is this claim very different from what we've seen?\n",
    "5. **Trust Score**: Aggregate measure of reliability\n",
    "\n",
    "### When to Use Each\n",
    "- **Development**: Use known data evaluation to improve your detector\n",
    "- **Production**: Use unknown data evaluation to monitor real-world performance\n",
    "- **Best Practice**: Use both! Known data for baseline, unknown for monitoring"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}