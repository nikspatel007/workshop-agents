{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04: Evaluation - Measuring Performance\n",
    "\n",
    "Now let's measure how well our BS detector works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Test Dataset: 30 aviation claims\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "from modules.m1_baseline import check_claim\n",
    "from modules.m3_langgraph import check_claim_with_graph\n",
    "from config.llm_factory import LLMFactory\n",
    "import json\n",
    "\n",
    "# Load test dataset\n",
    "with open('../data/aviation_claims_dataset.json', 'r') as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "print(f\"ðŸ“Š Test Dataset: {len(dataset['claims'])} aviation claims\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Accuracy Test\n",
    "\n",
    "Let's test our detectors on some claims:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Baseline detector:\n",
      "âœ…âœ…âœ…âŒ\n",
      "Baseline Accuracy: 75.0%\n",
      "\n",
      "Testing LangGraph detector:\n",
      "âœ…âœ…âœ…âŒ\n",
      "LangGraph Accuracy: 75.0%\n"
     ]
    }
   ],
   "source": [
    "# Test on easy claims\n",
    "easy_claims = [c for c in dataset['claims'] if c['difficulty'] == 'easy']\n",
    "\n",
    "# Create LLM instance for baseline\n",
    "llm = LLMFactory.create_llm()\n",
    "\n",
    "def evaluate_detector(detector_func, claims, name, needs_llm=False):\n",
    "    correct = 0\n",
    "    for claim_data in claims:\n",
    "        if needs_llm:\n",
    "            result = detector_func(claim_data['claim'], llm)\n",
    "        else:\n",
    "            result = detector_func(claim_data['claim'])\n",
    "        if result['verdict'] == claim_data['verdict']:\n",
    "            correct += 1\n",
    "            print(\"âœ…\", end=\"\")\n",
    "        else:\n",
    "            print(\"âŒ\", end=\"\")\n",
    "    \n",
    "    accuracy = correct / len(claims) * 100\n",
    "    print(f\"\\n{name} Accuracy: {accuracy:.1f}%\")\n",
    "    return accuracy\n",
    "\n",
    "print(\"Testing Baseline detector:\")\n",
    "baseline_acc = evaluate_detector(check_claim, easy_claims[:4], \"Baseline\", needs_llm=True)\n",
    "\n",
    "print(\"\\nTesting LangGraph detector:\")\n",
    "langgraph_acc = evaluate_detector(check_claim_with_graph, easy_claims[:4], \"LangGraph\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test by Difficulty Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ˆ Performance by Difficulty:\n",
      "\n",
      "EASY (4 claims):\n",
      "âœ…âœ…âœ…\n",
      "easy Accuracy: 100.0%\n",
      "\n",
      "MEDIUM (11 claims):\n",
      "âœ…âœ…âœ…\n",
      "medium Accuracy: 100.0%\n",
      "\n",
      "HARD (15 claims):\n",
      "âœ…âœ…âœ…\n",
      "hard Accuracy: 100.0%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Group claims by difficulty\n",
    "by_difficulty = {'easy': [], 'medium': [], 'hard': []}\n",
    "for claim in dataset['claims']:\n",
    "    by_difficulty[claim['difficulty']].append(claim)\n",
    "\n",
    "# Test on each difficulty\n",
    "print(\"ðŸ“ˆ Performance by Difficulty:\\n\")\n",
    "for level, claims in by_difficulty.items():\n",
    "    print(f\"{level.upper()} ({len(claims)} claims):\")\n",
    "    acc = evaluate_detector(check_claim_with_graph, claims[:3], level)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Production Evaluation (No Ground Truth)\n",
    "\n",
    "In production, we don't have labels. Let's evaluate using quality metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Quality Metrics for: 'The Boeing 747 can fly backwards using reverse thrust'\n",
      "  Reasoning length: 282 chars\n",
      "  Confidence matches language: âœ…\n",
      "  Response time: 3.13s\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class QualityMetrics(BaseModel):\n",
    "    \"\"\"Metrics we can measure without ground truth\"\"\"\n",
    "    reasoning_length: int\n",
    "    confidence_matches_language: bool\n",
    "    response_time: float\n",
    "\n",
    "def evaluate_quality(claim: str) -> QualityMetrics:\n",
    "    \"\"\"Evaluate claim quality without knowing the answer\"\"\"\n",
    "    import time\n",
    "    \n",
    "    start = time.time()\n",
    "    # Use baseline with LLM for consistency\n",
    "    llm = LLMFactory.create_llm()\n",
    "    result = check_claim(claim, llm)\n",
    "    duration = time.time() - start\n",
    "    \n",
    "    # Check if confidence matches the language\n",
    "    high_confidence_words = ['definitely', 'certainly', 'clearly']\n",
    "    low_confidence_words = ['possibly', 'might', 'could be']\n",
    "    \n",
    "    reasoning = result.get('reasoning', '').lower()\n",
    "    confidence = result.get('confidence', 0)\n",
    "    \n",
    "    matches = True\n",
    "    if confidence > 80 and any(word in reasoning for word in low_confidence_words):\n",
    "        matches = False\n",
    "    if confidence < 60 and any(word in reasoning for word in high_confidence_words):\n",
    "        matches = False\n",
    "    \n",
    "    return QualityMetrics(\n",
    "        reasoning_length=len(reasoning),\n",
    "        confidence_matches_language=matches,\n",
    "        response_time=duration\n",
    "    )\n",
    "\n",
    "# Test quality metrics\n",
    "test_claim = \"The Boeing 747 can fly backwards using reverse thrust\"\n",
    "metrics = evaluate_quality(test_claim)\n",
    "\n",
    "print(f\"ðŸ“Š Quality Metrics for: '{test_claim}'\")\n",
    "print(f\"  Reasoning length: {metrics.reasoning_length} chars\")\n",
    "print(f\"  Confidence matches language: {'âœ…' if metrics.confidence_matches_language else 'âŒ'}\")\n",
    "print(f\"  Response time: {metrics.response_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Test with labeled data** during development\n",
    "2. **Monitor quality metrics** in production\n",
    "3. **Performance drops** on harder claims\n",
    "4. **LangGraph's retry logic** helps with errors\n",
    "\n",
    "Next: Let's add tools to improve accuracy on hard claims!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
