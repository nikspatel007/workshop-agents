{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an AI BS Detector with LangGraph\n",
    "\n",
    "## Workshop Overview\n",
    "\n",
    "In this workshop, we'll build a sophisticated BS detector using LangGraph, starting from a simple prompt and progressively adding:\n",
    "- Structured outputs with Pydantic\n",
    "- State management with LangGraph\n",
    "- Retry logic and error handling\n",
    "- Multiple expert agents with routing\n",
    "- Tool integration for evidence gathering\n",
    "- Human-in-the-loop for low confidence cases\n",
    "- Memory for learning from past claims\n",
    "\n",
    "### What You'll Learn\n",
    "1. **LangGraph Fundamentals** - States, nodes, edges, and graphs\n",
    "2. **Agent Design** - Building specialized agents for different claim types\n",
    "3. **Tool Integration** - Adding search capabilities to agents\n",
    "4. **Human-in-the-Loop** - Using interrupts for human review\n",
    "5. **Memory Systems** - Building context from previous interactions\n",
    "\n",
    "### Prerequisites\n",
    "- Basic Python knowledge\n",
    "- Familiarity with LangChain concepts helpful but not required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "First, let's import everything we'll need for the workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All imports successful!\n"
     ]
    }
   ],
   "source": [
    "# Core imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "from langchain_core.tools import tool\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional, Dict, Any, Literal, Annotated\n",
    "\n",
    "# LangGraph imports\n",
    "from langgraph.graph import StateGraph, END, START\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.types import Command, interrupt\n",
    "\n",
    "# Utilities\n",
    "import time\n",
    "import re\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "import base64\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Our LLM factory (only external dependency)\n",
    "from config.llm_factory import LLMFactory\n",
    "\n",
    "print(\"✅ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "Let's define some helper functions we'll use throughout the workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LLM initialized: ChatOpenAI\n"
     ]
    }
   ],
   "source": [
    "def render_mermaid(graph_definition: str) -> Image:\n",
    "    \"\"\"Render a mermaid diagram in the notebook\"\"\"\n",
    "    graph_bytes = graph_definition.encode(\"utf-8\")\n",
    "    base64_string = base64.b64encode(graph_bytes).decode(\"ascii\")\n",
    "    image_url = f\"https://mermaid.ink/img/{base64_string}?type=png\"\n",
    "    return Image(url=image_url)\n",
    "\n",
    "# Initialize our LLM\n",
    "llm = LLMFactory.create_llm()\n",
    "print(f\"✅ LLM initialized: {type(llm).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Simple BS Detector\n",
    "\n",
    "Let's start with the simplest possible BS detector - just a prompt and an LLM call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://mermaid.ink/img/CmdyYXBoIExSCiAgICBBW0NsYWltXSAtLT4gQltMTE1dCiAgICBCIC0tPiBDW1ZlcmRpY3RdCg==?type=png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# What we'll build first\n",
    "simple_flow = \"\"\"\n",
    "graph LR\n",
    "    A[Claim] --> B[LLM]\n",
    "    B --> C[Verdict]\n",
    "\"\"\"\n",
    "display(render_mermaid(simple_flow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claim: The Boeing 747 has four engines\n",
      "Verdict: LEGITIMATE\n",
      "\n",
      "Claim: Cats can fly at supersonic speeds\n",
      "Verdict: BS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define a simple verdict model\n",
    "class SimpleVerdict(BaseModel):\n",
    "    \"\"\"Simple verdict output\"\"\"\n",
    "    verdict: Literal[\"BS\", \"LEGITIMATE\"] = Field(\n",
    "        description=\"The verdict on whether the claim is BS or LEGITIMATE\"\n",
    "    )\n",
    "\n",
    "def simple_bs_detector(claim: str) -> str:\n",
    "    \"\"\"The simplest possible BS detector\"\"\"\n",
    "    prompt = f\"\"\"Is this claim BS or LEGITIMATE? \n",
    "    \n",
    "Claim: {claim}\n",
    "\n",
    "Answer with just BS or LEGITIMATE.\"\"\"\n",
    "    \n",
    "    # Use structured output even for simple detector\n",
    "    structured_llm = llm.with_structured_output(SimpleVerdict)\n",
    "    result = structured_llm.invoke(prompt)\n",
    "    return result.verdict\n",
    "\n",
    "# Test it\n",
    "test_claims = [\n",
    "    \"The Boeing 747 has four engines\",\n",
    "    \"Cats can fly at supersonic speeds\"\n",
    "]\n",
    "\n",
    "for claim in test_claims:\n",
    "    verdict = simple_bs_detector(claim)\n",
    "    print(f\"Claim: {claim}\")\n",
    "    print(f\"Verdict: {verdict}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Structured Output with Pydantic\n",
    "\n",
    "Let's improve our detector by getting structured output with confidence scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important: How LangGraph Handles State\n",
    "\n",
    "Before we dive into LangGraph, it's important to understand how state works:\n",
    "\n",
    "1. **Input**: You pass a dictionary to `invoke()`\n",
    "2. **Processing**: Nodes receive state objects and return dictionaries with updates\n",
    "3. **Output**: LangGraph returns the final state as a dictionary\n",
    "\n",
    "This means:\n",
    "- When invoking: `app.invoke({\"claim\": \"...\"})`\n",
    "- When accessing results: `result['verdict']` (not `result.verdict`)\n",
    "- But within our functions, we work with Pydantic models directly\n",
    "\n",
    "### Key Pattern to Remember:\n",
    "```python\n",
    "# Inside a node function:\n",
    "structured_llm = llm.with_structured_output(BSDetectorOutput)\n",
    "result = structured_llm.invoke(prompt)  # This is a Pydantic model\n",
    "print(result.verdict)  # Access as attributes\n",
    "\n",
    "# But the node returns a dictionary:\n",
    "return {\n",
    "    \"verdict\": result.verdict,\n",
    "    \"confidence\": result.confidence\n",
    "}\n",
    "\n",
    "# And LangGraph returns dictionaries:\n",
    "final_result = app.invoke({\"claim\": \"...\"})\n",
    "print(final_result['verdict'])  # Access as dictionary\n",
    "```\n",
    "\n",
    "Let's see this in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verdict: BS\n",
      "Confidence: 100%\n",
      "Reasoning: Extensive scientific evidence from astronomy, physics, and direct observations (such as satellite imagery and circumnavigation) conclusively shows that the Earth is an oblate spheroid, not flat. The claim that the Earth is flat contradicts well-established facts and empirical data.\n"
     ]
    }
   ],
   "source": [
    "# Define our output structure\n",
    "class BSDetectorOutput(BaseModel):\n",
    "    \"\"\"Structured output for BS detection\"\"\"\n",
    "    verdict: Literal[\"BS\", \"LEGITIMATE\", \"UNCERTAIN\"] = Field(\n",
    "        description=\"The verdict on whether the claim is BS\"\n",
    "    )\n",
    "    confidence: int = Field(\n",
    "        description=\"Confidence level from 0-100\",\n",
    "        ge=0,\n",
    "        le=100\n",
    "    )\n",
    "    reasoning: str = Field(\n",
    "        description=\"Brief explanation of the verdict\"\n",
    "    )\n",
    "\n",
    "def structured_bs_detector(claim: str) -> BSDetectorOutput:\n",
    "    \"\"\"BS detector with structured output\"\"\"\n",
    "    prompt = f\"\"\"Analyze if this claim is BS or legitimate.\n",
    "    \n",
    "Claim: {claim}\n",
    "\n",
    "Provide your analysis with a verdict, confidence score, and reasoning.\"\"\"\n",
    "    \n",
    "    # Use structured output\n",
    "    structured_llm = llm.with_structured_output(BSDetectorOutput)\n",
    "    return structured_llm.invoke(prompt)\n",
    "\n",
    "# Test it\n",
    "result = structured_bs_detector(\"The Earth is flat\")\n",
    "print(f\"Verdict: {result.verdict}\")\n",
    "print(f\"Confidence: {result.confidence}%\")\n",
    "print(f\"Reasoning: {result.reasoning}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: LangGraph State Management\n",
    "\n",
    "Now let's use LangGraph to manage state and add retry logic for robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verdict: BS\n",
      "Confidence: 95%\n",
      "Reasoning: Airplanes do not fly by flapping their wings; they generate lift through fixed wings and engine thrust. Flapping wings are characteristic of birds or insects, not airplanes.\n"
     ]
    }
   ],
   "source": [
    "# Define our state\n",
    "class BSDetectorState(BaseModel):\n",
    "    \"\"\"State for the BS detector graph\"\"\"\n",
    "    claim: str\n",
    "    verdict: Optional[str] = None\n",
    "    confidence: Optional[int] = None\n",
    "    reasoning: Optional[str] = None\n",
    "    retry_count: int = 0\n",
    "    max_retries: int = 3\n",
    "    error: Optional[str] = None\n",
    "\n",
    "# Define our nodes\n",
    "def detect_bs_node(state: BSDetectorState) -> dict:\n",
    "    \"\"\"Node that detects BS\"\"\"\n",
    "    try:\n",
    "        result = structured_bs_detector(state.claim)\n",
    "        return {\n",
    "            \"verdict\": result.verdict,\n",
    "            \"confidence\": result.confidence,\n",
    "            \"reasoning\": result.reasoning\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"error\": str(e),\n",
    "            \"retry_count\": state.retry_count + 1\n",
    "        }\n",
    "\n",
    "def retry_node(state: BSDetectorState) -> dict:\n",
    "    \"\"\"Node that handles retries\"\"\"\n",
    "    print(f\"Retrying... (attempt {state.retry_count + 1}/{state.max_retries})\")\n",
    "    time.sleep(1)  # Brief pause before retry\n",
    "    return {}\n",
    "\n",
    "# Define routing logic\n",
    "def route_after_detection(state: BSDetectorState) -> str:\n",
    "    \"\"\"Decide where to go after detection attempt\"\"\"\n",
    "    if state.verdict and not state.error:\n",
    "        return \"success\"\n",
    "    elif state.retry_count < state.max_retries:\n",
    "        return \"retry\"\n",
    "    else:\n",
    "        return \"error\"\n",
    "\n",
    "# Build the graph\n",
    "def create_bs_detector_graph():\n",
    "    \"\"\"Create the BS detector graph\"\"\"\n",
    "    workflow = StateGraph(BSDetectorState)\n",
    "    \n",
    "    # Add nodes\n",
    "    workflow.add_node(\"detect_bs\", detect_bs_node)\n",
    "    workflow.add_node(\"retry\", retry_node)\n",
    "    \n",
    "    # Add edges\n",
    "    workflow.add_edge(START, \"detect_bs\")\n",
    "    \n",
    "    # Conditional routing\n",
    "    workflow.add_conditional_edges(\n",
    "        \"detect_bs\",\n",
    "        route_after_detection,\n",
    "        {\n",
    "            \"success\": END,\n",
    "            \"retry\": \"retry\",\n",
    "            \"error\": END\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    workflow.add_edge(\"retry\", \"detect_bs\")\n",
    "    \n",
    "    return workflow.compile()\n",
    "\n",
    "# Test it\n",
    "app = create_bs_detector_graph()\n",
    "state_input = {\"claim\": \"Airplanes fly by flapping their wings\"}\n",
    "result = app.invoke(state_input)\n",
    "\n",
    "# Access result as dictionary\n",
    "print(f\"\\nVerdict: {result['verdict']}\")\n",
    "print(f\"Confidence: {result['confidence']}%\")\n",
    "print(f\"Reasoning: {result['reasoning']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://mermaid.ink/img/CmdyYXBoIFRECiAgICBBW1N0YXJ0XSAtLT4gQltEZXRlY3QgQlNdCiAgICBCIC0tPiBDe1N1Y2Nlc3M/fQogICAgQyAtLT58WWVzfCBEW0VuZF0KICAgIEMgLS0+fE5vfCBFW1JldHJ5XQogICAgRSAtLT4gQgogICAgRSAtLT4gRntNYXggUmV0cmllcz99CiAgICBGIC0tPnxZZXN8IEdbRXJyb3JdCiAgICBGIC0tPnxOb3wgQgo=?type=png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# What we'll build with LangGraph\n",
    "langgraph_flow = \"\"\"\n",
    "graph TD\n",
    "    A[Start] --> B[Detect BS]\n",
    "    B --> C{Success?}\n",
    "    C -->|Yes| D[End]\n",
    "    C -->|No| E[Retry]\n",
    "    E --> B\n",
    "    E --> F{Max Retries?}\n",
    "    F -->|Yes| G[Error]\n",
    "    F -->|No| B\n",
    "\"\"\"\n",
    "display(render_mermaid(langgraph_flow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Claim: The Boeing 747 uses anti-gravity propulsion\n",
      "Type: technical\n",
      "Agent: technical_expert\n",
      "Verdict: BS (95%)\n",
      "\n",
      "Claim: Napoleon invaded Russia in 1812\n",
      "Type: historical\n",
      "Agent: historical_expert\n",
      "Verdict: LEGITIMATE (95%)\n",
      "\n",
      "Claim: A major tech company announced layoffs yesterday\n",
      "Type: current_event\n",
      "Agent: current_events_expert\n",
      "Verdict: UNCERTAIN (70%)\n",
      "\n",
      "Claim: Eating carrots improves night vision\n",
      "Type: general\n",
      "Agent: general_expert\n",
      "Verdict: UNCERTAIN (70%)\n"
     ]
    }
   ],
   "source": [
    "# Define routing output model\n",
    "class RoutingOutput(BaseModel):\n",
    "    \"\"\"Output for claim routing\"\"\"\n",
    "    claim_type: Literal[\"technical\", \"historical\", \"current_event\", \"general\"] = Field(\n",
    "        description=\"The category of the claim\"\n",
    "    )\n",
    "\n",
    "# Enhanced state with routing\n",
    "class RoutingState(BSDetectorState):\n",
    "    \"\"\"State with routing information\"\"\"\n",
    "    claim_type: Optional[str] = None\n",
    "    analyzing_agent: Optional[str] = None\n",
    "\n",
    "# Router node\n",
    "def router_node(state: RoutingState) -> dict:\n",
    "    \"\"\"Route claims to appropriate expert\"\"\"\n",
    "    routing_prompt = f\"\"\"Categorize this claim into one of these types:\n",
    "- technical: Engineering, physics, specifications\n",
    "- historical: Past events, dates, historical facts\n",
    "- current_event: Recent news, current happenings\n",
    "- general: Everything else\n",
    "\n",
    "Claim: {state.claim}\n",
    "\n",
    "Respond with just the category name.\"\"\"\n",
    "    \n",
    "    # Use structured output for routing\n",
    "    structured_llm = llm.with_structured_output(RoutingOutput)\n",
    "    result = structured_llm.invoke(routing_prompt)\n",
    "    \n",
    "    return {\"claim_type\": result.claim_type}\n",
    "\n",
    "# Expert nodes\n",
    "def technical_expert_node(state: RoutingState) -> dict:\n",
    "    \"\"\"Technical expert for engineering/physics claims\"\"\"\n",
    "    prompt = f\"\"\"You are a technical expert in engineering and physics.\n",
    "Analyze this technical claim for accuracy.\n",
    "\n",
    "Claim: {state.claim}\n",
    "\n",
    "Consider physical laws, engineering principles, and technical feasibility.\"\"\"\n",
    "    \n",
    "    structured_llm = llm.with_structured_output(BSDetectorOutput)\n",
    "    result = structured_llm.invoke(prompt)\n",
    "    \n",
    "    return {\n",
    "        \"verdict\": result.verdict,\n",
    "        \"confidence\": result.confidence,\n",
    "        \"reasoning\": f\"[Technical Expert] {result.reasoning}\",\n",
    "        \"analyzing_agent\": \"technical_expert\"\n",
    "    }\n",
    "\n",
    "def historical_expert_node(state: RoutingState) -> dict:\n",
    "    \"\"\"Historical expert for past events\"\"\"\n",
    "    prompt = f\"\"\"You are a historical expert.\n",
    "Analyze this historical claim for accuracy.\n",
    "\n",
    "Claim: {state.claim}\n",
    "\n",
    "Consider historical records, dates, and documented events.\"\"\"\n",
    "    \n",
    "    structured_llm = llm.with_structured_output(BSDetectorOutput)\n",
    "    result = structured_llm.invoke(prompt)\n",
    "    \n",
    "    return {\n",
    "        \"verdict\": result.verdict,\n",
    "        \"confidence\": result.confidence,\n",
    "        \"reasoning\": f\"[Historical Expert] {result.reasoning}\",\n",
    "        \"analyzing_agent\": \"historical_expert\"\n",
    "    }\n",
    "\n",
    "def current_events_expert_node(state: RoutingState) -> dict:\n",
    "    \"\"\"Current events expert\"\"\"\n",
    "    prompt = f\"\"\"You are a current events expert.\n",
    "Analyze this claim about recent events.\n",
    "\n",
    "Claim: {state.claim}\n",
    "\n",
    "Note: You may not have access to events after your training cutoff.\n",
    "Be transparent about limitations.\"\"\"\n",
    "    \n",
    "    structured_llm = llm.with_structured_output(BSDetectorOutput)\n",
    "    result = structured_llm.invoke(prompt)\n",
    "    \n",
    "    return {\n",
    "        \"verdict\": result.verdict,\n",
    "        \"confidence\": min(result.confidence, 70),  # Cap confidence for current events\n",
    "        \"reasoning\": f\"[Current Events Expert] {result.reasoning}\",\n",
    "        \"analyzing_agent\": \"current_events_expert\"\n",
    "    }\n",
    "\n",
    "def general_expert_node(state: RoutingState) -> dict:\n",
    "    \"\"\"General expert for other claims\"\"\"\n",
    "    prompt = f\"\"\"Analyze if this claim is BS or legitimate.\n",
    "\n",
    "Claim: {state.claim}\n",
    "\n",
    "Provide your analysis with a verdict, confidence score, and reasoning.\"\"\"\n",
    "    \n",
    "    structured_llm = llm.with_structured_output(BSDetectorOutput)\n",
    "    result = structured_llm.invoke(prompt)\n",
    "    \n",
    "    return {\n",
    "        \"verdict\": result.verdict,\n",
    "        \"confidence\": result.confidence,\n",
    "        \"reasoning\": f\"[General Expert] {result.reasoning}\",\n",
    "        \"analyzing_agent\": \"general_expert\"\n",
    "    }\n",
    "\n",
    "# Build multi-agent graph\n",
    "def create_multi_agent_graph():\n",
    "    \"\"\"Create the multi-agent BS detector\"\"\"\n",
    "    workflow = StateGraph(RoutingState)\n",
    "    \n",
    "    # Add nodes\n",
    "    workflow.add_node(\"router\", router_node)\n",
    "    workflow.add_node(\"technical_expert\", technical_expert_node)\n",
    "    workflow.add_node(\"historical_expert\", historical_expert_node)\n",
    "    workflow.add_node(\"current_events_expert\", current_events_expert_node)\n",
    "    workflow.add_node(\"general_expert\", general_expert_node)\n",
    "    \n",
    "    # Add edges\n",
    "    workflow.add_edge(START, \"router\")\n",
    "    \n",
    "    # Route to appropriate expert\n",
    "    def route_to_expert(state: RoutingState) -> str:\n",
    "        return f\"{state.claim_type}_expert\"\n",
    "    \n",
    "    workflow.add_conditional_edges(\n",
    "        \"router\",\n",
    "        route_to_expert,\n",
    "        {\n",
    "            \"technical_expert\": \"technical_expert\",\n",
    "            \"historical_expert\": \"historical_expert\",\n",
    "            \"current_event_expert\": \"current_events_expert\",\n",
    "            \"general_expert\": \"general_expert\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # All experts go to END\n",
    "    for expert in [\"technical_expert\", \"historical_expert\", \"current_events_expert\", \"general_expert\"]:\n",
    "        workflow.add_edge(expert, END)\n",
    "    \n",
    "    return workflow.compile()\n",
    "\n",
    "# Test with different claims\n",
    "multi_agent_app = create_multi_agent_graph()\n",
    "\n",
    "test_claims = [\n",
    "    \"The Boeing 747 uses anti-gravity propulsion\",\n",
    "    \"Napoleon invaded Russia in 1812\",\n",
    "    \"A major tech company announced layoffs yesterday\",\n",
    "    \"Eating carrots improves night vision\"\n",
    "]\n",
    "\n",
    "for claim in test_claims:\n",
    "    result = multi_agent_app.invoke({\"claim\": claim})  # Pass dictionary\n",
    "    print(f\"\\nClaim: {claim}\")\n",
    "    print(f\"Type: {result['claim_type']}\")\n",
    "    print(f\"Agent: {result['analyzing_agent']}\")\n",
    "    print(f\"Verdict: {result['verdict']} ({result['confidence']}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Multiple Expert Agents\n",
    "\n",
    "Let's add specialized agents for different types of claims."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://mermaid.ink/img/CmdyYXBoIFRECiAgICBBW0NsYWltXSAtLT4gQltSb3V0ZXJdCiAgICBCIC0tPnxUZWNobmljYWx8IENbVGVjaG5pY2FsIEV4cGVydF0KICAgIEIgLS0+fEhpc3RvcmljYWx8IERbSGlzdG9yaWNhbCBFeHBlcnRdCiAgICBCIC0tPnxDdXJyZW50IEV2ZW50c3wgRVtDdXJyZW50IEV2ZW50cyBFeHBlcnRdCiAgICBCIC0tPnxHZW5lcmFsfCBGW0dlbmVyYWwgRXhwZXJ0XQogICAgQyAtLT4gR1tWZXJkaWN0XQogICAgRCAtLT4gRwogICAgRSAtLT4gRwogICAgRiAtLT4gRwo=?type=png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Multi-agent architecture\n",
    "multi_agent_flow = \"\"\"\n",
    "graph TD\n",
    "    A[Claim] --> B[Router]\n",
    "    B -->|Technical| C[Technical Expert]\n",
    "    B -->|Historical| D[Historical Expert]\n",
    "    B -->|Current Events| E[Current Events Expert]\n",
    "    B -->|General| F[General Expert]\n",
    "    C --> G[Verdict]\n",
    "    D --> G\n",
    "    E --> G\n",
    "    F --> G\n",
    "\"\"\"\n",
    "display(render_mermaid(multi_agent_flow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Claim: The Boeing 747 uses anti-gravity propulsion\n",
      "Type: technical\n",
      "Agent: technical_expert\n",
      "Verdict: BS (95%)\n",
      "\n",
      "Claim: Napoleon invaded Russia in 1812\n",
      "Type: historical\n",
      "Agent: historical_expert\n",
      "Verdict: LEGITIMATE (95%)\n",
      "\n",
      "Claim: A major tech company announced layoffs yesterday\n",
      "Type: current_event\n",
      "Agent: current_events_expert\n",
      "Verdict: UNCERTAIN (70%)\n",
      "\n",
      "Claim: Eating carrots improves night vision\n",
      "Type: general\n",
      "Agent: general_expert\n",
      "Verdict: UNCERTAIN (75%)\n"
     ]
    }
   ],
   "source": [
    "# Reuse the RoutingOutput model defined earlier\n",
    "# Enhanced state with routing\n",
    "class RoutingState(BSDetectorState):\n",
    "    \"\"\"State with routing information\"\"\"\n",
    "    claim_type: Optional[str] = None\n",
    "    analyzing_agent: Optional[str] = None\n",
    "\n",
    "# Router node\n",
    "def router_node(state: RoutingState) -> dict:\n",
    "    \"\"\"Route claims to appropriate expert\"\"\"\n",
    "    routing_prompt = f\"\"\"Categorize this claim into one of these types:\n",
    "- technical: Engineering, physics, specifications\n",
    "- historical: Past events, dates, historical facts\n",
    "- current_event: Recent news, current happenings\n",
    "- general: Everything else\n",
    "\n",
    "Claim: {state.claim}\n",
    "\n",
    "Respond with just the category name.\"\"\"\n",
    "    \n",
    "    # Use structured output for routing\n",
    "    structured_llm = llm.with_structured_output(RoutingOutput)\n",
    "    result = structured_llm.invoke(routing_prompt)\n",
    "    \n",
    "    return {\"claim_type\": result.claim_type}\n",
    "\n",
    "# Expert nodes\n",
    "def technical_expert_node(state: RoutingState) -> dict:\n",
    "    \"\"\"Technical expert for engineering/physics claims\"\"\"\n",
    "    prompt = f\"\"\"You are a technical expert in engineering and physics.\n",
    "Analyze this technical claim for accuracy.\n",
    "\n",
    "Claim: {state.claim}\n",
    "\n",
    "Consider physical laws, engineering principles, and technical feasibility.\"\"\"\n",
    "    \n",
    "    structured_llm = llm.with_structured_output(BSDetectorOutput)\n",
    "    result = structured_llm.invoke(prompt)\n",
    "    \n",
    "    return {\n",
    "        \"verdict\": result.verdict,\n",
    "        \"confidence\": result.confidence,\n",
    "        \"reasoning\": f\"[Technical Expert] {result.reasoning}\",\n",
    "        \"analyzing_agent\": \"technical_expert\"\n",
    "    }\n",
    "\n",
    "def historical_expert_node(state: RoutingState) -> dict:\n",
    "    \"\"\"Historical expert for past events\"\"\"\n",
    "    prompt = f\"\"\"You are a historical expert.\n",
    "Analyze this historical claim for accuracy.\n",
    "\n",
    "Claim: {state.claim}\n",
    "\n",
    "Consider historical records, dates, and documented events.\"\"\"\n",
    "    \n",
    "    structured_llm = llm.with_structured_output(BSDetectorOutput)\n",
    "    result = structured_llm.invoke(prompt)\n",
    "    \n",
    "    return {\n",
    "        \"verdict\": result.verdict,\n",
    "        \"confidence\": result.confidence,\n",
    "        \"reasoning\": f\"[Historical Expert] {result.reasoning}\",\n",
    "        \"analyzing_agent\": \"historical_expert\"\n",
    "    }\n",
    "\n",
    "def current_events_expert_node(state: RoutingState) -> dict:\n",
    "    \"\"\"Current events expert\"\"\"\n",
    "    prompt = f\"\"\"You are a current events expert.\n",
    "Analyze this claim about recent events.\n",
    "\n",
    "Claim: {state.claim}\n",
    "\n",
    "Note: You may not have access to events after your training cutoff.\n",
    "Be transparent about limitations.\"\"\"\n",
    "    \n",
    "    structured_llm = llm.with_structured_output(BSDetectorOutput)\n",
    "    result = structured_llm.invoke(prompt)\n",
    "    \n",
    "    return {\n",
    "        \"verdict\": result.verdict,\n",
    "        \"confidence\": min(result.confidence, 70),  # Cap confidence for current events\n",
    "        \"reasoning\": f\"[Current Events Expert] {result.reasoning}\",\n",
    "        \"analyzing_agent\": \"current_events_expert\"\n",
    "    }\n",
    "\n",
    "def general_expert_node(state: RoutingState) -> dict:\n",
    "    \"\"\"General expert for other claims\"\"\"\n",
    "    prompt = f\"\"\"Analyze if this claim is BS or legitimate.\n",
    "\n",
    "Claim: {state.claim}\n",
    "\n",
    "Provide your analysis with a verdict, confidence score, and reasoning.\"\"\"\n",
    "    \n",
    "    structured_llm = llm.with_structured_output(BSDetectorOutput)\n",
    "    result = structured_llm.invoke(prompt)\n",
    "    \n",
    "    return {\n",
    "        \"verdict\": result.verdict,\n",
    "        \"confidence\": result.confidence,\n",
    "        \"reasoning\": f\"[General Expert] {result.reasoning}\",\n",
    "        \"analyzing_agent\": \"general_expert\"\n",
    "    }\n",
    "\n",
    "# Build multi-agent graph\n",
    "def create_multi_agent_graph():\n",
    "    \"\"\"Create the multi-agent BS detector\"\"\"\n",
    "    workflow = StateGraph(RoutingState)\n",
    "    \n",
    "    # Add nodes\n",
    "    workflow.add_node(\"router\", router_node)\n",
    "    workflow.add_node(\"technical_expert\", technical_expert_node)\n",
    "    workflow.add_node(\"historical_expert\", historical_expert_node)\n",
    "    workflow.add_node(\"current_events_expert\", current_events_expert_node)\n",
    "    workflow.add_node(\"general_expert\", general_expert_node)\n",
    "    \n",
    "    # Add edges\n",
    "    workflow.add_edge(START, \"router\")\n",
    "    \n",
    "    # Route to appropriate expert\n",
    "    def route_to_expert(state: RoutingState) -> str:\n",
    "        return f\"{state.claim_type}_expert\"\n",
    "    \n",
    "    workflow.add_conditional_edges(\n",
    "        \"router\",\n",
    "        route_to_expert,\n",
    "        {\n",
    "            \"technical_expert\": \"technical_expert\",\n",
    "            \"historical_expert\": \"historical_expert\",\n",
    "            \"current_event_expert\": \"current_events_expert\",\n",
    "            \"general_expert\": \"general_expert\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # All experts go to END\n",
    "    for expert in [\"technical_expert\", \"historical_expert\", \"current_events_expert\", \"general_expert\"]:\n",
    "        workflow.add_edge(expert, END)\n",
    "    \n",
    "    return workflow.compile()\n",
    "\n",
    "# Test with different claims\n",
    "multi_agent_app = create_multi_agent_graph()\n",
    "\n",
    "test_claims = [\n",
    "    \"The Boeing 747 uses anti-gravity propulsion\",\n",
    "    \"Napoleon invaded Russia in 1812\",\n",
    "    \"A major tech company announced layoffs yesterday\",\n",
    "    \"Eating carrots improves night vision\"\n",
    "]\n",
    "\n",
    "for claim in test_claims:\n",
    "    result = multi_agent_app.invoke({\"claim\": claim})  # Pass dictionary\n",
    "    print(f\"\\nClaim: {claim}\")\n",
    "    print(f\"Type: {result['claim_type']}\")\n",
    "    print(f\"Agent: {result['analyzing_agent']}\")\n",
    "    print(f\"Verdict: {result['verdict']} ({result['confidence']}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Adding Tools for Evidence\n",
    "\n",
    "Let's add search capabilities so our agents can gather evidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://mermaid.ink/img/CmdyYXBoIFRECiAgICBBW0NsYWltXSAtLT4gQltSb3V0ZXJdCiAgICBCIC0tPiBDW0V4cGVydF0KICAgIEMgLS0+IER7TmVlZCBFdmlkZW5jZT99CiAgICBEIC0tPnxZZXN8IEVbU2VhcmNoIFRvb2xdCiAgICBEIC0tPnxOb3wgRltWZXJkaWN0XQogICAgRSAtLT4gR1tBbmFseXplIEV2aWRlbmNlXQogICAgRyAtLT4gRgo=?type=png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tool-enhanced architecture\n",
    "tool_flow = \"\"\"\n",
    "graph TD\n",
    "    A[Claim] --> B[Router]\n",
    "    B --> C[Expert]\n",
    "    C --> D{Need Evidence?}\n",
    "    D -->|Yes| E[Search Tool]\n",
    "    D -->|No| F[Verdict]\n",
    "    E --> G[Analyze Evidence]\n",
    "    G --> F\n",
    "\"\"\"\n",
    "display(render_mermaid(tool_flow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing tool-enhanced BS detection:\n",
      "\n",
      "Claim: The Boeing 747 can fly faster than the speed of sound\n",
      "Evidence found: The Boeing 747 has 4 engines and cruises at Mach 0.85\n",
      "Verdict: BS (95%)\n",
      "Reasoning: The Boeing 747 is a large commercial airliner designed to cruise at subsonic speeds, specifically around Mach 0.85, which is below the speed of sound (Mach 1). While it has 4 engines, these are not capable of pushing the aircraft into supersonic speeds. Therefore, the claim that the Boeing 747 can fly faster than the speed of sound is false.\n"
     ]
    }
   ],
   "source": [
    "# Define search tool\n",
    "@tool\n",
    "def search_evidence(query: str) -> str:\n",
    "    \"\"\"Search for evidence about a claim.\n",
    "    \n",
    "    In a real implementation, this would use web search.\n",
    "    For the workshop, we'll simulate with aviation facts.\n",
    "    \"\"\"\n",
    "    # Simulated knowledge base\n",
    "    facts = {\n",
    "        \"747\": \"The Boeing 747 has 4 engines and cruises at Mach 0.85\",\n",
    "        \"a380\": \"The Airbus A380 is the world's largest passenger airliner\",\n",
    "        \"wright\": \"The Wright Brothers first flew on December 17, 1903\",\n",
    "        \"concorde\": \"The Concorde could fly at Mach 2.04\",\n",
    "        \"jet fuel\": \"Jet fuel is a type of aviation fuel, not a conspiracy theory\",\n",
    "        \"speed of sound\": \"The speed of sound is approximately 767 mph at sea level\"\n",
    "    }\n",
    "    \n",
    "    query_lower = query.lower()\n",
    "    relevant_facts = []\n",
    "    \n",
    "    for key, fact in facts.items():\n",
    "        if key in query_lower:\n",
    "            relevant_facts.append(fact)\n",
    "    \n",
    "    if relevant_facts:\n",
    "        return \" | \".join(relevant_facts)\n",
    "    else:\n",
    "        return \"No specific evidence found for this query.\"\n",
    "\n",
    "# Enhanced state with tools\n",
    "class ToolState(RoutingState):\n",
    "    \"\"\"State with tool usage tracking\"\"\"\n",
    "    messages: List[Any] = Field(default_factory=list)\n",
    "    evidence: Optional[str] = None\n",
    "    used_tools: List[str] = Field(default_factory=list)\n",
    "\n",
    "# Tool-enhanced expert node\n",
    "def expert_with_tools_node(state: ToolState) -> dict:\n",
    "    \"\"\"Expert that can use tools\"\"\"\n",
    "    # Bind tools to LLM\n",
    "    llm_with_tools = llm.bind_tools([search_evidence])\n",
    "    \n",
    "    # Initial prompt\n",
    "    messages = [\n",
    "        SystemMessage(content=\"\"\"You are a BS detector with access to search.\n",
    "        First search for evidence about the claim, then make your verdict.\n",
    "        Always use the search tool before making a decision.\"\"\"),\n",
    "        HumanMessage(content=f\"Analyze this claim: {state.claim}\")\n",
    "    ]\n",
    "    \n",
    "    # Get LLM response\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    \n",
    "    # Check if tool was called\n",
    "    if response.tool_calls:\n",
    "        tool_call = response.tool_calls[0]\n",
    "        tool_result = search_evidence.invoke(tool_call['args'])\n",
    "        \n",
    "        # Add tool result to messages\n",
    "        messages.extend([\n",
    "            response,\n",
    "            {\"role\": \"tool\", \"content\": tool_result, \"tool_call_id\": tool_call['id']}\n",
    "        ])\n",
    "        \n",
    "        # Get final response with evidence\n",
    "        final_prompt = f\"\"\"Based on the evidence, analyze if this claim is BS:\n",
    "        \n",
    "Claim: {state.claim}\n",
    "Evidence: {tool_result}\n",
    "\n",
    "Provide verdict, confidence, and reasoning.\"\"\"\n",
    "        \n",
    "        structured_llm = llm.with_structured_output(BSDetectorOutput)\n",
    "        result = structured_llm.invoke(final_prompt)\n",
    "        \n",
    "        return {\n",
    "            \"verdict\": result.verdict,\n",
    "            \"confidence\": result.confidence,\n",
    "            \"reasoning\": result.reasoning,\n",
    "            \"evidence\": tool_result,\n",
    "            \"used_tools\": [\"search_evidence\"]\n",
    "        }\n",
    "    \n",
    "    # Fallback if no tool used\n",
    "    return detect_bs_node(state)\n",
    "\n",
    "# Test tool-enhanced detection\n",
    "print(\"Testing tool-enhanced BS detection:\\n\")\n",
    "\n",
    "state = ToolState(claim=\"The Boeing 747 can fly faster than the speed of sound\")\n",
    "result = expert_with_tools_node(state)\n",
    "\n",
    "print(f\"Claim: {state.claim}\")\n",
    "print(f\"Evidence found: {result['evidence']}\")\n",
    "print(f\"Verdict: {result['verdict']} ({result['confidence']}%)\")\n",
    "print(f\"Reasoning: {result['reasoning']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Human-in-the-Loop\n",
    "\n",
    "Add human review for low confidence cases using LangGraph's interrupt feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://mermaid.ink/img/CmdyYXBoIFRECiAgICBBW0NsYWltXSAtLT4gQltBSSBBbmFseXNpc10KICAgIEIgLS0+IEN7Q29uZmlkZW5jZSA+IDcwJT99CiAgICBDIC0tPnxZZXN8IERbRmluYWwgVmVyZGljdF0KICAgIEMgLS0+fE5vfCBFW0ludGVycnVwdCBmb3IgSHVtYW5dCiAgICBFIC0tPiBGW0h1bWFuIFJldmlld10KICAgIEYgLS0+IEdbUmVzdW1lIHdpdGggSHVtYW4gSW5wdXRdCiAgICBHIC0tPiBECg==?type=png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Human-in-the-loop flow\n",
    "human_flow = \"\"\"\n",
    "graph TD\n",
    "    A[Claim] --> B[AI Analysis]\n",
    "    B --> C{Confidence > 70%?}\n",
    "    C -->|Yes| D[Final Verdict]\n",
    "    C -->|No| E[Interrupt for Human]\n",
    "    E --> F[Human Review]\n",
    "    F --> G[Resume with Human Input]\n",
    "    G --> D\n",
    "\"\"\"\n",
    "display(render_mermaid(human_flow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating human-in-the-loop:\n",
      "\n",
      "Claim: Quantum computers can solve NP-complete problems in polynomial time\n",
      "AI Verdict: BS (90%)\n",
      "Needs human review: False\n"
     ]
    }
   ],
   "source": [
    "# Human review tool\n",
    "@tool\n",
    "def request_human_review(claim: str, ai_verdict: str, confidence: int, reasoning: str) -> str:\n",
    "    \"\"\"Request human review when confidence is low.\"\"\"\n",
    "    # Use LangGraph's interrupt to pause execution\n",
    "    human_response = interrupt({\n",
    "        \"claim\": claim,\n",
    "        \"ai_verdict\": ai_verdict,\n",
    "        \"confidence\": confidence,\n",
    "        \"reasoning\": reasoning,\n",
    "        \"message\": f\"Low confidence ({confidence}%) - requesting human review\"\n",
    "    })\n",
    "    return human_response.get(\"verdict\", ai_verdict)\n",
    "\n",
    "# State with human review\n",
    "class HumanState(ToolState):\n",
    "    \"\"\"State with human review tracking\"\"\"\n",
    "    needs_human_review: bool = False\n",
    "    human_reviewed: bool = False\n",
    "    human_verdict: Optional[str] = None\n",
    "\n",
    "# Human-in-the-loop node\n",
    "def bs_detector_with_human(state: HumanState) -> dict:\n",
    "    \"\"\"BS detector that requests human input for low confidence\"\"\"\n",
    "    # First, get AI verdict\n",
    "    ai_result = expert_with_tools_node(state)\n",
    "    \n",
    "    # Check if human review needed\n",
    "    if ai_result['confidence'] < 70:\n",
    "        # Bind human review tool\n",
    "        llm_with_human = llm.bind_tools([request_human_review])\n",
    "        \n",
    "        # Create message requesting human review\n",
    "        review_prompt = f\"\"\"The AI analysis has low confidence. Please use the request_human_review tool.\n",
    "        \n",
    "Claim: {state.claim}\n",
    "AI Verdict: {ai_result['verdict']}\n",
    "Confidence: {ai_result['confidence']}%\n",
    "Reasoning: {ai_result['reasoning']}\"\"\"\n",
    "        \n",
    "        response = llm_with_human.invoke(review_prompt)\n",
    "        \n",
    "        # This will interrupt execution\n",
    "        if response.tool_calls:\n",
    "            return {\n",
    "                **ai_result,\n",
    "                \"needs_human_review\": True\n",
    "            }\n",
    "    \n",
    "    return ai_result\n",
    "\n",
    "# Build human-in-the-loop graph\n",
    "def create_human_loop_graph():\n",
    "    \"\"\"Create graph with human-in-the-loop\"\"\"\n",
    "    workflow = StateGraph(HumanState)\n",
    "    \n",
    "    workflow.add_node(\"analyze\", bs_detector_with_human)\n",
    "    \n",
    "    workflow.add_edge(START, \"analyze\")\n",
    "    workflow.add_edge(\"analyze\", END)\n",
    "    \n",
    "    # Compile with memory for interrupts\n",
    "    memory = MemorySaver()\n",
    "    return workflow.compile(\n",
    "        checkpointer=memory,\n",
    "        interrupt_before=[\"analyze\"]  # Can interrupt before analysis\n",
    "    )\n",
    "\n",
    "# Simulate human-in-the-loop\n",
    "print(\"Simulating human-in-the-loop:\\n\")\n",
    "\n",
    "# This would normally interrupt and wait for human input\n",
    "# For demo, we'll show the concept\n",
    "low_confidence_claim = \"Quantum computers can solve NP-complete problems in polynomial time\"\n",
    "state = HumanState(claim=low_confidence_claim)\n",
    "result = bs_detector_with_human(state)\n",
    "\n",
    "print(f\"Claim: {low_confidence_claim}\")\n",
    "print(f\"AI Verdict: {result['verdict']} ({result['confidence']}%)\")\n",
    "print(f\"Needs human review: {result.get('needs_human_review', False)}\")\n",
    "\n",
    "if result.get('needs_human_review'):\n",
    "    print(\"\\n⚠️  In a real system, execution would pause here for human input.\")\n",
    "    print(\"The human could provide a final verdict and additional context.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Memory System\n",
    "\n",
    "Finally, let's add memory so our BS detector learns from past claims."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://mermaid.ink/img/CmdyYXBoIExSCiAgICBBW05ldyBDbGFpbV0gLS0+IEJbRXh0cmFjdCBFbnRpdGllc10KICAgIEIgLS0+IENbU2VhcmNoIE1lbW9yeV0KICAgIEMgLS0+IERbRW5oYW5jZWQgQ29udGV4dF0KICAgIEQgLS0+IEVbQUkgQW5hbHlzaXNdCiAgICBFIC0tPiBGW1N0b3JlIFJlc3VsdF0KICAgIEYgLS0+IEdbVXBkYXRlIFBhdHRlcm5zXQo=?type=png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Memory architecture\n",
    "memory_flow = \"\"\"\n",
    "graph LR\n",
    "    A[New Claim] --> B[Extract Entities]\n",
    "    B --> C[Search Memory]\n",
    "    C --> D[Enhanced Context]\n",
    "    D --> E[AI Analysis]\n",
    "    E --> F[Store Result]\n",
    "    F --> G[Update Patterns]\n",
    "\"\"\"\n",
    "display(render_mermaid(memory_flow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing memory system:\n",
      "\n",
      "\n",
      "Claim 1: The Boeing 747 has four engines\n",
      "Verdict: LEGITIMATE (95%)\n",
      "\n",
      "Claim 2: The Boeing 747 can fly at Mach 2\n",
      "Verdict: BS (90%)\n",
      "Used context from 2 related claims\n",
      "\n",
      "Claim 3: A quantum engine can achieve light speed\n",
      "Verdict: BS (95%)\n",
      "\n",
      "Claim 4: Another quantum breakthrough defies physics\n",
      "Verdict: UNCERTAIN (60%)\n",
      "\n",
      "\n",
      "📊 Memory Statistics:\n",
      "Total claims stored: 4\n",
      "Unique entities: 5\n",
      "BS patterns detected: {'quantum': 1}\n"
     ]
    }
   ],
   "source": [
    "# Simple in-memory storage\n",
    "MEMORY_STORE = {\n",
    "    \"claims\": [],\n",
    "    \"entities\": defaultdict(list),\n",
    "    \"patterns\": defaultdict(int)\n",
    "}\n",
    "\n",
    "class MemoryManager:\n",
    "    \"\"\"Manages memory operations\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_entities(text: str) -> List[str]:\n",
    "        \"\"\"Extract entities from text\"\"\"\n",
    "        # Simple entity extraction\n",
    "        entities = []\n",
    "        \n",
    "        # Find capitalized words (proper nouns)\n",
    "        entities.extend(re.findall(r'\\b[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*\\b', text))\n",
    "        \n",
    "        # Find numbers with context (747, A380, etc)\n",
    "        entities.extend(re.findall(r'\\b[A-Z]?\\d+\\b', text))\n",
    "        \n",
    "        return list(set(entities))  # Remove duplicates\n",
    "    \n",
    "    @staticmethod\n",
    "    def store_claim(claim: str, verdict: str, confidence: int, reasoning: str):\n",
    "        \"\"\"Store claim and verdict in memory\"\"\"\n",
    "        entities = MemoryManager.extract_entities(claim)\n",
    "        \n",
    "        # Store claim record\n",
    "        claim_record = {\n",
    "            \"claim\": claim,\n",
    "            \"verdict\": verdict,\n",
    "            \"confidence\": confidence,\n",
    "            \"reasoning\": reasoning,\n",
    "            \"entities\": entities,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "        MEMORY_STORE[\"claims\"].append(claim_record)\n",
    "        \n",
    "        # Index by entities\n",
    "        for entity in entities:\n",
    "            MEMORY_STORE[\"entities\"][entity].append(len(MEMORY_STORE[\"claims\"]) - 1)\n",
    "        \n",
    "        # Track BS patterns\n",
    "        if verdict == \"BS\":\n",
    "            bs_keywords = ['quantum', 'perpetual', 'free energy', 'anti-gravity']\n",
    "            for keyword in bs_keywords:\n",
    "                if keyword in claim.lower():\n",
    "                    MEMORY_STORE[\"patterns\"][keyword] += 1\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_context(claim: str) -> dict:\n",
    "        \"\"\"Get relevant context from memory\"\"\"\n",
    "        entities = MemoryManager.extract_entities(claim)\n",
    "        \n",
    "        # Find related claims\n",
    "        related_claims = []\n",
    "        for entity in entities:\n",
    "            for idx in MEMORY_STORE[\"entities\"].get(entity, []):\n",
    "                if idx < len(MEMORY_STORE[\"claims\"]):\n",
    "                    related_claims.append(MEMORY_STORE[\"claims\"][idx])\n",
    "        \n",
    "        # Check for BS patterns\n",
    "        detected_patterns = []\n",
    "        claim_lower = claim.lower()\n",
    "        for pattern, count in MEMORY_STORE[\"patterns\"].items():\n",
    "            if pattern in claim_lower and count >= 2:\n",
    "                detected_patterns.append(pattern)\n",
    "        \n",
    "        return {\n",
    "            \"related_claims\": related_claims[:3],  # Limit to 3\n",
    "            \"detected_patterns\": detected_patterns,\n",
    "            \"entities\": entities\n",
    "        }\n",
    "\n",
    "# Memory-enhanced state\n",
    "class MemoryState(HumanState):\n",
    "    \"\"\"State with memory context\"\"\"\n",
    "    memory_context: Optional[dict] = None\n",
    "    related_claims: List[dict] = Field(default_factory=list)\n",
    "\n",
    "# Memory-enhanced BS detector\n",
    "def memory_enhanced_detector(state: MemoryState) -> dict:\n",
    "    \"\"\"BS detector with memory\"\"\"\n",
    "    # Get memory context\n",
    "    context = MemoryManager.get_context(state.claim)\n",
    "    \n",
    "    # Build enhanced prompt with context\n",
    "    context_prompt = \"Analyze this claim for accuracy.\\n\\n\"\n",
    "    \n",
    "    if context[\"related_claims\"]:\n",
    "        context_prompt += \"Related previous claims:\\n\"\n",
    "        for rc in context[\"related_claims\"]:\n",
    "            context_prompt += f\"- {rc['claim']}: {rc['verdict']} ({rc['confidence']}%)\\n\"\n",
    "        context_prompt += \"\\n\"\n",
    "    \n",
    "    if context[\"detected_patterns\"]:\n",
    "        context_prompt += f\"Warning: Contains known BS patterns: {', '.join(context['detected_patterns'])}\\n\\n\"\n",
    "    \n",
    "    context_prompt += f\"Current claim: {state.claim}\"\n",
    "    \n",
    "    # Get verdict with context\n",
    "    structured_llm = llm.with_structured_output(BSDetectorOutput)\n",
    "    result = structured_llm.invoke(context_prompt)\n",
    "    \n",
    "    # Store in memory\n",
    "    MemoryManager.store_claim(\n",
    "        state.claim,\n",
    "        result.verdict,\n",
    "        result.confidence,\n",
    "        result.reasoning\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"verdict\": result.verdict,\n",
    "        \"confidence\": result.confidence,\n",
    "        \"reasoning\": result.reasoning,\n",
    "        \"memory_context\": context,\n",
    "        \"related_claims\": context[\"related_claims\"]\n",
    "    }\n",
    "\n",
    "# Test memory system\n",
    "print(\"Testing memory system:\\n\")\n",
    "\n",
    "# Clear memory\n",
    "MEMORY_STORE[\"claims\"].clear()\n",
    "MEMORY_STORE[\"entities\"].clear()\n",
    "MEMORY_STORE[\"patterns\"].clear()\n",
    "\n",
    "# Process related claims\n",
    "test_sequence = [\n",
    "    \"The Boeing 747 has four engines\",\n",
    "    \"The Boeing 747 can fly at Mach 2\",  # Should use context about 747\n",
    "    \"A quantum engine can achieve light speed\",\n",
    "    \"Another quantum breakthrough defies physics\"  # Should detect pattern\n",
    "]\n",
    "\n",
    "for i, claim in enumerate(test_sequence):\n",
    "    state = MemoryState(claim=claim)\n",
    "    result = memory_enhanced_detector(state)\n",
    "    \n",
    "    print(f\"\\nClaim {i+1}: {claim}\")\n",
    "    print(f\"Verdict: {result['verdict']} ({result['confidence']}%)\")\n",
    "    \n",
    "    if result['related_claims']:\n",
    "        print(f\"Used context from {len(result['related_claims'])} related claims\")\n",
    "    \n",
    "    if result['memory_context']['detected_patterns']:\n",
    "        print(f\"⚠️  Detected BS pattern: {result['memory_context']['detected_patterns']}\")\n",
    "\n",
    "# Show memory statistics\n",
    "print(f\"\\n\\n📊 Memory Statistics:\")\n",
    "print(f\"Total claims stored: {len(MEMORY_STORE['claims'])}\")\n",
    "print(f\"Unique entities: {len(MEMORY_STORE['entities'])}\")\n",
    "print(f\"BS patterns detected: {dict(MEMORY_STORE['patterns'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting It All Together\n",
    "\n",
    "Now let's build the complete BS detector with all features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://mermaid.ink/img/CmdyYXBoIFRECiAgICBBW0NsYWltXSAtLT4gQltNZW1vcnkgUmV0cmlldmFsXQogICAgQiAtLT4gQ1tSb3V0ZXJdCiAgICBDIC0tPiBEW0V4cGVydCArIFRvb2xzXQogICAgRCAtLT4gRXtDb25maWRlbmNlIENoZWNrfQogICAgRSAtLT58SGlnaHwgRltTdG9yZSAmIFJldHVybl0KICAgIEUgLS0+fExvd3wgR1tIdW1hbiBSZXZpZXddCiAgICBHIC0tPiBGCiAgICBGIC0tPiBIW1VwZGF0ZSBNZW1vcnldCg==?type=png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Complete architecture\n",
    "complete_flow = \"\"\"\n",
    "graph TD\n",
    "    A[Claim] --> B[Memory Retrieval]\n",
    "    B --> C[Router]\n",
    "    C --> D[Expert + Tools]\n",
    "    D --> E{Confidence Check}\n",
    "    E -->|High| F[Store & Return]\n",
    "    E -->|Low| G[Human Review]\n",
    "    G --> F\n",
    "    F --> H[Update Memory]\n",
    "\"\"\"\n",
    "display(render_mermaid(complete_flow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing complete BS detector:\n",
      "\n",
      "\n",
      "Claim: The Wright Brothers first flew in 1903\n",
      "Type: historical (Agent: historical_expert)\n",
      "Verdict: LEGITIMATE (95%)\n",
      "Reasoning: The claim that the Wright Brothers first flew in 1903 is well-documented and widely accepted by hist...\n",
      "\n",
      "Claim: The Wright Brothers used jet engines\n",
      "Type: historical (Agent: historical_expert)\n",
      "Verdict: BS (95%)\n",
      "Reasoning: The Wright Brothers achieved their first powered flight in 1903 using a piston engine, not jet engin...\n",
      "\n",
      "Claim: Quantum computers violate thermodynamics\n",
      "Type: technical (Agent: technical_expert)\n",
      "Verdict: BS (90%)\n",
      "Reasoning: Quantum computers operate based on the laws of quantum mechanics and do not violate the fundamental ...\n",
      "\n",
      "Claim: Another quantum device creates free energy\n",
      "Type: technical (Agent: technical_expert)\n",
      "Verdict: BS (90%)\n",
      "Reasoning: The claim that a quantum device creates free energy violates the fundamental laws of physics, specif...\n",
      "🚨 BS Pattern: ['quantum']\n"
     ]
    }
   ],
   "source": [
    "# Complete state\n",
    "class CompleteState(BaseModel):\n",
    "    \"\"\"Complete state with all features\"\"\"\n",
    "    # Core fields\n",
    "    claim: str\n",
    "    verdict: Optional[str] = None\n",
    "    confidence: Optional[int] = None\n",
    "    reasoning: Optional[str] = None\n",
    "    \n",
    "    # Routing\n",
    "    claim_type: Optional[str] = None\n",
    "    analyzing_agent: Optional[str] = None\n",
    "    \n",
    "    # Tools\n",
    "    evidence: Optional[str] = None\n",
    "    used_tools: List[str] = Field(default_factory=list)\n",
    "    \n",
    "    # Human review\n",
    "    needs_human_review: bool = False\n",
    "    human_reviewed: bool = False\n",
    "    \n",
    "    # Memory\n",
    "    memory_context: Optional[dict] = None\n",
    "    related_claims: List[dict] = Field(default_factory=list)\n",
    "\n",
    "# Build complete graph\n",
    "def create_complete_bs_detector():\n",
    "    \"\"\"Create the complete BS detector graph\"\"\"\n",
    "    workflow = StateGraph(CompleteState)\n",
    "    \n",
    "    # Memory retrieval node\n",
    "    def memory_node(state: CompleteState) -> dict:\n",
    "        context = MemoryManager.get_context(state.claim)\n",
    "        return {\"memory_context\": context}\n",
    "    \n",
    "    # Enhanced router with memory\n",
    "    def smart_router(state: CompleteState) -> dict:\n",
    "        # Use memory context in routing\n",
    "        context_info = \"\"\n",
    "        if state.memory_context and state.memory_context[\"detected_patterns\"]:\n",
    "            context_info = f\"(Known BS patterns detected: {state.memory_context['detected_patterns']})\"\n",
    "        \n",
    "        routing_prompt = f\"\"\"Categorize this claim {context_info}:\n",
    "        \n",
    "Categories: technical, historical, current_event, general\n",
    "\n",
    "Claim: {state.claim}\"\"\"\n",
    "        \n",
    "        # Use structured output for routing\n",
    "        structured_llm = llm.with_structured_output(RoutingOutput)\n",
    "        result = structured_llm.invoke(routing_prompt)\n",
    "        \n",
    "        return {\"claim_type\": result.claim_type}\n",
    "    \n",
    "    # Expert with all features\n",
    "    def complete_expert(state: CompleteState) -> dict:\n",
    "        # Build context-aware prompt\n",
    "        prompt = f\"Analyze this {state.claim_type} claim:\\n\\n\"\n",
    "        \n",
    "        if state.memory_context and state.memory_context[\"related_claims\"]:\n",
    "            prompt += \"Related claims:\\n\"\n",
    "            for rc in state.memory_context[\"related_claims\"]:\n",
    "                prompt += f\"- {rc['claim']}: {rc['verdict']}\\n\"\n",
    "            prompt += \"\\n\"\n",
    "        \n",
    "        prompt += f\"Claim: {state.claim}\"\n",
    "        \n",
    "        # Get verdict\n",
    "        structured_llm = llm.with_structured_output(BSDetectorOutput)\n",
    "        result = structured_llm.invoke(prompt)\n",
    "        \n",
    "        # Store in memory\n",
    "        MemoryManager.store_claim(\n",
    "            state.claim,\n",
    "            result.verdict,\n",
    "            result.confidence,\n",
    "            result.reasoning\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"verdict\": result.verdict,\n",
    "            \"confidence\": result.confidence,\n",
    "            \"reasoning\": result.reasoning,\n",
    "            \"analyzing_agent\": f\"{state.claim_type}_expert\",\n",
    "            \"needs_human_review\": result.confidence < 70\n",
    "        }\n",
    "    \n",
    "    # Add nodes\n",
    "    workflow.add_node(\"memory\", memory_node)\n",
    "    workflow.add_node(\"router\", smart_router)\n",
    "    workflow.add_node(\"expert\", complete_expert)\n",
    "    \n",
    "    # Add edges\n",
    "    workflow.add_edge(START, \"memory\")\n",
    "    workflow.add_edge(\"memory\", \"router\")\n",
    "    workflow.add_edge(\"router\", \"expert\")\n",
    "    workflow.add_edge(\"expert\", END)\n",
    "    \n",
    "    return workflow.compile()\n",
    "\n",
    "# Test complete system\n",
    "print(\"Testing complete BS detector:\\n\")\n",
    "\n",
    "complete_app = create_complete_bs_detector()\n",
    "\n",
    "# Test various claims\n",
    "final_test_claims = [\n",
    "    \"The Wright Brothers first flew in 1903\",\n",
    "    \"The Wright Brothers used jet engines\",\n",
    "    \"Quantum computers violate thermodynamics\",\n",
    "    \"Another quantum device creates free energy\"\n",
    "]\n",
    "\n",
    "for claim in final_test_claims:\n",
    "    result = complete_app.invoke({\"claim\": claim})  # Pass dictionary\n",
    "    \n",
    "    print(f\"\\nClaim: {claim}\")\n",
    "    print(f\"Type: {result['claim_type']} (Agent: {result['analyzing_agent']})\")\n",
    "    print(f\"Verdict: {result['verdict']} ({result['confidence']}%)\")\n",
    "    print(f\"Reasoning: {result['reasoning'][:100]}...\")\n",
    "    \n",
    "    if result['needs_human_review']:\n",
    "        print(\"⚠️  Flagged for human review\")\n",
    "    \n",
    "    if result['memory_context'] and result['memory_context'][\"detected_patterns\"]:\n",
    "        print(f\"🚨 BS Pattern: {result['memory_context']['detected_patterns']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "### What We Built\n",
    "1. **Simple Detector** → Basic prompt engineering\n",
    "2. **Structured Output** → Pydantic models for consistent results\n",
    "3. **LangGraph Basics** → State management and retry logic\n",
    "4. **Multi-Agent System** → Specialized experts for different domains\n",
    "5. **Tool Integration** → Evidence gathering with search\n",
    "6. **Human-in-the-Loop** → Interrupt pattern for human review\n",
    "7. **Memory System** → Learning from past claims\n",
    "\n",
    "### Key LangGraph Concepts\n",
    "- **StateGraph** - Manages state throughout execution\n",
    "- **Nodes** - Functions that process and update state\n",
    "- **Edges** - Define flow between nodes\n",
    "- **Conditional Edges** - Dynamic routing based on state\n",
    "- **Tools** - Extend agent capabilities\n",
    "- **Interrupts** - Pause for human input\n",
    "- **Memory** - Persist state across runs\n",
    "\n",
    "### Production Considerations\n",
    "1. **Persistence** - Use proper database for memory\n",
    "2. **Search** - Integrate real web search APIs\n",
    "3. **Monitoring** - Track performance and accuracy\n",
    "4. **Scaling** - Consider async execution\n",
    "5. **Security** - Validate inputs and outputs\n",
    "\n",
    "### Exercises\n",
    "1. Add a new expert type (e.g., medical claims)\n",
    "2. Implement confidence calibration\n",
    "3. Add citation tracking for evidence\n",
    "4. Build a UI for human review\n",
    "5. Export memory to analyze patterns\n",
    "\n",
    "### Resources\n",
    "- [LangGraph Documentation](https://github.com/langchain-ai/langgraph)\n",
    "- [LangChain Tools](https://python.langchain.com/docs/modules/tools/)\n",
    "- [Pydantic Models](https://docs.pydantic.dev/)\n",
    "\n",
    "Thank you for participating! 🚀"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
