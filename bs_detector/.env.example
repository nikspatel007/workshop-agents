# LLM Provider Configuration
LLM_PROVIDER=openai  # Options: openai, anthropic, bedrock, azure

# OpenAI
OPENAI_API_KEY=sk-...

# Anthropic
ANTHROPIC_API_KEY=sk-ant-...

# AWS Bedrock
AWS_REGION=us-east-1
AWS_ACCESS_KEY_ID=...
AWS_SECRET_ACCESS_KEY=...

# Azure OpenAI
AZURE_OPENAI_ENDPOINT=https://...
AZURE_OPENAI_API_KEY=...

#Lm Studio
# LM Studio endpoint (default: http://localhost:1234/v1)
# Change this if you're using a different port
LMSTUDIO_BASE_URL=http://localhost:1234/v1

# Optional: Specify model name
# If not set, LM Studio will use whatever model is currently loaded
# LMSTUDIO_MODEL=mistral-7b-instruct-v0.2

# General LLM settings
LLM_TEMPERATURE=0.7
LLM_TIMEOUT=60

# Debug Mode
BS_DETECTOR_DEBUG=0